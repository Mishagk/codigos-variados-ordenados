{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El presente notebook tiene como objetivo el brindar aplicaciones de ejemplo de las ANN. Importante indicarse que los ejemplos y materiales fueron consultados de fuentes como tensorflow.org y scikit-learn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T01:37:50.294115Z",
     "start_time": "2022-02-13T01:37:50.267118Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Conjunto de librerias a usar\n",
    "\n",
    "### En esta lista se trató de importar en lo posible de forma individual cada funcion o clase\n",
    "### Además, se debe considerar que aqui se importaron librerias de codigos previos, por lo que\n",
    "### muchas de las librerias importadas puede que no se usen o que falte optimizar aun más (limpiar/depurar)\n",
    "\n",
    "#############################################################################################################################\n",
    "### NOTA: Se reitera, no todo lo importado se usa o aplica. Asi mismo, falta limpiar/depurar algunas lineas de importacion\n",
    "#############################################################################################################################\n",
    "\n",
    "# Generales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "import dill\n",
    "from IPython.display import Image  \n",
    "import joblib\n",
    "from IPython.display import Image\n",
    "import sklearn as skl\n",
    "from IPython import display\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import scipy\n",
    "import sys\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import pickle\n",
    "from scipy import stats\n",
    "\n",
    "# Clonar modelos creados\n",
    "from sklearn.base import clone \n",
    "\n",
    "## Funciones/Classes para Modelos\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "## Preprocesamiento\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Conjunto de Datos a cargar\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "## Clasificadores \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## librerias Regresion Lineal\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import SGDRegressor ## Stochastic gradient descent (alternativa Mini-Batch)\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## SVM \n",
    "from sklearn.svm import LinearSVC # clasificacion\n",
    "from sklearn.svm import LinearSVR # regresion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "## Trees - Arboles de decisiones\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ensemble methods\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Reduction of Dimension\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Unsupervised learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import silhouette_visualizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "## Machine Learning - Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# ANN para Scikit\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "## advertencias\n",
    "import warnings\n",
    "## Ignorar las advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "## warnings.filterwarnings('always') ## mostrar advertencias\n",
    "\n",
    "## fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T01:36:33.784135Z",
     "start_time": "2022-02-13T01:36:33.774136Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "# ver version de tf y keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regresion NN (Scikit Learn + Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br><br><b>Importante !!!</b>: Es vital tener en cuenta que si lo que se quiere es convertir un modelo <b>Keras</b> a uno tipo <b>Scikit</b> para poder aplicar sus bondades como es el GridSearchCV, no se puede usar todos los modelos de Keras, el recomendado o unica opcion es usar <b>keras.Sequential()</b> esto debido a que para el Clasificador le brinda acceso al calculo mediante <b>modelo.predic_proba</b> y <b>modelo.predic</b>, estos son fundamentales en el calculo una vez entrenado (fit) el modelo a los datos.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Regresion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:46:17.301162Z",
     "start_time": "2021-02-15T14:46:17.249304Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "X_train shape:  (11610, 8)\n",
      "X_valid shape:  (3870, 8)\n",
      "X_test shape:  (5160, 8)\n",
      "-------------\n",
      "y_train shape:  (11610,)\n",
      "y_valid shape:  (3870,)\n",
      "y_test shape:  (5160,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# dividir los datos en conjuntos\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "# estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_valid_s = scaler.transform(X_valid)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print('\\n\\nX_train shape: ', X_train_s.shape)\n",
    "print('X_valid shape: ', X_valid_s.shape)\n",
    "print('X_test shape: ', X_test_s.shape)\n",
    "print('-------------')\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_valid shape: ', y_valid.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "print('\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### definir modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:46:40.838390Z",
     "start_time": "2021-02-15T14:46:19.794711Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.80951477\n",
      "Validation score: -0.013174\n",
      "Iteration 2, loss = 0.72109949\n",
      "Validation score: -0.013754\n",
      "Iteration 3, loss = 0.70458694\n",
      "Validation score: -0.015687\n",
      "Iteration 4, loss = 0.69740345\n",
      "Validation score: -0.009356\n",
      "Iteration 5, loss = 0.68612880\n",
      "Validation score: -0.003990\n",
      "Iteration 6, loss = 0.68242209\n",
      "Validation score: -0.027999\n",
      "Iteration 7, loss = 0.68017747\n",
      "Validation score: -0.020010\n",
      "Iteration 8, loss = 0.30079553\n",
      "Validation score: 0.660442\n",
      "Iteration 9, loss = 0.23242067\n",
      "Validation score: 0.578564\n",
      "Iteration 10, loss = 0.23161566\n",
      "Validation score: 0.670363\n",
      "Iteration 11, loss = 0.22849354\n",
      "Validation score: 0.656691\n",
      "Iteration 12, loss = 0.22458844\n",
      "Validation score: 0.650202\n",
      "Iteration 13, loss = 0.22633501\n",
      "Validation score: 0.622384\n",
      "Iteration 14, loss = 0.22354506\n",
      "Validation score: 0.669487\n",
      "Iteration 15, loss = 0.22150550\n",
      "Validation score: 0.684103\n",
      "Iteration 16, loss = 0.22237133\n",
      "Validation score: 0.675843\n",
      "Iteration 17, loss = 0.22049829\n",
      "Validation score: 0.678381\n",
      "Iteration 18, loss = 0.21946769\n",
      "Validation score: 0.641190\n",
      "Iteration 19, loss = 0.22064444\n",
      "Validation score: 0.623792\n",
      "Iteration 20, loss = 0.21758078\n",
      "Validation score: 0.677717\n",
      "Iteration 21, loss = 0.22079167\n",
      "Validation score: 0.687467\n",
      "Iteration 22, loss = 0.21802888\n",
      "Validation score: 0.670299\n",
      "Iteration 23, loss = 0.21932145\n",
      "Validation score: 0.648354\n",
      "Iteration 24, loss = 0.21777463\n",
      "Validation score: 0.682915\n",
      "Iteration 25, loss = 0.21672963\n",
      "Validation score: 0.689258\n",
      "Iteration 26, loss = 0.21778608\n",
      "Validation score: 0.661079\n",
      "Iteration 27, loss = 0.21486540\n",
      "Validation score: 0.690795\n",
      "Iteration 28, loss = 0.21764360\n",
      "Validation score: 0.692313\n",
      "Iteration 29, loss = 0.21699536\n",
      "Validation score: 0.694398\n",
      "Iteration 30, loss = 0.21825002\n",
      "Validation score: 0.624965\n",
      "Iteration 31, loss = 0.21484906\n",
      "Validation score: 0.686568\n",
      "Iteration 32, loss = 0.21620042\n",
      "Validation score: 0.619169\n",
      "Iteration 33, loss = 0.21619167\n",
      "Validation score: 0.642163\n",
      "Iteration 34, loss = 0.21904425\n",
      "Validation score: 0.671132\n",
      "Iteration 35, loss = 0.21636986\n",
      "Validation score: 0.696960\n",
      "Iteration 36, loss = 0.21627366\n",
      "Validation score: 0.675505\n",
      "Iteration 37, loss = 0.21603896\n",
      "Validation score: 0.653405\n",
      "Iteration 38, loss = 0.21533598\n",
      "Validation score: 0.675485\n",
      "Iteration 39, loss = 0.21507275\n",
      "Validation score: 0.658293\n",
      "Iteration 40, loss = 0.21437733\n",
      "Validation score: 0.629545\n",
      "Iteration 41, loss = 0.21594706\n",
      "Validation score: 0.671546\n",
      "Iteration 42, loss = 0.21633234\n",
      "Validation score: 0.693519\n",
      "Iteration 43, loss = 0.21609440\n",
      "Validation score: 0.686650\n",
      "Iteration 44, loss = 0.21204291\n",
      "Validation score: 0.695676\n",
      "Iteration 45, loss = 0.21521965\n",
      "Validation score: 0.676477\n",
      "Iteration 46, loss = 0.21121421\n",
      "Validation score: 0.681395\n",
      "Iteration 47, loss = 0.21533710\n",
      "Validation score: 0.699872\n",
      "Iteration 48, loss = 0.21192019\n",
      "Validation score: 0.698793\n",
      "Iteration 49, loss = 0.21255126\n",
      "Validation score: 0.699191\n",
      "Iteration 50, loss = 0.21419085\n",
      "Validation score: 0.702573\n",
      "Iteration 51, loss = 0.21377308\n",
      "Validation score: 0.681448\n",
      "Iteration 52, loss = 0.21165388\n",
      "Validation score: 0.675873\n",
      "Iteration 53, loss = 0.21125898\n",
      "Validation score: 0.683664\n",
      "Iteration 54, loss = 0.21297367\n",
      "Validation score: 0.704353\n",
      "Iteration 55, loss = 0.21200931\n",
      "Validation score: 0.682678\n",
      "Iteration 56, loss = 0.21171576\n",
      "Validation score: 0.697647\n",
      "Iteration 57, loss = 0.20966199\n",
      "Validation score: 0.699854\n",
      "Iteration 58, loss = 0.21078421\n",
      "Validation score: 0.710591\n",
      "Iteration 59, loss = 0.21225081\n",
      "Validation score: 0.590951\n",
      "Iteration 60, loss = 0.21129121\n",
      "Validation score: 0.704019\n",
      "Iteration 61, loss = 0.21095390\n",
      "Validation score: 0.692217\n",
      "Iteration 62, loss = 0.21334021\n",
      "Validation score: 0.663406\n",
      "Iteration 63, loss = 0.21275303\n",
      "Validation score: 0.683506\n",
      "Iteration 64, loss = 0.20986487\n",
      "Validation score: 0.696771\n",
      "Iteration 65, loss = 0.21055448\n",
      "Validation score: 0.681284\n",
      "Iteration 66, loss = 0.21117770\n",
      "Validation score: 0.695751\n",
      "Iteration 67, loss = 0.20964019\n",
      "Validation score: 0.702141\n",
      "Iteration 68, loss = 0.21098321\n",
      "Validation score: 0.617922\n",
      "Iteration 69, loss = 0.20980957\n",
      "Validation score: 0.657136\n",
      "Iteration 70, loss = 0.20812065\n",
      "Validation score: 0.708235\n",
      "Iteration 71, loss = 0.21000258\n",
      "Validation score: 0.655665\n",
      "Iteration 72, loss = 0.20961532\n",
      "Validation score: 0.689594\n",
      "Iteration 73, loss = 0.21120506\n",
      "Validation score: 0.704737\n",
      "Iteration 74, loss = 0.20745264\n",
      "Validation score: 0.714125\n",
      "Iteration 75, loss = 0.20875440\n",
      "Validation score: 0.712160\n",
      "Iteration 76, loss = 0.21090303\n",
      "Validation score: 0.710057\n",
      "Iteration 77, loss = 0.21020140\n",
      "Validation score: 0.707386\n",
      "Iteration 78, loss = 0.20755704\n",
      "Validation score: 0.712143\n",
      "Iteration 79, loss = 0.20797090\n",
      "Validation score: 0.646192\n",
      "Iteration 80, loss = 0.20846482\n",
      "Validation score: 0.702721\n",
      "Iteration 81, loss = 0.20938709\n",
      "Validation score: 0.710105\n",
      "Iteration 82, loss = 0.20832378\n",
      "Validation score: 0.653399\n",
      "Iteration 83, loss = 0.20963593\n",
      "Validation score: 0.681754\n",
      "Iteration 84, loss = 0.20910714\n",
      "Validation score: 0.698180\n",
      "Iteration 85, loss = 0.20887905\n",
      "Validation score: 0.688314\n",
      "Iteration 86, loss = 0.20930720\n",
      "Validation score: 0.686576\n",
      "Iteration 87, loss = 0.21103193\n",
      "Validation score: 0.714895\n",
      "Iteration 88, loss = 0.20847391\n",
      "Validation score: 0.714307\n",
      "Iteration 89, loss = 0.20782359\n",
      "Validation score: 0.714647\n",
      "Iteration 90, loss = 0.20789416\n",
      "Validation score: 0.715430\n",
      "Iteration 91, loss = 0.20660668\n",
      "Validation score: 0.699323\n",
      "Iteration 92, loss = 0.20560430\n",
      "Validation score: 0.670777\n",
      "Iteration 93, loss = 0.20764284\n",
      "Validation score: 0.696489\n",
      "Iteration 94, loss = 0.20633947\n",
      "Validation score: 0.714350\n",
      "Iteration 95, loss = 0.20500550\n",
      "Validation score: 0.699556\n",
      "Iteration 96, loss = 0.20857699\n",
      "Validation score: 0.684653\n",
      "Iteration 97, loss = 0.20780988\n",
      "Validation score: 0.633268\n",
      "Iteration 98, loss = 0.20823452\n",
      "Validation score: 0.629093\n",
      "Iteration 99, loss = 0.20849617\n",
      "Validation score: 0.695329\n",
      "Iteration 100, loss = 0.20716169\n",
      "Validation score: 0.708865\n",
      "Iteration 101, loss = 0.20741755\n",
      "Validation score: 0.651319\n",
      "Iteration 102, loss = 0.20796789\n",
      "Validation score: 0.707042\n",
      "Iteration 103, loss = 0.20813593\n",
      "Validation score: 0.712933\n",
      "Iteration 104, loss = 0.20891568\n",
      "Validation score: 0.709631\n",
      "Iteration 105, loss = 0.20772242\n",
      "Validation score: 0.715379\n",
      "Iteration 106, loss = 0.20617987\n",
      "Validation score: 0.708232\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "n\\Fin entrenamiento\n",
      "R^2 modelo: 0.7586591480942306\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "\n",
    "model_reg = MLPRegressor((100,60,30),\n",
    "                      activation='logistic',\n",
    "                      solver='sgd',\n",
    "                      random_state=42,\n",
    "                      batch_size = 'auto',\n",
    "                      max_iter=300,\n",
    "                      verbose=True,\n",
    "                      early_stopping=True,\n",
    "                      learning_rate_init = 0.2,\n",
    "                      power_t = 0.9,\n",
    "                      alpha = 0.01, \n",
    "                      shuffle=True,\n",
    "                      epsilon=1e-10,\n",
    "                      tol=1e-10,\n",
    "                      n_iter_no_change=15,\n",
    "                      validation_fraction=0.1)\n",
    "\n",
    "model_reg.fit(X_train_s,y_train)\n",
    "\n",
    "valor = model_reg.score(X_test_s,y_test)\n",
    "\n",
    "print('n\\Fin entrenamiento')\n",
    "print('R^2 modelo:' ,valor)\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### ver parametros y ajuste del modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:47:50.796254Z",
     "start_time": "2021-02-15T14:47:50.633665Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------\n",
      "R^2 del modelo:  75.866 %\n",
      "Funcion activacion:  identity\n",
      "Cantidad de capas:  identity\n",
      "Numero iteraciones:  106\n",
      "Cantidad datos usados:  1107594\n",
      "\n",
      "\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ver capas entrenadas: \n",
      "---------------------\n",
      "\n",
      "Capa N°  1\n",
      "Dimension:  (8, 100)\n",
      "\n",
      "\n",
      "[[-9.15237349e-02 -3.74483436e-02 -6.50874774e-02 -6.36353540e-02\n",
      "  -5.93822720e-02 -1.27812667e-01 -1.04961735e+00 -2.37227910e-02\n",
      "   1.10271116e-01 -6.17699447e-02 -1.03705673e-01 -7.37629704e-02\n",
      "  -3.92229727e-02 -3.83419488e-01 -4.39463805e-01 -9.19296219e-02\n",
      "  -7.81864747e-02 -7.85523741e-02 -8.96038138e-02 -8.46149470e-02\n",
      "  -5.06470508e-02 -6.66873253e-02 -8.45007472e-02 -1.05014645e+00\n",
      "  -9.25133752e-02 -9.35119562e-02 -3.94947497e-02 -8.82790565e-02\n",
      "  -8.46914187e-02 -5.77732284e-01 -7.67133993e-02 -6.72869132e-01\n",
      "  -9.89402996e-02 -5.34062904e-02  6.27908811e-01 -7.92128290e-02\n",
      "  -1.22434159e-01 -9.58329802e-02 -8.68258575e-02 -8.12698089e-02\n",
      "  -8.54357912e-02 -8.48901244e-02 -5.12385171e-02 -6.09924093e-02\n",
      "  -1.01417068e-01 -9.96617860e-02 -8.94508752e-02 -7.98885641e-02\n",
      "  -9.35882549e-02 -9.75984794e-02 -3.85482338e-02 -1.04065813e-01\n",
      "   6.93886455e-01 -8.61565858e-02 -7.01007306e-02 -6.06916037e-02\n",
      "  -8.30938061e-02 -9.10808740e-02 -2.47107973e-01 -7.94073389e-02\n",
      "  -9.05525306e-02 -9.16328155e-02  5.20856791e-01 -9.19512957e-02\n",
      "  -1.28255704e-01 -8.45907086e-02 -7.73006065e-01  9.93277801e-01\n",
      "  -9.49312471e-02 -8.04686963e-02  2.75150781e-01 -9.23852259e-02\n",
      "  -9.36979096e-02 -6.26843641e-02 -5.82974956e-02 -8.82651111e-02\n",
      "  -9.22381290e-03 -1.05492449e-01 -2.86463966e-01 -9.16090070e-02\n",
      "  -2.93538259e-02 -7.55124028e-02 -7.94356555e-02 -9.36147813e-02\n",
      "  -9.66037421e-02 -7.98583709e-02 -7.87002164e-02 -8.54063858e-02\n",
      "  -6.09388725e-02 -6.72995590e-02 -3.46031316e-01 -6.34716746e-02\n",
      "   4.78336636e-01 -7.37916824e-02 -8.49415103e-02 -9.10890496e-02\n",
      "  -7.44678181e-02 -9.20559141e-02 -1.51934123e+00 -9.31235495e-02]\n",
      " [ 7.52531935e-02  3.85533032e-02  5.30168267e-02  5.20156173e-02\n",
      "   4.89142130e-02  9.20077208e-02 -7.26533994e-01  2.94767057e-02\n",
      "   2.46572480e-01  4.99148981e-02  8.14449477e-02  6.06680422e-02\n",
      "   3.90046543e-02  7.95715090e-02  7.60253589e-02  7.63675100e-02\n",
      "   6.45395008e-02  6.44505726e-02  7.17862245e-02  7.00416565e-02\n",
      "   4.47620777e-02  5.38739474e-02  6.95875463e-02 -8.05090358e-01\n",
      "   7.54497278e-02  7.89256209e-02  3.82172076e-02  7.33152801e-02\n",
      "   7.05376830e-02 -2.14419436e-01  6.26285651e-02  5.81263144e-02\n",
      "   6.59579889e-02  4.53557526e-02  1.26681609e-01  6.49904939e-02\n",
      "   8.46660360e-02  7.55031922e-02  7.21273684e-02  6.74324461e-02\n",
      "   7.01829604e-02  7.04937896e-02  4.40602214e-02  5.11451316e-02\n",
      "   7.13155428e-02  7.92885740e-02  7.29722472e-02  6.63595046e-02\n",
      "   7.70804719e-02  7.78934187e-02  3.70397309e-02  8.15332279e-02\n",
      "   2.04555172e-02  7.20452980e-02  5.69728581e-02  4.90836307e-02\n",
      "   6.84299066e-02  7.56066746e-02 -7.09640587e-01  6.49903782e-02\n",
      "   7.55209602e-02  7.66625855e-02 -1.44116537e-01  7.56972449e-02\n",
      "   8.44691916e-02  7.02468683e-02  1.18149989e+00 -1.69069892e-01\n",
      "   7.64173714e-02  6.68619615e-02 -1.17706540e-01  7.28334648e-02\n",
      "   7.58690258e-02  5.08497977e-02  4.76642809e-02  7.16740032e-02\n",
      "   2.01944282e-02  7.45818215e-02  4.70870356e-03  7.44453397e-02\n",
      "   3.25188346e-02  6.21104938e-02  6.49376309e-02  7.40539371e-02\n",
      "   8.04360674e-02  6.55993353e-02  6.49635397e-02  7.02761576e-02\n",
      "   4.94581331e-02  5.44927158e-02  8.23302665e-02  5.21402099e-02\n",
      "  -2.11354353e-01  6.04938057e-02  7.02780296e-02  7.47712364e-02\n",
      "   6.07201114e-02  7.77703871e-02 -2.54725202e-02  7.43226421e-02]\n",
      " [-1.60636658e-02 -7.55196962e-03 -1.18977838e-02 -1.16516770e-02\n",
      "  -1.09490399e-02 -2.01532093e-02 -3.87143670e-01 -4.44549544e-03\n",
      "  -7.02992782e-01 -1.13375088e-02 -1.76970211e-02 -1.33375439e-02\n",
      "  -7.83487113e-03  1.14404587e-01  1.33542296e-01 -1.61328065e-02\n",
      "  -1.40534887e-02 -1.41116049e-02 -1.57487716e-02 -1.50586647e-02\n",
      "  -9.61740076e-03 -1.21649507e-02 -1.50365940e-02  2.86653472e-01\n",
      "  -1.61948824e-02 -1.63744284e-02 -7.76659338e-03 -1.56076692e-02\n",
      "  -1.50710722e-02  5.67976384e-01 -1.38161451e-02  2.15557494e-01\n",
      "  -1.66449114e-02 -1.00222571e-02  3.29073340e-01 -1.42163685e-02\n",
      "  -1.93678433e-02 -1.66266828e-02 -1.53941491e-02 -1.45411908e-02\n",
      "  -1.51753615e-02 -1.51017302e-02 -9.67233316e-03 -1.12330110e-02\n",
      "  -1.71330542e-02 -1.71590270e-02 -1.57523951e-02 -1.43220938e-02\n",
      "  -1.63602988e-02 -1.68762599e-02 -7.58198999e-03 -1.77506780e-02\n",
      "   1.04500721e+00 -1.52935913e-02 -1.27357274e-02 -1.11512817e-02\n",
      "  -1.48226956e-02 -1.60108334e-02  2.28562820e+00 -1.42460073e-02\n",
      "  -1.59429361e-02 -1.61008351e-02  4.07559539e-01 -1.61251538e-02\n",
      "  -2.05779169e-02 -1.50559322e-02  1.68492260e-01  1.28392831e+00\n",
      "  -1.65273582e-02 -1.44141637e-02  2.14854158e-01 -1.61347665e-02\n",
      "  -1.63548098e-02 -1.14896673e-02 -1.07666417e-02 -1.55731808e-02\n",
      "   3.56662198e-05 -1.76636315e-02  2.51546992e-01 -1.60586878e-02\n",
      "  -5.85007456e-03 -1.36227528e-02 -1.42493665e-02 -1.63127853e-02\n",
      "  -1.67945973e-02 -1.43188833e-02 -1.41353527e-02 -1.51702947e-02\n",
      "  -1.11989993e-02 -1.22671097e-02  1.00844567e-01 -1.16217549e-02\n",
      "   4.11919258e-01 -1.33443771e-02 -1.51081445e-02 -1.60024760e-02\n",
      "  -1.34526306e-02 -1.61652610e-02  5.34373972e-01 -1.62512160e-02]\n",
      " [ 2.21181855e-02  9.93641647e-03  1.81848508e-02  1.78329580e-02\n",
      "   1.67274208e-02  1.77555804e-02 -9.14941012e-02  5.24273466e-03\n",
      "  -8.07131317e-01  1.74388732e-02  2.16574189e-02  1.99678461e-02\n",
      "   1.05011905e-02  1.54288523e-01  1.64306446e-01  2.23469917e-02\n",
      "   2.07277717e-02  2.07523277e-02  2.13699037e-02  2.16261111e-02\n",
      "   1.41078775e-02  1.85696974e-02  2.15388261e-02 -2.40077518e-01\n",
      "   2.20012274e-02  2.28505408e-02  1.05907855e-02  2.20467628e-02\n",
      "   2.17259961e-02 -1.61923714e-01  2.04448271e-02  2.14284439e-01\n",
      "   1.53287402e-02  1.49986251e-02 -3.40003602e-02  2.08546501e-02\n",
      "   1.58990667e-02  2.14381454e-02  2.19310208e-02  2.12175420e-02\n",
      "   2.15810664e-02  2.17129270e-02  1.43468034e-02  1.70597993e-02\n",
      "   1.74436537e-02  2.15487923e-02  2.17546585e-02  2.10359843e-02\n",
      "   2.22607375e-02  2.15714438e-02  1.03004118e-02  2.12964445e-02\n",
      "   1.89977130e+00  2.19485362e-02  1.92599775e-02  1.71730388e-02\n",
      "   2.13659750e-02  2.22624097e-02 -9.11667790e-01  2.08439158e-02\n",
      "   2.23528994e-02  2.24846000e-02 -1.52290083e-01  2.21514493e-02\n",
      "   1.69562659e-02  2.16740689e-02  4.38811764e-01 -9.64956637e-02\n",
      "   2.19477390e-02  2.11210868e-02 -3.46821300e-02  2.12642071e-02\n",
      "   2.19181261e-02  1.76455933e-02  1.64864595e-02  2.15937246e-02\n",
      "  -1.19796012e-04  1.76321052e-02  4.07649896e-02  2.18135201e-02\n",
      "   7.26554318e-03  2.02788690e-02  2.08384759e-02  2.13476605e-02\n",
      "   2.27399166e-02  2.09386421e-02  2.08094992e-02  2.15970543e-02\n",
      "   1.72103350e-02  1.86979500e-02  1.45984256e-01  1.77707460e-02\n",
      "  -1.69414985e-01  1.99574631e-02  2.16787504e-02  2.20217767e-02\n",
      "   2.00729301e-02  2.27242920e-02  4.85789396e-01  2.14778493e-02]\n",
      " [ 1.03601504e-02 -2.50492187e-03  4.84415722e-03  4.44964678e-03\n",
      "   3.49274102e-03  2.02775539e-02  4.27521643e-02 -5.71798918e-03\n",
      "  -5.79954810e-02  4.29999152e-03  1.34033546e-02  6.51880711e-03\n",
      "  -1.95227962e-03  1.29758900e-01  1.54970235e-01  1.02964696e-02\n",
      "   7.42321960e-03  7.60479441e-03  1.04292993e-02  8.77263071e-03\n",
      "   9.52706703e-04  5.34476057e-03  8.83666219e-03 -1.04682238e-02\n",
      "   1.07052346e-02  1.03281231e-02 -1.85845946e-03  9.51540807e-03\n",
      "   8.65914032e-03 -1.18870648e+00  7.29332842e-03  2.19872193e-01\n",
      "   1.62498444e-02  2.01478494e-03 -2.09210460e-02  7.73981484e-03\n",
      "   2.07544397e-02  1.18396972e-02  9.17574110e-03  8.00650886e-03\n",
      "   9.10226052e-03  8.75939909e-03  1.40364070e-03  3.53894406e-03\n",
      "   1.53632238e-02  1.27640160e-02  1.00951150e-02  7.66387612e-03\n",
      "   1.08133070e-02  1.22519933e-02 -1.92640372e-03  1.39396672e-02\n",
      "  -3.06865768e-01  8.90785048e-03  5.96781623e-03  4.02181414e-03\n",
      "   8.55164336e-03  1.01320167e-02 -1.25978080e-01  7.84804760e-03\n",
      "   9.90634242e-03  1.01099457e-02  1.37149296e-03  1.04525477e-02\n",
      "   2.28154095e-02  8.69777727e-03 -5.05004795e-01 -1.56948841e-01\n",
      "   1.13131513e-02  7.79187943e-03 -3.86143092e-03  1.11610411e-02\n",
      "   1.10566604e-02  4.38588620e-03  3.43267764e-03  9.89739995e-03\n",
      "  -8.19049641e-03  1.60329226e-02 -1.70310508e-01  1.06186979e-02\n",
      "  -4.33422373e-03  6.89789892e-03  7.87145905e-03  1.14088057e-02\n",
      "   1.12446503e-02  7.88031877e-03  7.53167865e-03  9.06602151e-03\n",
      "   4.04335372e-03  5.42075343e-03  1.11280390e-01  4.30404345e-03\n",
      "   2.65384722e-02  6.60910500e-03  8.82576967e-03  1.03413541e-02\n",
      "   6.83160081e-03  1.00097333e-02  3.99815498e-01  1.12224483e-02]\n",
      " [-9.44298435e-02 -5.99449243e-02 -7.26878414e-02 -7.17913939e-02\n",
      "  -6.87059789e-02 -1.08632013e-01  2.63440622e+00 -5.08530301e-02\n",
      "  -8.94146188e-02 -6.93636363e-02 -1.00059496e-01 -8.04673703e-02\n",
      "  -6.00952579e-02 -6.54948194e-02 -5.75866353e-02 -9.56179305e-02\n",
      "  -8.42732573e-02 -8.40731797e-02 -9.06838404e-02 -8.95822320e-02\n",
      "  -6.53268195e-02 -7.33550260e-02 -8.90561981e-02  2.55345684e+00\n",
      "  -9.44935870e-02 -9.83469222e-02 -5.91701820e-02 -9.27349861e-02\n",
      "  -9.02038940e-02  6.06448196e-01 -8.22151603e-02 -1.67193946e-02\n",
      "  -8.26412779e-02 -6.52903784e-02 -5.33834240e-01 -8.46050415e-02\n",
      "  -1.00310315e-01 -9.42739397e-02 -9.16459145e-02 -8.71425168e-02\n",
      "  -8.95502268e-02 -9.00967136e-02 -6.41563512e-02 -7.13327964e-02\n",
      "  -8.87257122e-02 -9.75886853e-02 -9.20794208e-02 -8.61732608e-02\n",
      "  -9.61228848e-02 -9.63550228e-02 -5.77110817e-02 -9.94961297e-02\n",
      "  -2.58270428e+00 -9.17071920e-02 -7.65597523e-02 -6.85866124e-02\n",
      "  -8.79237187e-02 -9.48818814e-02  2.49493489e+00 -8.45254680e-02\n",
      "  -9.49272773e-02 -9.60201449e-02  1.23487899e-01 -9.48392521e-02\n",
      "  -9.91513387e-02 -8.98577339e-02  6.34949180e-01 -6.19585531e-01\n",
      "  -9.53909876e-02 -8.66491143e-02  1.82200304e-01 -9.16470890e-02\n",
      "  -9.48138610e-02 -7.04489668e-02 -6.72131147e-02 -9.08001797e-02\n",
      "  -3.93307922e-02 -9.20390467e-02 -3.25490166e-01 -9.34244222e-02\n",
      "  -5.37063597e-02 -8.18673103e-02 -8.44571646e-02 -9.27999361e-02\n",
      "  -9.94831417e-02 -8.51834864e-02 -8.46853435e-02 -8.96712099e-02\n",
      "  -6.89964464e-02 -7.40403036e-02 -7.12826445e-02 -7.20575075e-02\n",
      "   1.85586261e-01 -8.01924143e-02 -8.98344653e-02 -9.38809572e-02\n",
      "  -8.03315039e-02 -9.72721607e-02  8.63078402e-02 -9.30827481e-02]\n",
      " [-1.65864354e-02 -1.05723266e-02 -1.24187249e-02 -1.22329635e-02\n",
      "  -1.16294220e-02 -1.71032979e-02  7.20570425e-01 -8.36044041e-03\n",
      "  -1.73254483e+00 -1.17144927e-02 -1.69405293e-02 -1.40230571e-02\n",
      "  -1.04902704e-02  3.16770131e-02  4.03932041e-02 -1.68989156e-02\n",
      "  -1.47861400e-02 -1.47049119e-02 -1.57247610e-02 -1.57971772e-02\n",
      "  -1.11963810e-02 -1.25328493e-02 -1.56579013e-02  8.72299971e-01\n",
      "  -1.65168438e-02 -1.75299490e-02 -1.00499874e-02 -1.63939140e-02\n",
      "  -1.59578645e-02  1.76872140e-01 -1.43188814e-02  8.74201467e-02\n",
      "  -1.36023760e-02 -1.10572821e-02 -1.37450831e-01 -1.48021739e-02\n",
      "  -1.59174439e-02 -1.61172471e-02 -1.61969111e-02 -1.53678482e-02\n",
      "  -1.57320255e-02 -1.59134642e-02 -1.08283611e-02 -1.22480812e-02\n",
      "  -1.45495061e-02 -1.67920129e-02 -1.61212967e-02 -1.51868691e-02\n",
      "  -1.68986444e-02 -1.66299870e-02 -9.67305032e-03 -1.69110967e-02\n",
      "  -1.57627816e-01 -1.62677622e-02 -1.31996077e-02 -1.15371708e-02\n",
      "  -1.54576694e-02 -1.67661081e-02  1.11491484e+00 -1.47775378e-02\n",
      "  -1.68129669e-02 -1.70411480e-02  1.15990668e-01 -1.66738642e-02\n",
      "  -1.51389160e-02 -1.58713054e-02  7.24129228e-01 -1.00492074e-01\n",
      "  -1.65062616e-02 -1.52814217e-02  6.98520770e-02 -1.57142889e-02\n",
      "  -1.64952718e-02 -1.19334212e-02 -1.13118284e-02 -1.58531863e-02\n",
      "  -6.06862954e-03 -1.50027845e-02  4.52059307e+00 -1.63149493e-02\n",
      "  -8.96943394e-03 -1.42972079e-02 -1.47555843e-02 -1.59446206e-02\n",
      "  -1.75678069e-02 -1.49264289e-02 -1.48635561e-02 -1.57680145e-02\n",
      "  -1.16449841e-02 -1.26738993e-02  2.60613663e-02 -1.22929927e-02\n",
      "   1.96574805e-01 -1.39600163e-02 -1.58311794e-02 -1.64869385e-02\n",
      "  -1.39499594e-02 -1.73591320e-02  2.00946728e-01 -1.60877498e-02]\n",
      " [ 6.17510257e-02  2.21431454e-02  4.15830398e-02  4.05106594e-02\n",
      "   3.74400805e-02  8.71688523e-02  5.67282800e-01  1.19163425e-02\n",
      "   2.18710797e+00  3.90966742e-02  7.02990549e-02  4.81410207e-02\n",
      "   2.33770107e-02  1.35698705e-01  1.46191967e-01  6.22186493e-02\n",
      "   5.15367240e-02  5.17543374e-02  5.99656030e-02  5.64959678e-02\n",
      "   3.13653483e-02  4.27274415e-02  5.63505598e-02  6.27662236e-01\n",
      "   6.24062497e-02  6.36529303e-02  2.33684519e-02  5.93742137e-02\n",
      "   5.66116445e-02  5.71102956e-01  5.03109840e-02  1.69580694e-01\n",
      "   6.49962812e-02  3.32679884e-02 -2.62435889e-01  5.22516261e-02\n",
      "   8.24396424e-02  6.43904779e-02  5.82369517e-02  5.39501216e-02\n",
      "   5.70544264e-02  5.67339631e-02  3.16975230e-02  3.87280757e-02\n",
      "   6.72290165e-02  6.76158172e-02  6.00751574e-02  5.28888443e-02\n",
      "   6.34005951e-02  6.60789500e-02  2.26537139e-02  7.06949684e-02\n",
      "  -5.07723433e-01  5.77931829e-02  4.53101616e-02  3.82849831e-02\n",
      "   5.52810471e-02  6.15506908e-02  5.66279382e-01  5.23901658e-02\n",
      "   6.11797702e-02  6.20795757e-02 -3.05552014e-01  6.21126595e-02\n",
      "   8.43194145e-02  5.65054856e-02  3.36241532e-01 -2.84666927e-03\n",
      "   6.40154825e-02  5.33409801e-02 -1.64993996e-01  6.18045114e-02\n",
      "   6.32102736e-02  3.97672605e-02  3.66301392e-02  5.90922659e-02\n",
      "   1.91598712e-03  7.02513937e-02  3.33099499e+00  6.16887782e-02\n",
      "   1.60451188e-02  4.94662035e-02  5.23986632e-02  6.28152383e-02\n",
      "   6.58960695e-02  5.27691458e-02  5.19266132e-02  5.70515635e-02\n",
      "   3.85020375e-02  4.31883227e-02  1.28752386e-01  4.04033621e-02\n",
      "  -3.38207891e-01  4.81506497e-02  5.67264178e-02  6.14179443e-02\n",
      "   4.86041426e-02  6.25299641e-02  1.95852741e-01  6.26003134e-02]]\n",
      "\n",
      "\n",
      "**********************\n",
      "Capa N°  2\n",
      "Dimension:  (100, 60)\n",
      "\n",
      "\n",
      "[[-0.03675469 -0.02786945 -0.02585288 ... -0.03088419 -0.0309424\n",
      "  -0.0320107 ]\n",
      " [-0.0217011  -0.01859821 -0.01753389 ... -0.02006667 -0.02005103\n",
      "  -0.02057825]\n",
      " [-0.02810195 -0.02238875 -0.02091542 ... -0.02452964 -0.02454939\n",
      "  -0.02531109]\n",
      " ...\n",
      " [-0.03752412 -0.02839176 -0.02632658 ... -0.03148234 -0.03154307\n",
      "  -0.03263802]\n",
      " [-0.16243616 -0.0875827  -0.07726456 ... -0.10525198 -0.10618512\n",
      "  -0.11261466]\n",
      " [-0.03662939 -0.02775416 -0.02574599 ... -0.03075797 -0.03081643\n",
      "  -0.03188102]]\n",
      "\n",
      "\n",
      "**********************\n",
      "Capa N°  3\n",
      "Dimension:  (60, 30)\n",
      "\n",
      "\n",
      "[[-0.05864296 -0.10059282 -0.09694928 ... -0.08776518 -0.06037919\n",
      "  -0.08685639]\n",
      " [-0.03135251 -0.05398887 -0.05195429 ... -0.04678671 -0.03219985\n",
      "  -0.04627668]\n",
      " [-0.02761573 -0.04737906 -0.04559154 ... -0.04104021 -0.02833838\n",
      "  -0.04059105]\n",
      " ...\n",
      " [-0.03773818 -0.06515725 -0.06271528 ... -0.05653169 -0.0387971\n",
      "  -0.05592115]\n",
      " [-0.03809453 -0.0657774  -0.06331332 ... -0.05707467 -0.03916552\n",
      "  -0.05645867]\n",
      " [-0.04039297 -0.06975297 -0.06714708 ... -0.0605546  -0.04153915\n",
      "  -0.05990356]]\n",
      "\n",
      "\n",
      "**********************\n",
      "Capa N°  4\n",
      "Dimension:  (30, 1)\n",
      "\n",
      "\n",
      "[[-0.57432557]\n",
      " [-0.91796336]\n",
      " [-0.89306699]\n",
      " [-0.77034463]\n",
      " [-0.74922138]\n",
      " [-0.68074646]\n",
      " [-0.72529329]\n",
      " [-0.70031609]\n",
      " [-0.78526344]\n",
      " [-0.69234302]\n",
      " [-0.71970023]\n",
      " [-0.82952394]\n",
      " [-0.7802191 ]\n",
      " [-0.69525699]\n",
      " [-0.88004063]\n",
      " [-0.64865393]\n",
      " [-0.9063681 ]\n",
      " [-0.57976711]\n",
      " [-0.83382807]\n",
      " [-0.86758031]\n",
      " [-0.66383395]\n",
      " [-0.87418981]\n",
      " [-0.72703716]\n",
      " [-0.78542687]\n",
      " [-0.72905965]\n",
      " [-0.7598367 ]\n",
      " [-0.63152234]\n",
      " [-0.82832307]\n",
      " [-0.59245571]\n",
      " [-0.82167462]]\n",
      "\n",
      "\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "# Ver detalles de parametros y ajustes del modelo entrenado\n",
    "\n",
    "print('\\n\\n----------------------')\n",
    "print('R^2 del modelo: ',np.round(valor*100.0,3),'%')\n",
    "print('Funcion activacion: ',model_reg.out_activation_)\n",
    "print('Cantidad de capas: ',model_reg.n_layers_)\n",
    "print('Numero iteraciones: ',model_reg.n_iter_)\n",
    "print('Cantidad datos usados: ',model_reg.t_)\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Curvas de Error (durante entrenamiento)\n",
    "\n",
    "loss_curve = model_reg.loss_curve_\n",
    "\n",
    "print('\\n\\n----------------------')\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.title('Curva de Error')\n",
    "plt.xlabel('Interacion \"i\"')\n",
    "plt.ylabel('Error (Loss)')\n",
    "plt.plot(loss_curve)\n",
    "plt.show()\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Pesos de capas de neuronas\n",
    "\n",
    "capas = model_reg.coefs_\n",
    "\n",
    "i = 1\n",
    "print('\\n\\nVer capas entrenadas: ')\n",
    "print('---------------------\\n')\n",
    "for capa in capas:\n",
    "    print('Capa N° ',i)\n",
    "    print('Dimension: ',capa.shape)\n",
    "    print('\\n')\n",
    "    print(capa)\n",
    "    i +=1\n",
    "    print('\\n\\n**********************')\n",
    "    \n",
    "    \n",
    "# fin     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### prediccion/validacion con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:47:55.992752Z",
     "start_time": "2021-02-15T14:47:55.950901Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R^2 de modelo: 75.87 %\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "[2.041 1.743 2.479 5.178 0.864 0.871 1.644 2.65  1.411 3.701]\n",
      "\n",
      "Valores Orginales: \n",
      "[1.531 1.47  3.277 5.    0.919 2.194 1.371 2.938 1.116 1.838]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediccion\n",
    "\n",
    "valor = model_reg.score(X_test_s,y_test)\n",
    "print('\\nR^2 de modelo:' ,np.round(valor*100.0,2),'%')\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test_s.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(model_reg.predict(X_test_s[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Regresor con Pipeline + GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T15:38:50.166188Z",
     "start_time": "2021-02-15T15:34:55.488524Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# modelo con pipeline\n",
    "\n",
    "# cargar datos\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# cargar datos\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "print('\\n')\n",
    "print('Dim X train: ',X_train_full.shape)\n",
    "print('Dim Y train: ',y_train_full.shape)\n",
    "print('\\n')\n",
    "\n",
    "# Definir modelo de regression por ANN (modelo inicial)\n",
    "model = MLPRegressor((100,60,30),\n",
    "                      activation='logistic',\n",
    "                      solver='sgd',\n",
    "                      random_state=42,\n",
    "                      batch_size = 'auto',\n",
    "                      max_iter=300,\n",
    "                      verbose=True,\n",
    "                      early_stopping=True,\n",
    "                      learning_rate_init = 0.2,\n",
    "                      power_t = 0.9,\n",
    "                      alpha = 0.01, \n",
    "                      shuffle=True,\n",
    "                      epsilon=1e-10,\n",
    "                      tol=1e-8,\n",
    "                      n_iter_no_change=15,\n",
    "                      validation_fraction=0.2)\n",
    "\n",
    "# Definir Pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('reg', model)])\n",
    "\n",
    "# Grilla de busqueda de mejores parametros para la ANN\n",
    "param_grid = {\n",
    "    'reg__hidden_layer_sizes': [(50,20,10),(80,20),(20,10)],\n",
    "    'reg__activation': ['tanh','logistic'],\n",
    "    'reg__alpha': [0.0001, 0.05]}\n",
    "\n",
    "# Gridsearch CV\n",
    "search = GridSearchCV(pipe, param_grid,cv=3)\n",
    "\n",
    "# entrenar modelos y busqueda mediante GridSearchCV\n",
    "search.fit(X_train_full, y_train_full)\n",
    "\n",
    "# ver ajuste del modelo entrenado\n",
    "valor = search.score(X_test,y_test)\n",
    "\n",
    "# Mejores parametros\n",
    "print('\\n\\n----------------------------')\n",
    "print('Mejores parametros: ')\n",
    "print(search.best_params_)\n",
    "print('R^2 modelo:' ,np.round(valor*100.0,3),'%')\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T16:02:21.811918Z",
     "start_time": "2021-02-15T16:02:21.783993Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R^2 de modelo: 78.66 %\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "[2.554 1.379 2.143 1.509 1.075 1.453 1.668 1.24  1.06  2.201]\n",
      "\n",
      "Valores Orginales: \n",
      "[0.55  2.033 1.984 1.421 0.987 1.431 1.628 0.948 1.073 2.178]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediccion\n",
    "\n",
    "valor = search.score(X_test,y_test)\n",
    "print('\\nR^2 de modelo:' ,np.round(valor*100.0,2),'%')\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(search.predict(X_test[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regresor con Keras + Scikit (Con GridSearchCV & Opcion Multicapas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br><br><b>Importante !!!</b>: Es vital tener en cuenta que si lo que se quiere es convertir un modelo <b>Keras</b> a uno tipo <b>Scikit</b> para poder aplicar sus bondades como es el GridSearchCV, no se puede usar todos los modelos de Keras, el recomendado o unica opcion es usar <b>keras.Sequential()</b> esto debido a que para el Clasificador le brinda acceso al calculo mediante <b>modelo.predic_proba</b> y <b>modelo.predic</b>, estos son fundamentales en el calculo una vez entrenado (fit) el modelo a los datos.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Opcion 01 - con keras.Sequential() - (Sin GridSearchCV & Sin Opcion Multicapa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:59:37.115793Z",
     "start_time": "2021-02-15T21:58:10.298841Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dim X train:  (15480, 8)\n",
      "Dim Y train:  (15480,)\n",
      "\n",
      "\n",
      "Train on 12384 samples, validate on 3096 samples\n",
      "Epoch 1/100\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.9750 - mse: 0.8674\n",
      "Epoch 00001: val_loss improved from inf to 0.60488, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 90us/sample - loss: 0.9597 - mse: 0.8520 - val_loss: 0.6049 - val_mse: 0.4955\n",
      "Epoch 2/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.5982 - mse: 0.4893\n",
      "Epoch 00002: val_loss improved from 0.60488 to 0.49296, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.5975 - mse: 0.4886 - val_loss: 0.4930 - val_mse: 0.3850\n",
      "Epoch 3/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.5178 - mse: 0.4103\n",
      "Epoch 00003: val_loss improved from 0.49296 to 0.46991, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.5167 - mse: 0.4092 - val_loss: 0.4699 - val_mse: 0.3632\n",
      "Epoch 4/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.5020 - mse: 0.3959\n",
      "Epoch 00004: val_loss improved from 0.46991 to 0.45367, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 76us/sample - loss: 0.4958 - mse: 0.3897 - val_loss: 0.4537 - val_mse: 0.3483\n",
      "Epoch 5/100\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.4734 - mse: 0.3685- ETA: 0s - loss: 0.4638 - mse:\n",
      "Epoch 00005: val_loss improved from 0.45367 to 0.44731, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.4747 - mse: 0.3699 - val_loss: 0.4473 - val_mse: 0.3430\n",
      "Epoch 6/100\n",
      "12000/12384 [============================>.] - ETA: 0s - loss: 0.4664 - mse: 0.362 - ETA: 0s - loss: 0.4621 - mse: 0.3583\n",
      "Epoch 00006: val_loss improved from 0.44731 to 0.44570, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 75us/sample - loss: 0.4654 - mse: 0.3617 - val_loss: 0.4457 - val_mse: 0.3426\n",
      "Epoch 7/100\n",
      "11936/12384 [===========================>..] - ETA: 0s - loss: 0.4583 - mse: 0.3557\n",
      "Epoch 00007: val_loss improved from 0.44570 to 0.43367, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 76us/sample - loss: 0.4590 - mse: 0.3564 - val_loss: 0.4337 - val_mse: 0.3317\n",
      "Epoch 8/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.4518 - mse: 0.3504- ETA: 0s - loss: 0.4396 - mse: 0 - ETA: 0s - loss: 0.4497 - mse: 0.3\n",
      "Epoch 00008: val_loss improved from 0.43367 to 0.43358, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.4524 - mse: 0.3509 - val_loss: 0.4336 - val_mse: 0.3327\n",
      "Epoch 9/100\n",
      "12320/12384 [============================>.] - ETA: 0s - loss: 0.4471 - mse: 0.3468\n",
      "Epoch 00009: val_loss improved from 0.43358 to 0.42394, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.4468 - mse: 0.3465 - val_loss: 0.4239 - val_mse: 0.3242\n",
      "Epoch 10/100\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.4409 - mse: 0.3416\n",
      "Epoch 00010: val_loss did not improve from 0.42394\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.4413 - mse: 0.3421 - val_loss: 0.4472 - val_mse: 0.3486\n",
      "Epoch 11/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.4379 - mse: 0.3398\n",
      "Epoch 00011: val_loss improved from 0.42394 to 0.41826, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.4373 - mse: 0.3392 - val_loss: 0.4183 - val_mse: 0.3207\n",
      "Epoch 12/100\n",
      "12160/12384 [============================>.] - ETA: 0s - loss: 0.4336 - mse: 0.3365\n",
      "Epoch 00012: val_loss did not improve from 0.41826\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.4329 - mse: 0.3358 - val_loss: 0.4234 - val_mse: 0.3268\n",
      "Epoch 13/100\n",
      "12128/12384 [============================>.] - ETA: 0s - loss: 0.4243 - mse: 0.3282\n",
      "Epoch 00013: val_loss improved from 0.41826 to 0.41265, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.4273 - mse: 0.3312 - val_loss: 0.4126 - val_mse: 0.3170\n",
      "Epoch 14/100\n",
      "12352/12384 [============================>.] - ETA: 0s - loss: 0.4229 - mse: 0.3278- ETA: 0s - loss: 0.4035 - mse: 0.308 - ETA: 0s - loss: 0.4129 - mse:\n",
      "Epoch 00014: val_loss improved from 0.41265 to 0.40731, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.4224 - mse: 0.3273 - val_loss: 0.4073 - val_mse: 0.3128\n",
      "Epoch 15/100\n",
      "12320/12384 [============================>.] - ETA: 0s - loss: 0.4191 - mse: 0.3250- ETA: 0s - loss: 0.4222 - mse: \n",
      "Epoch 00015: val_loss improved from 0.40731 to 0.40208, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.4191 - mse: 0.3250 - val_loss: 0.4021 - val_mse: 0.3085\n",
      "Epoch 16/100\n",
      "12288/12384 [============================>.] - ETA: 0s - loss: 0.4156 - mse: 0.3224\n",
      "Epoch 00016: val_loss did not improve from 0.40208\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.4150 - mse: 0.3218 - val_loss: 0.4023 - val_mse: 0.3096\n",
      "Epoch 17/100\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.4143 - mse: 0.3220- ETA: 0s - loss: 0.4055 - mse: 0\n",
      "Epoch 00017: val_loss improved from 0.40208 to 0.39167, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.4101 - mse: 0.3179 - val_loss: 0.3917 - val_mse: 0.2999\n",
      "Epoch 18/100\n",
      "11488/12384 [==========================>...] - ETA: 0s - loss: 0.4090 - mse: 0.317 - ETA: 0s - loss: 0.4072 - mse: 0.3158\n",
      "Epoch 00018: val_loss improved from 0.39167 to 0.39078, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.4059 - mse: 0.3146 - val_loss: 0.3908 - val_mse: 0.2999\n",
      "Epoch 19/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.4018 - mse: 0.3113\n",
      "Epoch 00019: val_loss did not improve from 0.39078\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.4012 - mse: 0.3107 - val_loss: 0.3944 - val_mse: 0.3045\n",
      "Epoch 20/100\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.3992 - mse: 0.3096- ETA: 0s - loss: 0.3802 - mse\n",
      "Epoch 00020: val_loss improved from 0.39078 to 0.38571, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3987 - mse: 0.3092 - val_loss: 0.3857 - val_mse: 0.2967\n",
      "Epoch 21/100\n",
      "11968/12384 [===========================>..] - ETA: 0s - loss: 0.3942 - mse: 0.305 - ETA: 0s - loss: 0.3940 - mse: 0.3054\n",
      "Epoch 00021: val_loss did not improve from 0.38571\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3951 - mse: 0.3065 - val_loss: 0.3927 - val_mse: 0.3045\n",
      "Epoch 22/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.3922 - mse: 0.3045- ETA: 0s - loss: 0.3933 - ms\n",
      "Epoch 00022: val_loss improved from 0.38571 to 0.38404, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 77us/sample - loss: 0.3934 - mse: 0.3057 - val_loss: 0.3840 - val_mse: 0.2967\n",
      "Epoch 23/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.3888 - mse: 0.3018\n",
      "Epoch 00023: val_loss did not improve from 0.38404\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3884 - mse: 0.3015 - val_loss: 0.3840 - val_mse: 0.2976\n",
      "Epoch 24/100\n",
      "12096/12384 [============================>.] - ETA: 0s - loss: 0.3853 - mse: 0.2992- ETA: 0s - loss: 0.3951 - mse: 0. - ETA: 0s - loss: 0.3869 - mse: 0.3\n",
      "Epoch 00024: val_loss improved from 0.38404 to 0.37646, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3856 - mse: 0.2995 - val_loss: 0.3765 - val_mse: 0.2908\n",
      "Epoch 25/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3803 - mse: 0.2950\n",
      "Epoch 00025: val_loss did not improve from 0.37646\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3827 - mse: 0.2974 - val_loss: 0.3831 - val_mse: 0.2983\n",
      "Epoch 26/100\n",
      "12000/12384 [============================>.] - ETA: 0s - loss: 0.3810 - mse: 0.2967\n",
      "Epoch 00026: val_loss improved from 0.37646 to 0.37173, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3822 - mse: 0.2978 - val_loss: 0.3717 - val_mse: 0.2877\n",
      "Epoch 27/100\n",
      "11776/12384 [===========================>..] - ETA: 0s - loss: 0.3823 - mse: 0.2988\n",
      "Epoch 00027: val_loss improved from 0.37173 to 0.36679, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3803 - mse: 0.2967 - val_loss: 0.3668 - val_mse: 0.2837\n",
      "Epoch 28/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.3797 - mse: 0.2969- ETA: 0s - loss: 0.3717 - mse: 0.28\n",
      "Epoch 00028: val_loss did not improve from 0.36679\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3779 - mse: 0.2951 - val_loss: 0.3678 - val_mse: 0.2854\n",
      "Epoch 29/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3764 - mse: 0.2944\n",
      "Epoch 00029: val_loss did not improve from 0.36679\n",
      "12384/12384 [==============================] - 1s 65us/sample - loss: 0.3764 - mse: 0.2944 - val_loss: 0.3742 - val_mse: 0.2927\n",
      "Epoch 30/100\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.3735 - mse: 0.2923\n",
      "Epoch 00030: val_loss improved from 0.36679 to 0.36438, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 65us/sample - loss: 0.3727 - mse: 0.2916 - val_loss: 0.3644 - val_mse: 0.2836\n",
      "Epoch 31/100\n",
      "12160/12384 [============================>.] - ETA: 0s - loss: 0.3738 - mse: 0.2934- ETA: 0s - loss: 0.3704 - mse: 0.\n",
      "Epoch 00031: val_loss did not improve from 0.36438\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3730 - mse: 0.2926 - val_loss: 0.3703 - val_mse: 0.2904\n",
      "Epoch 32/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.3708 - mse: 0.2912\n",
      "Epoch 00032: val_loss did not improve from 0.36438\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3707 - mse: 0.2911 - val_loss: 0.3691 - val_mse: 0.2898\n",
      "Epoch 33/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.3672 - mse: 0.2882\n",
      "Epoch 00033: val_loss improved from 0.36438 to 0.35374, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3678 - mse: 0.2889 - val_loss: 0.3537 - val_mse: 0.2752\n",
      "Epoch 34/100\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.3583 - mse: 0.2801\n",
      "Epoch 00034: val_loss did not improve from 0.35374\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3629 - mse: 0.2847 - val_loss: 0.3591 - val_mse: 0.2813\n",
      "Epoch 35/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.3648 - mse: 0.2873\n",
      "Epoch 00035: val_loss did not improve from 0.35374\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3642 - mse: 0.2868 - val_loss: 0.3767 - val_mse: 0.2996\n",
      "Epoch 36/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3653 - mse: 0.2886\n",
      "Epoch 00036: val_loss did not improve from 0.35374\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3634 - mse: 0.2867 - val_loss: 0.3627 - val_mse: 0.2864\n",
      "Epoch 37/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.3590 - mse: 0.2830\n",
      "Epoch 00037: val_loss did not improve from 0.35374\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3599 - mse: 0.2839 - val_loss: 0.3547 - val_mse: 0.2791\n",
      "Epoch 38/100\n",
      "12160/12384 [============================>.] - ETA: 0s - loss: 0.3733 - mse: 0.2979\n",
      "Epoch 00038: val_loss did not improve from 0.35374\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3720 - mse: 0.2966 - val_loss: 0.3790 - val_mse: 0.3040\n",
      "Epoch 39/100\n",
      "12128/12384 [============================>.] - ETA: 0s - loss: 0.3654 - mse: 0.2907\n",
      "Epoch 00039: val_loss improved from 0.35374 to 0.35141, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3643 - mse: 0.2896 - val_loss: 0.3514 - val_mse: 0.2772\n",
      "Epoch 40/100\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.3581 - mse: 0.2842- ETA: 0s - loss: 0.3469 - mse: - ETA: 0s - loss: 0.3595 - mse: 0.2857\n",
      "Epoch 00040: val_loss did not improve from 0.35141\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3600 - mse: 0.2861 - val_loss: 0.3679 - val_mse: 0.2944\n",
      "Epoch 41/100\n",
      "11936/12384 [===========================>..] - ETA: 0s - loss: 0.3560 - mse: 0.2828\n",
      "Epoch 00041: val_loss did not improve from 0.35141\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3575 - mse: 0.2843 - val_loss: 0.3537 - val_mse: 0.2808\n",
      "Epoch 42/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3527 - mse: 0.2801- ETA: 0s - loss: 0.3514 - mse: 0.27\n",
      "Epoch 00042: val_loss did not improve from 0.35141\n",
      "12384/12384 [==============================] - 1s 65us/sample - loss: 0.3556 - mse: 0.2831 - val_loss: 0.3723 - val_mse: 0.3001\n",
      "Epoch 43/100\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.3553 - mse: 0.2833\n",
      "Epoch 00043: val_loss improved from 0.35141 to 0.34369, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 64us/sample - loss: 0.3542 - mse: 0.2823 - val_loss: 0.3437 - val_mse: 0.2721\n",
      "Epoch 44/100\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.3462 - mse: 0.2749- ETA: 0s - loss: 0.3420 - mse: 0.27 - ETA: 0s - loss: 0.3451 - mse: 0.273\n",
      "Epoch 00044: val_loss improved from 0.34369 to 0.34082, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3520 - mse: 0.2807 - val_loss: 0.3408 - val_mse: 0.2699\n",
      "Epoch 45/100\n",
      "12224/12384 [============================>.] - ETA: 0s - loss: 0.3509 - mse: 0.2803- ETA: 0s - loss: 0.3546 - mse: 0\n",
      "Epoch 00045: val_loss did not improve from 0.34082\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3505 - mse: 0.2798 - val_loss: 0.3408 - val_mse: 0.2705\n",
      "Epoch 46/100\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.3473 - mse: 0.2773- ETA: 0s - loss: 0.3429 - mse: 0.\n",
      "Epoch 00046: val_loss did not improve from 0.34082\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3475 - mse: 0.2774 - val_loss: 0.3462 - val_mse: 0.2765\n",
      "Epoch 47/100\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.3475 - mse: 0.2780\n",
      "Epoch 00047: val_loss did not improve from 0.34082\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3479 - mse: 0.2784 - val_loss: 0.3531 - val_mse: 0.2840\n",
      "Epoch 48/100\n",
      "12160/12384 [============================>.] - ETA: 0s - loss: 0.3459 - mse: 0.2770\n",
      "Epoch 00048: val_loss did not improve from 0.34082\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3453 - mse: 0.2764 - val_loss: 0.3527 - val_mse: 0.2841\n",
      "Epoch 49/100\n",
      "11904/12384 [===========================>..] - ETA: 0s - loss: 0.3433 - mse: 0.2749- ETA: 0s - loss: 0.3420 - mse: 0\n",
      "Epoch 00049: val_loss improved from 0.34082 to 0.33651, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3430 - mse: 0.2747 - val_loss: 0.3365 - val_mse: 0.2684\n",
      "Epoch 50/100\n",
      "12224/12384 [============================>.] - ETA: 0s - loss: 0.3419 - mse: 0.2741\n",
      "Epoch 00050: val_loss did not improve from 0.33651\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3417 - mse: 0.2740 - val_loss: 0.3413 - val_mse: 0.2738\n",
      "Epoch 51/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.3419 - mse: 0.2746- ETA: 0s - loss: 0.3474 - ms\n",
      "Epoch 00051: val_loss did not improve from 0.33651\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3414 - mse: 0.2741 - val_loss: 0.3472 - val_mse: 0.2802\n",
      "Epoch 52/100\n",
      "11904/12384 [===========================>..] - ETA: 0s - loss: 0.3424 - mse: 0.2757\n",
      "Epoch 00052: val_loss improved from 0.33651 to 0.33579, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3413 - mse: 0.2746 - val_loss: 0.3358 - val_mse: 0.2694\n",
      "Epoch 53/100\n",
      "11904/12384 [===========================>..] - ETA: 0s - loss: 0.3420 - mse: 0.2758\n",
      "Epoch 00053: val_loss did not improve from 0.33579\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3393 - mse: 0.2731 - val_loss: 0.3363 - val_mse: 0.2703\n",
      "Epoch 54/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.3395 - mse: 0.2738\n",
      "Epoch 00054: val_loss improved from 0.33579 to 0.33485, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3394 - mse: 0.2737 - val_loss: 0.3349 - val_mse: 0.2695\n",
      "Epoch 55/100\n",
      "11520/12384 [==========================>...] - ETA: 0s - loss: 0.3412 - mse: 0.2760\n",
      "Epoch 00055: val_loss did not improve from 0.33485\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3376 - mse: 0.2724 - val_loss: 0.3377 - val_mse: 0.2728\n",
      "Epoch 56/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.3379 - mse: 0.2733- ETA: 0s - loss: 0.3370 - mse\n",
      "Epoch 00056: val_loss improved from 0.33485 to 0.33374, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3363 - mse: 0.2718 - val_loss: 0.3337 - val_mse: 0.2694\n",
      "Epoch 57/100\n",
      "12224/12384 [============================>.] - ETA: 0s - loss: 0.3349 - mse: 0.2708\n",
      "Epoch 00057: val_loss did not improve from 0.33374\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3357 - mse: 0.2717 - val_loss: 0.3468 - val_mse: 0.2829\n",
      "Epoch 58/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.3368 - mse: 0.2732- ETA: 0s - loss: 0.3422 - mse: 0.278\n",
      "Epoch 00058: val_loss did not improve from 0.33374\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3363 - mse: 0.2727 - val_loss: 0.3526 - val_mse: 0.2893\n",
      "Epoch 59/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3361 - mse: 0.2730\n",
      "Epoch 00059: val_loss improved from 0.33374 to 0.33348, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3341 - mse: 0.2710 - val_loss: 0.3335 - val_mse: 0.2706\n",
      "Epoch 60/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.3309 - mse: 0.2683- ETA: 0s - loss: 0.3292 - mse: 0.26 - ETA: 0s - loss: 0.3289 - mse: 0\n",
      "Epoch 00060: val_loss did not improve from 0.33348\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3330 - mse: 0.2704 - val_loss: 0.3423 - val_mse: 0.2799\n",
      "Epoch 61/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.3320 - mse: 0.2699\n",
      "Epoch 00061: val_loss did not improve from 0.33348\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3321 - mse: 0.2700 - val_loss: 0.3363 - val_mse: 0.2744\n",
      "Epoch 62/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.3291 - mse: 0.2674\n",
      "Epoch 00062: val_loss did not improve from 0.33348\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3299 - mse: 0.2683 - val_loss: 0.3553 - val_mse: 0.2939\n",
      "Epoch 63/100\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.3302 - mse: 0.2690\n",
      "Epoch 00063: val_loss improved from 0.33348 to 0.32907, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3285 - mse: 0.2673 - val_loss: 0.3291 - val_mse: 0.2681\n",
      "Epoch 64/100\n",
      "12032/12384 [============================>.] - ETA: 0s - loss: 0.3302 - mse: 0.2694\n",
      "Epoch 00064: val_loss did not improve from 0.32907\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3291 - mse: 0.2684 - val_loss: 0.3328 - val_mse: 0.2722\n",
      "Epoch 65/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3263 - mse: 0.2660\n",
      "Epoch 00065: val_loss improved from 0.32907 to 0.32845, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.3277 - mse: 0.2674 - val_loss: 0.3285 - val_mse: 0.2684\n",
      "Epoch 66/100\n",
      "12320/12384 [============================>.] - ETA: 0s - loss: 0.3273 - mse: 0.2674\n",
      "Epoch 00066: val_loss improved from 0.32845 to 0.32373, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.3268 - mse: 0.2669 - val_loss: 0.3237 - val_mse: 0.2641\n",
      "Epoch 67/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3242 - mse: 0.2647- ETA: 0s - loss: 0.3236 - mse: 0.\n",
      "Epoch 00067: val_loss did not improve from 0.32373\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3262 - mse: 0.2667 - val_loss: 0.3272 - val_mse: 0.2681\n",
      "Epoch 68/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.3269 - mse: 0.267 - ETA: 0s - loss: 0.3256 - mse: 0.2666\n",
      "Epoch 00068: val_loss did not improve from 0.32373\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3256 - mse: 0.2666 - val_loss: 0.3303 - val_mse: 0.2716\n",
      "Epoch 69/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3363 - mse: 0.2777\n",
      "Epoch 00069: val_loss did not improve from 0.32373\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3336 - mse: 0.2750 - val_loss: 0.3284 - val_mse: 0.2700\n",
      "Epoch 70/100\n",
      "12096/12384 [============================>.] - ETA: 0s - loss: 0.3246 - mse: 0.2664\n",
      "Epoch 00070: val_loss did not improve from 0.32373\n",
      "12384/12384 [==============================] - 1s 74us/sample - loss: 0.3253 - mse: 0.2671 - val_loss: 0.3490 - val_mse: 0.2910\n",
      "Epoch 71/100\n",
      "11936/12384 [===========================>..] - ETA: 0s - loss: 0.3245 - mse: 0.2666\n",
      "Epoch 00071: val_loss did not improve from 0.32373\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3238 - mse: 0.2660 - val_loss: 0.3286 - val_mse: 0.2710\n",
      "Epoch 72/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.3213 - mse: 0.2639\n",
      "Epoch 00072: val_loss did not improve from 0.32373\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3214 - mse: 0.2639 - val_loss: 0.3327 - val_mse: 0.2754\n",
      "Epoch 73/100\n",
      "12032/12384 [============================>.] - ETA: 0s - loss: 0.3236 - mse: 0.2666- ETA: 0s - loss: 0.3169 - m\n",
      "Epoch 00073: val_loss did not improve from 0.32373\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3223 - mse: 0.2653 - val_loss: 0.3284 - val_mse: 0.2715\n",
      "Epoch 74/100\n",
      "12128/12384 [============================>.] - ETA: 0s - loss: 0.3214 - mse: 0.2647\n",
      "Epoch 00074: val_loss did not improve from 0.32373\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.008999999798834325.\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3216 - mse: 0.2649 - val_loss: 0.3364 - val_mse: 0.2799\n",
      "Epoch 75/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3179 - mse: 0.2615\n",
      "Epoch 00075: val_loss improved from 0.32373 to 0.31917, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 76us/sample - loss: 0.3196 - mse: 0.2633 - val_loss: 0.3192 - val_mse: 0.2630\n",
      "Epoch 76/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3182 - mse: 0.2621\n",
      "Epoch 00076: val_loss did not improve from 0.31917\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3200 - mse: 0.2640 - val_loss: 0.3435 - val_mse: 0.2876\n",
      "Epoch 77/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3132 - mse: 0.2575\n",
      "Epoch 00077: val_loss did not improve from 0.31917\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3159 - mse: 0.2601 - val_loss: 0.3213 - val_mse: 0.2657\n",
      "Epoch 78/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.3163 - mse: 0.2609\n",
      "Epoch 00078: val_loss did not improve from 0.31917\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3169 - mse: 0.2614 - val_loss: 0.3198 - val_mse: 0.2646\n",
      "Epoch 79/100\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.3159 - mse: 0.2608\n",
      "Epoch 00079: val_loss did not improve from 0.31917\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3168 - mse: 0.2617 - val_loss: 0.3244 - val_mse: 0.2695\n",
      "Epoch 80/100\n",
      "12032/12384 [============================>.] - ETA: 0s - loss: 0.3190 - mse: 0.2642- ETA: 0s - loss: 0.3159 - mse: 0.2\n",
      "Epoch 00080: val_loss improved from 0.31917 to 0.31674, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3167 - mse: 0.2619 - val_loss: 0.3167 - val_mse: 0.2621\n",
      "Epoch 81/100\n",
      "12320/12384 [============================>.] - ETA: 0s - loss: 0.3151 - mse: 0.2606\n",
      "Epoch 00081: val_loss did not improve from 0.31674\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3154 - mse: 0.2609 - val_loss: 0.3279 - val_mse: 0.2735\n",
      "Epoch 82/100\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.3177 - mse: 0.2635\n",
      "Epoch 00082: val_loss improved from 0.31674 to 0.31323, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3154 - mse: 0.2611 - val_loss: 0.3132 - val_mse: 0.2592\n",
      "Epoch 83/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.3134 - mse: 0.2595- ETA: 0s - loss: 0.3086 - mse\n",
      "Epoch 00083: val_loss improved from 0.31323 to 0.30885, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3141 - mse: 0.2602 - val_loss: 0.3088 - val_mse: 0.2550\n",
      "Epoch 84/100\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.3146 - mse: 0.2609\n",
      "Epoch 00084: val_loss did not improve from 0.30885\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3144 - mse: 0.2607 - val_loss: 0.3101 - val_mse: 0.2566\n",
      "Epoch 85/100\n",
      "12160/12384 [============================>.] - ETA: 0s - loss: 0.3132 - mse: 0.2599\n",
      "Epoch 00085: val_loss did not improve from 0.30885\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3129 - mse: 0.2596 - val_loss: 0.3449 - val_mse: 0.2916\n",
      "Epoch 86/100\n",
      "11488/12384 [==========================>...] - ETA: 0s - loss: 0.3154 - mse: 0.2624\n",
      "Epoch 00086: val_loss did not improve from 0.30885\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3111 - mse: 0.2580 - val_loss: 0.3178 - val_mse: 0.2648\n",
      "Epoch 87/100\n",
      "11936/12384 [===========================>..] - ETA: 0s - loss: 0.3113 - mse: 0.2584- ETA: 0s - loss: 0.3163 - mse: 0. - ETA: 0s - loss: 0.3155 - mse: 0.\n",
      "Epoch 00087: val_loss did not improve from 0.30885\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3124 - mse: 0.2596 - val_loss: 0.3183 - val_mse: 0.2656\n",
      "Epoch 88/100\n",
      "11456/12384 [==========================>...] - ETA: 0s - loss: 0.3132 - mse: 0.2607- ETA: 0s - loss: 0.3099 - mse: \n",
      "Epoch 00088: val_loss did not improve from 0.30885\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3118 - mse: 0.2593 - val_loss: 0.3126 - val_mse: 0.2602\n",
      "Epoch 89/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.3128 - mse: 0.2606\n",
      "Epoch 00089: val_loss did not improve from 0.30885\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3112 - mse: 0.2589 - val_loss: 0.3266 - val_mse: 0.2745\n",
      "Epoch 90/100\n",
      "11552/12384 [==========================>...] - ETA: 0s - loss: 0.3045 - mse: 0.2525\n",
      "Epoch 00090: val_loss did not improve from 0.30885\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3090 - mse: 0.2570 - val_loss: 0.3343 - val_mse: 0.2824\n",
      "Epoch 91/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3096 - mse: 0.2579- ETA: 0s - loss: 0.3159 - mse: 0. - ETA: 0s - loss: 0.3138 - mse: 0.2\n",
      "Epoch 00091: val_loss did not improve from 0.30885\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.008099999651312828.\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3110 - mse: 0.2593 - val_loss: 0.3095 - val_mse: 0.2579\n",
      "Epoch 92/100\n",
      "11904/12384 [===========================>..] - ETA: 0s - loss: 0.3064 - mse: 0.2549- ETA: 0s - loss: 0.3044 - mse: 0\n",
      "Epoch 00092: val_loss improved from 0.30885 to 0.30630, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3066 - mse: 0.2551 - val_loss: 0.3063 - val_mse: 0.2549\n",
      "Epoch 93/100\n",
      "12096/12384 [============================>.] - ETA: 0s - loss: 0.3058 - mse: 0.2545\n",
      "Epoch 00093: val_loss did not improve from 0.30630\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3063 - mse: 0.2550 - val_loss: 0.3250 - val_mse: 0.2738\n",
      "Epoch 94/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.3033 - mse: 0.2523\n",
      "Epoch 00094: val_loss did not improve from 0.30630\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3046 - mse: 0.2535 - val_loss: 0.3194 - val_mse: 0.2685\n",
      "Epoch 95/100\n",
      "11936/12384 [===========================>..] - ETA: 0s - loss: 0.3063 - mse: 0.2554- ETA: 0s - loss: 0.3096 - mse: 0.2\n",
      "Epoch 00095: val_loss did not improve from 0.30630\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3045 - mse: 0.2537 - val_loss: 0.3095 - val_mse: 0.2588\n",
      "Epoch 96/100\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.3042 - mse: 0.2535\n",
      "Epoch 00096: val_loss did not improve from 0.30630\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3063 - mse: 0.2557 - val_loss: 0.3107 - val_mse: 0.2602\n",
      "Epoch 97/100\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.3046 - mse: 0.2542\n",
      "Epoch 00097: val_loss did not improve from 0.30630\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3028 - mse: 0.2524 - val_loss: 0.3142 - val_mse: 0.2639\n",
      "Epoch 98/100\n",
      "12224/12384 [============================>.] - ETA: 0s - loss: 0.3045 - mse: 0.2543\n",
      "Epoch 00098: val_loss did not improve from 0.30630\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3038 - mse: 0.2536 - val_loss: 0.3089 - val_mse: 0.2588\n",
      "Epoch 99/100\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.3007 - mse: 0.2507\n",
      "Epoch 00099: val_loss did not improve from 0.30630\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3027 - mse: 0.2527 - val_loss: 0.3372 - val_mse: 0.2873\n",
      "Epoch 100/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.3027 - mse: 0.2529\n",
      "Epoch 00100: val_loss improved from 0.30630 to 0.30254, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3039 - mse: 0.2541 - val_loss: 0.3025 - val_mse: 0.2529\n",
      "5160/5160 [==============================] - 0s 50us/sample - loss: 0.3383 - mse: 0.2886\n",
      "\n",
      "\n",
      "Puntaje total:  -0.33828336046647656\n"
     ]
    }
   ],
   "source": [
    "# modelo keras\n",
    "\n",
    "# cargar datos\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "# estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # fit + transform\n",
    "X_test = scaler.transform(X_test) # transform (usando entrenado)\n",
    "\n",
    "print('\\n')\n",
    "print('Dim X train: ',X_train.shape)\n",
    "print('Dim Y train: ',y_train.shape)\n",
    "print('\\n')\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "dimension = X_train.shape[1:]\n",
    "\n",
    "def crear_modelo(): \n",
    "    # modelo secuencial\n",
    "    modeli = keras.Sequential()\n",
    "    modeli.add(Dense(80,activation='relu',kernel_initializer='glorot_uniform',kernel_regularizer=keras.regularizers.l2(0.001),input_shape=dimension))\n",
    "    modeli.add(Dense(50,activation='relu',kernel_initializer='glorot_uniform',kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    modeli.add(Dense(20,activation='relu',kernel_initializer='glorot_uniform',kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    modeli.add(Dense(1))\n",
    "    \n",
    "    #compilar y retornar objeto modelo\n",
    "    modeli.compile(loss='mse', optimizer = 'sgd',metrics=['mse'])\n",
    "    return modeli\n",
    "\n",
    "# Callbacks para el entrenamiento\n",
    "direccion = 'mejor_modelo_entrenado.h5' # nombre de archivo a guardar modelo entrenado\n",
    "chk = keras.callbacks.ModelCheckpoint(direccion,save_best_only=True,verbose=2)\n",
    "stp = keras.callbacks.EarlyStopping(patience=20,mode='auto',min_delta=0,restore_best_weights=True,verbose=2)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(factor=0.9,patience=8,verbose=2)\n",
    "\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "epochs = 100\n",
    "#batch_size = 128\n",
    "shuffle =True\n",
    "callbacks = [chk,stp,lrs]\n",
    "\n",
    "# wrapping... de keras a scikit model... esto convierte a model a tipo Scikite (posee .fit() , .predict() ,  .evaluate())\n",
    "model= tf.keras.wrappers.scikit_learn.KerasRegressor(crear_modelo,epochs=epochs,callbacks=callbacks,shuffle=True,validation_split=0.2)\n",
    "\n",
    "# entrenar modelo\n",
    "history = model.fit(X_train, y_train)\n",
    "\n",
    "#############################################################\n",
    "####### Ver curvas de Aprendizaje del modelo #############\n",
    "\n",
    "historia = history.history\n",
    "\n",
    "print('\\n\\nCurvas de Aprendizaje\\n')\n",
    "for nombre,valores in historia.items():\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.title('Curva de ' + nombre)\n",
    "    plt.xlabel('Interacion \"i\"')\n",
    "    plt.ylabel(nombre)\n",
    "    plt.plot(valores)\n",
    "    plt.show()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# test de rendimiento - mse\n",
    "mse_total = model.score(X_test,y_test)\n",
    "print('\\n\\nPuntaje total: ',mse_total)\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:59:37.545582Z",
     "start_time": "2021-02-15T21:59:37.180565Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 54us/sample - loss: 0.3383 - mse: 0.2886\n",
      "\n",
      "Puntaje de modelo: -0.34\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "[4.908 1.02  1.275 2.347 0.879 1.512 1.101 2.153 1.503 1.516]\n",
      "\n",
      "Valores Orginales: \n",
      "[5.    1.185 1.16  2.295 0.621 1.612 0.981 2.683 1.584 1.555]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediccion con modelo\n",
    "\n",
    "valor = model.score(X_test,y_test)\n",
    "print('\\nPuntaje de modelo:' ,np.round(valor,2))\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(model.predict(X_test[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Opcion 02 - con keras.add()  - (Sin GridSearchCV &  Sin Opcion Multicapa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:57:28.524466Z",
     "start_time": "2021-02-15T21:56:01.576254Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dim X train:  (15480, 8)\n",
      "Dim Y train:  (15480,)\n",
      "\n",
      "\n",
      "Train on 12384 samples, validate on 3096 samples\n",
      "Epoch 1/100\n",
      "11456/12384 [==========================>...] - ETA: 0s - loss: 1.0062 - mse: 0.9014- ETA: 0s - loss: 1.0849 - mse: 0.9 - ETA: 0s - loss: 0.9758 - mse: 0.8708\n",
      "Epoch 00001: val_loss improved from inf to 0.60850, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 87us/sample - loss: 0.9431 - mse: 0.8379 - val_loss: 0.6085 - val_mse: 0.5013\n",
      "Epoch 2/100\n",
      "12000/12384 [============================>.] - ETA: 0s - loss: 0.7269 - mse: 0.6202\n",
      "Epoch 00002: val_loss improved from 0.60850 to 0.60007, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.7249 - mse: 0.6182 - val_loss: 0.6001 - val_mse: 0.4938\n",
      "Epoch 3/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.5301 - mse: 0.4243\n",
      "Epoch 00003: val_loss improved from 0.60007 to 0.54422, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.5288 - mse: 0.4231 - val_loss: 0.5442 - val_mse: 0.4391\n",
      "Epoch 4/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.5014 - mse: 0.3967\n",
      "Epoch 00004: val_loss improved from 0.54422 to 0.54081, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.5016 - mse: 0.3970 - val_loss: 0.5408 - val_mse: 0.4367\n",
      "Epoch 5/100\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.4875 - mse: 0.3839\n",
      "Epoch 00005: val_loss improved from 0.54081 to 0.51421, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.4860 - mse: 0.3824 - val_loss: 0.5142 - val_mse: 0.4112\n",
      "Epoch 6/100\n",
      "11488/12384 [==========================>...] - ETA: 0s - loss: 0.4711 - mse: 0.3685\n",
      "Epoch 00006: val_loss improved from 0.51421 to 0.50213, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.4701 - mse: 0.3676 - val_loss: 0.5021 - val_mse: 0.4001\n",
      "Epoch 7/100\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.4622 - mse: 0.3606\n",
      "Epoch 00007: val_loss improved from 0.50213 to 0.49412, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.4608 - mse: 0.3592 - val_loss: 0.4941 - val_mse: 0.3930\n",
      "Epoch 8/100\n",
      "11456/12384 [==========================>...] - ETA: 0s - loss: 0.4522 - mse: 0.3515\n",
      "Epoch 00008: val_loss improved from 0.49412 to 0.47940, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.4525 - mse: 0.3519 - val_loss: 0.4794 - val_mse: 0.3792\n",
      "Epoch 9/100\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.4420 - mse: 0.3423- ETA: 0s - loss: 0.4416 - mse: 0.341\n",
      "Epoch 00009: val_loss improved from 0.47940 to 0.47306, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.4428 - mse: 0.3431 - val_loss: 0.4731 - val_mse: 0.3738\n",
      "Epoch 10/100\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.4351 - mse: 0.336 - ETA: 0s - loss: 0.4344 - mse: 0.3356\n",
      "Epoch 00010: val_loss improved from 0.47306 to 0.46484, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.4343 - mse: 0.3355 - val_loss: 0.4648 - val_mse: 0.3664\n",
      "Epoch 11/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.4324 - mse: 0.3345\n",
      "Epoch 00011: val_loss improved from 0.46484 to 0.46140, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.4321 - mse: 0.3342 - val_loss: 0.4614 - val_mse: 0.3640\n",
      "Epoch 12/100\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.4283 - mse: 0.3312- ETA: 0s - loss: 0.4502 - m\n",
      "Epoch 00012: val_loss did not improve from 0.46140\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.4252 - mse: 0.3281 - val_loss: 0.4684 - val_mse: 0.3718\n",
      "Epoch 13/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.4212 - mse: 0.3251- ETA: 0s - loss: 0.4204 - mse: 0.3 - ETA: 0s - loss: 0.4234 - mse: 0.327\n",
      "Epoch 00013: val_loss improved from 0.46140 to 0.45868, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.4200 - mse: 0.3239 - val_loss: 0.4587 - val_mse: 0.3630\n",
      "Epoch 14/100\n",
      "11552/12384 [==========================>...] - ETA: 0s - loss: 0.4188 - mse: 0.3236\n",
      "Epoch 00014: val_loss did not improve from 0.45868\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.4173 - mse: 0.3221 - val_loss: 0.4614 - val_mse: 0.3666\n",
      "Epoch 15/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.4079 - mse: 0.3135\n",
      "Epoch 00015: val_loss improved from 0.45868 to 0.44950, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 74us/sample - loss: 0.4087 - mse: 0.3144 - val_loss: 0.4495 - val_mse: 0.3556\n",
      "Epoch 16/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.4062 - mse: 0.3127\n",
      "Epoch 00016: val_loss improved from 0.44950 to 0.43717, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.4064 - mse: 0.3129 - val_loss: 0.4372 - val_mse: 0.3441\n",
      "Epoch 17/100\n",
      "12224/12384 [============================>.] - ETA: 0s - loss: 0.4018 - mse: 0.3092\n",
      "Epoch 00017: val_loss did not improve from 0.43717\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.4017 - mse: 0.3091 - val_loss: 0.4386 - val_mse: 0.3464\n",
      "Epoch 18/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.4013 - mse: 0.3095\n",
      "Epoch 00018: val_loss did not improve from 0.43717\n",
      "12384/12384 [==============================] - 1s 62us/sample - loss: 0.3995 - mse: 0.3077 - val_loss: 0.4386 - val_mse: 0.3472\n",
      "Epoch 19/100\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.3925 - mse: 0.3015\n",
      "Epoch 00019: val_loss did not improve from 0.43717\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3947 - mse: 0.3037 - val_loss: 0.4570 - val_mse: 0.3663\n",
      "Epoch 20/100\n",
      "11776/12384 [===========================>..] - ETA: 0s - loss: 0.3932 - mse: 0.3030\n",
      "Epoch 00020: val_loss improved from 0.43717 to 0.43320, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3926 - mse: 0.3024 - val_loss: 0.4332 - val_mse: 0.3434\n",
      "Epoch 21/100\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.3888 - mse: 0.2994\n",
      "Epoch 00021: val_loss did not improve from 0.43320\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3896 - mse: 0.3003 - val_loss: 0.4401 - val_mse: 0.3511\n",
      "Epoch 22/100\n",
      "12224/12384 [============================>.] - ETA: 0s - loss: 0.3862 - mse: 0.2976- ETA: 0s - loss: 0.3853 - mse: 0.2\n",
      "Epoch 00022: val_loss did not improve from 0.43320\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3865 - mse: 0.2979 - val_loss: 0.4379 - val_mse: 0.3498\n",
      "Epoch 23/100\n",
      "11488/12384 [==========================>...] - ETA: 0s - loss: 0.3853 - mse: 0.2975\n",
      "Epoch 00023: val_loss improved from 0.43320 to 0.41909, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3826 - mse: 0.2948 - val_loss: 0.4191 - val_mse: 0.3317\n",
      "Epoch 24/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.3836 - mse: 0.2966\n",
      "Epoch 00024: val_loss improved from 0.41909 to 0.41892, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 74us/sample - loss: 0.3816 - mse: 0.2946 - val_loss: 0.4189 - val_mse: 0.3323\n",
      "Epoch 25/100\n",
      "11776/12384 [===========================>..] - ETA: 0s - loss: 0.3808 - mse: 0.2946\n",
      "Epoch 00025: val_loss did not improve from 0.41892\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3805 - mse: 0.2943 - val_loss: 0.4221 - val_mse: 0.3363\n",
      "Epoch 26/100\n",
      "11552/12384 [==========================>...] - ETA: 0s - loss: 0.3754 - mse: 0.2900\n",
      "Epoch 00026: val_loss did not improve from 0.41892\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3779 - mse: 0.2925 - val_loss: 0.4196 - val_mse: 0.3346\n",
      "Epoch 27/100\n",
      "11776/12384 [===========================>..] - ETA: 0s - loss: 0.3737 - mse: 0.2891- ETA: 0s - loss: 0.3684 - mse: 0.283 - ETA: 0s - loss: 0.3753 - mse: 0.\n",
      "Epoch 00027: val_loss improved from 0.41892 to 0.41076, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3735 - mse: 0.2889 - val_loss: 0.4108 - val_mse: 0.3266\n",
      "Epoch 28/100\n",
      "11520/12384 [==========================>...] - ETA: 0s - loss: 0.3740 - mse: 0.2900- ETA: 0s - loss: 0.3738 - mse: 0.289\n",
      "Epoch 00028: val_loss improved from 0.41076 to 0.40802, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.3731 - mse: 0.2892 - val_loss: 0.4080 - val_mse: 0.3245\n",
      "Epoch 29/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3684 - mse: 0.2852\n",
      "Epoch 00029: val_loss improved from 0.40802 to 0.40531, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.3693 - mse: 0.2861 - val_loss: 0.4053 - val_mse: 0.3225\n",
      "Epoch 30/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.3696 - mse: 0.2872- ETA: 0s - loss: 0.3603 - mse: 0\n",
      "Epoch 00030: val_loss did not improve from 0.40531\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3685 - mse: 0.2861 - val_loss: 0.4122 - val_mse: 0.3301\n",
      "Epoch 31/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3689 - mse: 0.2873- ETA: 0s - loss: 0.3663 - mse\n",
      "Epoch 00031: val_loss did not improve from 0.40531\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3702 - mse: 0.2885 - val_loss: 0.4206 - val_mse: 0.3393\n",
      "Epoch 32/100\n",
      "12032/12384 [============================>.] - ETA: 0s - loss: 0.3683 - mse: 0.2874- ETA: 0s - loss: 0.3651 - mse: 0\n",
      "Epoch 00032: val_loss did not improve from 0.40531\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.3690 - mse: 0.2881 - val_loss: 0.4152 - val_mse: 0.3346\n",
      "Epoch 33/100\n",
      "12160/12384 [============================>.] - ETA: 0s - loss: 0.3642 - mse: 0.2839\n",
      "Epoch 00033: val_loss did not improve from 0.40531\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.3634 - mse: 0.2831 - val_loss: 0.4170 - val_mse: 0.3371\n",
      "Epoch 34/100\n",
      "12000/12384 [============================>.] - ETA: 0s - loss: 0.3622 - mse: 0.2825\n",
      "Epoch 00034: val_loss improved from 0.40531 to 0.40085, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3642 - mse: 0.2846 - val_loss: 0.4008 - val_mse: 0.3216\n",
      "Epoch 35/100\n",
      "11840/12384 [===========================>..] - ETA: 0s - loss: 0.3633 - mse: 0.2844\n",
      "Epoch 00035: val_loss did not improve from 0.40085\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3617 - mse: 0.2827 - val_loss: 0.4013 - val_mse: 0.3227\n",
      "Epoch 36/100\n",
      "11840/12384 [===========================>..] - ETA: 0s - loss: 0.3602 - mse: 0.2819\n",
      "Epoch 00036: val_loss improved from 0.40085 to 0.39766, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3604 - mse: 0.2821 - val_loss: 0.3977 - val_mse: 0.3197\n",
      "Epoch 37/100\n",
      "11776/12384 [===========================>..] - ETA: 0s - loss: 0.3622 - mse: 0.2846\n",
      "Epoch 00037: val_loss did not improve from 0.39766\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3597 - mse: 0.2822 - val_loss: 0.4027 - val_mse: 0.3255\n",
      "Epoch 38/100\n",
      "11488/12384 [==========================>...] - ETA: 0s - loss: 0.3562 - mse: 0.2792\n",
      "Epoch 00038: val_loss did not improve from 0.39766\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3559 - mse: 0.2789 - val_loss: 0.4213 - val_mse: 0.3447\n",
      "Epoch 39/100\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.3553 - mse: 0.2790- ETA: 0s - loss: 0.3472 - mse:\n",
      "Epoch 00039: val_loss did not improve from 0.39766\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3550 - mse: 0.2787 - val_loss: 0.4364 - val_mse: 0.3604\n",
      "Epoch 40/100\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.3556 - mse: 0.2799- ETA: 0s - loss: 0.3481 - mse: - ETA: 0s - loss: 0.3561 - mse: 0.28\n",
      "Epoch 00040: val_loss did not improve from 0.39766\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3557 - mse: 0.2800 - val_loss: 0.4168 - val_mse: 0.3414\n",
      "Epoch 41/100\n",
      "11456/12384 [==========================>...] - ETA: 0s - loss: 0.3534 - mse: 0.2784- ETA: 0s - loss: 0.3698 - mse:\n",
      "Epoch 00041: val_loss did not improve from 0.39766\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3535 - mse: 0.2785 - val_loss: 0.4175 - val_mse: 0.3427\n",
      "Epoch 42/100\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.3493 - mse: 0.2748\n",
      "Epoch 00042: val_loss improved from 0.39766 to 0.39201, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3508 - mse: 0.2763 - val_loss: 0.3920 - val_mse: 0.3178\n",
      "Epoch 43/100\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.3524 - mse: 0.2785\n",
      "Epoch 00043: val_loss did not improve from 0.39201\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3502 - mse: 0.2764 - val_loss: 0.3924 - val_mse: 0.3189\n",
      "Epoch 44/100\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.3469 - mse: 0.2736- ETA: 0s - loss: 0.3399 - mse\n",
      "Epoch 00044: val_loss did not improve from 0.39201\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3473 - mse: 0.2740 - val_loss: 0.3952 - val_mse: 0.3222\n",
      "Epoch 45/100\n",
      "11904/12384 [===========================>..] - ETA: 0s - loss: 0.3470 - mse: 0.2743- ETA: 0s - loss: 0.3559 - mse: \n",
      "Epoch 00045: val_loss improved from 0.39201 to 0.39192, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3468 - mse: 0.2741 - val_loss: 0.3919 - val_mse: 0.3195\n",
      "Epoch 46/100\n",
      "12288/12384 [============================>.] - ETA: 0s - loss: 0.3476 - mse: 0.2755- ETA: 0s - loss: 0.3488 - mse\n",
      "Epoch 00046: val_loss improved from 0.39192 to 0.38843, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3468 - mse: 0.2747 - val_loss: 0.3884 - val_mse: 0.3166\n",
      "Epoch 47/100\n",
      "12000/12384 [============================>.] - ETA: 0s - loss: 0.3466 - mse: 0.2750\n",
      "Epoch 00047: val_loss did not improve from 0.38843\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3469 - mse: 0.2753 - val_loss: 0.3978 - val_mse: 0.3265\n",
      "Epoch 48/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3453 - mse: 0.2743- ETA: 0s - loss: 0.3404 - mse\n",
      "Epoch 00048: val_loss did not improve from 0.38843\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3463 - mse: 0.2752 - val_loss: 0.3902 - val_mse: 0.3194\n",
      "Epoch 49/100\n",
      "11488/12384 [==========================>...] - ETA: 0s - loss: 0.3407 - mse: 0.2702\n",
      "Epoch 00049: val_loss improved from 0.38843 to 0.38374, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.3444 - mse: 0.2739 - val_loss: 0.3837 - val_mse: 0.3135\n",
      "Epoch 50/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3392 - mse: 0.2691\n",
      "Epoch 00050: val_loss did not improve from 0.38374\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3394 - mse: 0.2693 - val_loss: 0.3852 - val_mse: 0.3155\n",
      "Epoch 51/100\n",
      "11840/12384 [===========================>..] - ETA: 0s - loss: 0.3449 - mse: 0.2754\n",
      "Epoch 00051: val_loss improved from 0.38374 to 0.38007, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3454 - mse: 0.2759 - val_loss: 0.3801 - val_mse: 0.3109\n",
      "Epoch 52/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.3399 - mse: 0.2709- ETA: 0s - loss: 0.3354 - mse\n",
      "Epoch 00052: val_loss did not improve from 0.38007\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3405 - mse: 0.2715 - val_loss: 0.4027 - val_mse: 0.3341\n",
      "Epoch 53/100\n",
      "11968/12384 [===========================>..] - ETA: 0s - loss: 0.3395 - mse: 0.2709\n",
      "Epoch 00053: val_loss improved from 0.38007 to 0.37872, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3385 - mse: 0.2700 - val_loss: 0.3787 - val_mse: 0.3105\n",
      "Epoch 54/100\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.3378 - mse: 0.2698\n",
      "Epoch 00054: val_loss improved from 0.37872 to 0.37783, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3379 - mse: 0.2699 - val_loss: 0.3778 - val_mse: 0.3101\n",
      "Epoch 55/100\n",
      "11776/12384 [===========================>..] - ETA: 0s - loss: 0.3378 - mse: 0.2703- ETA: 0s - loss: 0.3408 - mse: 0.27\n",
      "Epoch 00055: val_loss did not improve from 0.37783\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3371 - mse: 0.2696 - val_loss: 0.4054 - val_mse: 0.3382\n",
      "Epoch 56/100\n",
      "12128/12384 [============================>.] - ETA: 0s - loss: 0.3350 - mse: 0.2679- ETA: 0s - loss: 0.3418 - mse: \n",
      "Epoch 00056: val_loss did not improve from 0.37783\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3370 - mse: 0.2700 - val_loss: 0.3838 - val_mse: 0.3170\n",
      "Epoch 57/100\n",
      "12224/12384 [============================>.] - ETA: 0s - loss: 0.3375 - mse: 0.2709\n",
      "Epoch 00057: val_loss did not improve from 0.37783\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3378 - mse: 0.2712 - val_loss: 0.3814 - val_mse: 0.3150\n",
      "Epoch 58/100\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.3372 - mse: 0.2710\n",
      "Epoch 00058: val_loss improved from 0.37783 to 0.37559, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3356 - mse: 0.2695 - val_loss: 0.3756 - val_mse: 0.3097\n",
      "Epoch 59/100\n",
      "12352/12384 [============================>.] - ETA: 0s - loss: 0.3330 - mse: 0.2674- ETA: 0s - loss: 0.3502 - \n",
      "Epoch 00059: val_loss did not improve from 0.37559\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3334 - mse: 0.2678 - val_loss: 0.4092 - val_mse: 0.3438\n",
      "Epoch 60/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3298 - mse: 0.2647\n",
      "Epoch 00060: val_loss improved from 0.37559 to 0.37531, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.3322 - mse: 0.2671 - val_loss: 0.3753 - val_mse: 0.3104\n",
      "Epoch 61/100\n",
      "11840/12384 [===========================>..] - ETA: 0s - loss: 0.3342 - mse: 0.2695- ETA: 0s - loss: 0.3349 - mse\n",
      "Epoch 00061: val_loss did not improve from 0.37531\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3345 - mse: 0.2698 - val_loss: 0.3901 - val_mse: 0.3255\n",
      "Epoch 62/100\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.3330 - mse: 0.2686\n",
      "Epoch 00062: val_loss did not improve from 0.37531\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3338 - mse: 0.2694 - val_loss: 0.3811 - val_mse: 0.3170\n",
      "Epoch 63/100\n",
      "12032/12384 [============================>.] - ETA: 0s - loss: 0.3323 - mse: 0.2684\n",
      "Epoch 00063: val_loss improved from 0.37531 to 0.37475, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 81us/sample - loss: 0.3313 - mse: 0.2674 - val_loss: 0.3748 - val_mse: 0.3110\n",
      "Epoch 64/100\n",
      "12224/12384 [============================>.] - ETA: 0s - loss: 0.3280 - mse: 0.2645- ETA: 0s - loss: 0.3206 - mse: 0.2\n",
      "Epoch 00064: val_loss did not improve from 0.37475\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3287 - mse: 0.2652 - val_loss: 0.3754 - val_mse: 0.3122\n",
      "Epoch 65/100\n",
      "12352/12384 [============================>.] - ETA: 0s - loss: 0.3265 - mse: 0.2634- ETA: 0s - loss: 0.3255 - mse: 0.\n",
      "Epoch 00065: val_loss improved from 0.37475 to 0.36957, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 74us/sample - loss: 0.3263 - mse: 0.2633 - val_loss: 0.3696 - val_mse: 0.3067\n",
      "Epoch 66/100\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.3272 - mse: 0.2646- ETA: 0s - loss: 0.3276 - mse: 0.\n",
      "Epoch 00066: val_loss did not improve from 0.36957\n",
      "12384/12384 [==============================] - 1s 76us/sample - loss: 0.3294 - mse: 0.2667 - val_loss: 0.3837 - val_mse: 0.3212\n",
      "Epoch 67/100\n",
      "11968/12384 [===========================>..] - ETA: 0s - loss: 0.3289 - mse: 0.2666- ETA: 0s - loss: 0.3368 - mse\n",
      "Epoch 00067: val_loss did not improve from 0.36957\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3288 - mse: 0.2665 - val_loss: 0.3850 - val_mse: 0.3229\n",
      "Epoch 68/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3234 - mse: 0.2615\n",
      "Epoch 00068: val_loss improved from 0.36957 to 0.36943, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.3266 - mse: 0.2647 - val_loss: 0.3694 - val_mse: 0.3078\n",
      "Epoch 69/100\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.3274 - mse: 0.2659- ETA: 0s - loss: 0.3348 - mse: 0.2 - ETA: 0s - loss: 0.3307 - mse: 0.26 - ETA: 0s - loss: 0.3301 - mse: 0.268 - ETA: 0s - loss: 0.3277 - mse: 0.\n",
      "Epoch 00069: val_loss did not improve from 0.36943\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3278 - mse: 0.2663 - val_loss: 0.3801 - val_mse: 0.3187\n",
      "Epoch 70/100\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.3255 - mse: 0.2643- ETA: 0s - loss: 0.3230 - mse: \n",
      "Epoch 00070: val_loss did not improve from 0.36943\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3252 - mse: 0.2641 - val_loss: 0.3902 - val_mse: 0.3292\n",
      "Epoch 71/100\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.3241 - mse: 0.2633\n",
      "Epoch 00071: val_loss improved from 0.36943 to 0.36699, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3241 - mse: 0.2634 - val_loss: 0.3670 - val_mse: 0.3064\n",
      "Epoch 72/100\n",
      "12032/12384 [============================>.] - ETA: 0s - loss: 0.3245 - mse: 0.2641- ETA: 0s - loss: 0.3228 - mse: 0.\n",
      "Epoch 00072: val_loss improved from 0.36699 to 0.36678, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 65us/sample - loss: 0.3243 - mse: 0.2639 - val_loss: 0.3668 - val_mse: 0.3065\n",
      "Epoch 73/100\n",
      "11840/12384 [===========================>..] - ETA: 0s - loss: 0.3248 - mse: 0.2647\n",
      "Epoch 00073: val_loss improved from 0.36678 to 0.36286, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3253 - mse: 0.2653 - val_loss: 0.3629 - val_mse: 0.3030\n",
      "Epoch 74/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3222 - mse: 0.2625\n",
      "Epoch 00074: val_loss did not improve from 0.36286\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3222 - mse: 0.2625 - val_loss: 0.3635 - val_mse: 0.3039\n",
      "Epoch 75/100\n",
      "12032/12384 [============================>.] - ETA: 0s - loss: 0.3226 - mse: 0.2632\n",
      "Epoch 00075: val_loss did not improve from 0.36286\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3227 - mse: 0.2634 - val_loss: 0.3768 - val_mse: 0.3176\n",
      "Epoch 76/100\n",
      "12352/12384 [============================>.] - ETA: 0s - loss: 0.3206 - mse: 0.2616\n",
      "Epoch 00076: val_loss did not improve from 0.36286\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3209 - mse: 0.2619 - val_loss: 0.3681 - val_mse: 0.3093\n",
      "Epoch 77/100\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.3208 - mse: 0.262 - ETA: 0s - loss: 0.3227 - mse: 0.2640\n",
      "Epoch 00077: val_loss did not improve from 0.36286\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3230 - mse: 0.2643 - val_loss: 0.3877 - val_mse: 0.3292\n",
      "Epoch 78/100\n",
      "11936/12384 [===========================>..] - ETA: 0s - loss: 0.3246 - mse: 0.2662\n",
      "Epoch 00078: val_loss did not improve from 0.36286\n",
      "12384/12384 [==============================] - 1s 64us/sample - loss: 0.3220 - mse: 0.2637 - val_loss: 0.3794 - val_mse: 0.3213\n",
      "Epoch 79/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.3221 - mse: 0.2640\n",
      "Epoch 00079: val_loss improved from 0.36286 to 0.36144, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3189 - mse: 0.2608 - val_loss: 0.3614 - val_mse: 0.3036\n",
      "Epoch 80/100\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.3174 - mse: 0.2597- ETA: 0s - loss: 0.3161 - mse: 0.25\n",
      "Epoch 00080: val_loss improved from 0.36144 to 0.36111, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3190 - mse: 0.2613 - val_loss: 0.3611 - val_mse: 0.3035\n",
      "Epoch 81/100\n",
      "11424/12384 [==========================>...] - ETA: 0s - loss: 0.3150 - mse: 0.2575\n",
      "Epoch 00081: val_loss did not improve from 0.36111\n",
      "12384/12384 [==============================] - 1s 65us/sample - loss: 0.3192 - mse: 0.2618 - val_loss: 0.3723 - val_mse: 0.3151\n",
      "Epoch 82/100\n",
      "12128/12384 [============================>.] - ETA: 0s - loss: 0.3172 - mse: 0.2601- ETA: 0s - loss: 0.3122 - mse\n",
      "Epoch 00082: val_loss did not improve from 0.36111\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3172 - mse: 0.2601 - val_loss: 0.3632 - val_mse: 0.3063\n",
      "Epoch 83/100\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.3175 - mse: 0.2607\n",
      "Epoch 00083: val_loss did not improve from 0.36111\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3188 - mse: 0.2620 - val_loss: 0.3702 - val_mse: 0.3136\n",
      "Epoch 84/100\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.3145 - mse: 0.2580\n",
      "Epoch 00084: val_loss did not improve from 0.36111\n",
      "12384/12384 [==============================] - 1s 65us/sample - loss: 0.3159 - mse: 0.2594 - val_loss: 0.3671 - val_mse: 0.3108\n",
      "Epoch 85/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.3176 - mse: 0.2615- ETA: 0s - loss: 0.3079 - mse: 0.251 - ETA: 0s - loss: 0.3137 - mse: 0 - ETA: 0s - loss: 0.3155 - mse: 0.259\n",
      "Epoch 00085: val_loss improved from 0.36111 to 0.36074, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3162 - mse: 0.2600 - val_loss: 0.3607 - val_mse: 0.3047\n",
      "Epoch 86/100\n",
      "12288/12384 [============================>.] - ETA: 0s - loss: 0.3157 - mse: 0.2598- ETA: 0s - loss: 0.3001 - mse\n",
      "Epoch 00086: val_loss improved from 0.36074 to 0.35500, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 63us/sample - loss: 0.3152 - mse: 0.2592 - val_loss: 0.3550 - val_mse: 0.2992\n",
      "Epoch 87/100\n",
      "11936/12384 [===========================>..] - ETA: 0s - loss: 0.3157 - mse: 0.2600\n",
      "Epoch 00087: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3153 - mse: 0.2596 - val_loss: 0.3649 - val_mse: 0.3094\n",
      "Epoch 88/100\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.3139 - mse: 0.2585- ETA: 0s - loss: 0.3147 - mse: 0.25 - ETA: 0s - loss: 0.3115 - mse: 0\n",
      "Epoch 00088: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 68us/sample - loss: 0.3141 - mse: 0.2587 - val_loss: 0.3915 - val_mse: 0.3363\n",
      "Epoch 89/100\n",
      "12320/12384 [============================>.] - ETA: 0s - loss: 0.3173 - mse: 0.2622- ETA: 0s - loss: 0.3208 - mse: 0.\n",
      "Epoch 00089: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 66us/sample - loss: 0.3169 - mse: 0.2618 - val_loss: 0.3631 - val_mse: 0.3081\n",
      "Epoch 90/100\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.3125 - mse: 0.2577- ETA: 0s - loss: 0.3174 - mse: 0\n",
      "Epoch 00090: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3137 - mse: 0.2589 - val_loss: 0.3699 - val_mse: 0.3152\n",
      "Epoch 91/100\n",
      "11520/12384 [==========================>...] - ETA: 0s - loss: 0.3168 - mse: 0.2623\n",
      "Epoch 00091: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3146 - mse: 0.2600 - val_loss: 0.3594 - val_mse: 0.3050\n",
      "Epoch 92/100\n",
      "11904/12384 [===========================>..] - ETA: 0s - loss: 0.3131 - mse: 0.2589\n",
      "Epoch 00092: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3139 - mse: 0.2596 - val_loss: 0.3620 - val_mse: 0.3078\n",
      "Epoch 93/100\n",
      "11936/12384 [===========================>..] - ETA: 0s - loss: 0.3117 - mse: 0.2576- ETA: 0s - loss: 0.3126 - mse: \n",
      "Epoch 00093: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.3112 - mse: 0.2571 - val_loss: 0.3703 - val_mse: 0.3164\n",
      "Epoch 94/100\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.3139 - mse: 0.2600- ETA: 0s - loss: 0.3176 - mse\n",
      "Epoch 00094: val_loss did not improve from 0.35500\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.008999999798834325.\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.3132 - mse: 0.2594 - val_loss: 0.3894 - val_mse: 0.3358\n",
      "Epoch 95/100\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.3079 - mse: 0.2542\n",
      "Epoch 00095: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3076 - mse: 0.2540 - val_loss: 0.3568 - val_mse: 0.3033\n",
      "Epoch 96/100\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.3094 - mse: 0.2560- ETA: 0s - loss: 0.3419 - mse:  - ETA: 0s - loss: 0.3159 - mse: \n",
      "Epoch 00096: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3095 - mse: 0.2561 - val_loss: 0.3702 - val_mse: 0.3170\n",
      "Epoch 97/100\n",
      "11552/12384 [==========================>...] - ETA: 0s - loss: 0.3057 - mse: 0.2525\n",
      "Epoch 00097: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.3065 - mse: 0.2533 - val_loss: 0.3606 - val_mse: 0.3075\n",
      "Epoch 98/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.3105 - mse: 0.2575\n",
      "Epoch 00098: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3106 - mse: 0.2576 - val_loss: 0.3552 - val_mse: 0.3024\n",
      "Epoch 99/100\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.3072 - mse: 0.2545- ETA: 0s - loss: 0.2916 - ms\n",
      "Epoch 00099: val_loss did not improve from 0.35500\n",
      "12384/12384 [==============================] - 1s 67us/sample - loss: 0.3064 - mse: 0.2537 - val_loss: 0.3797 - val_mse: 0.3270\n",
      "Epoch 100/100\n",
      "12064/12384 [============================>.] - ETA: 0s - loss: 0.3082 - mse: 0.2557- ETA: 0s - loss: 0.3042 - mse: 0.251 - ETA: 0s - loss: 0.3095 - mse: 0.25\n",
      "Epoch 00100: val_loss improved from 0.35500 to 0.35030, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.3077 - mse: 0.2552 - val_loss: 0.3503 - val_mse: 0.2979\n",
      "5160/5160 [==============================] - 0s 51us/sample - loss: 0.3159 - mse: 0.2634\n",
      "\n",
      "\n",
      "Puntaje total:  -0.3158582472061926\n"
     ]
    }
   ],
   "source": [
    "# modelo keras\n",
    "\n",
    "# cargar datos\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "# estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # fit + transform\n",
    "X_test = scaler.transform(X_test) # transform (usando entrenado)\n",
    "\n",
    "print('\\n')\n",
    "print('Dim X train: ',X_train.shape)\n",
    "print('Dim Y train: ',y_train.shape)\n",
    "print('\\n')\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "dimension = X_train.shape[1:]\n",
    "\n",
    "def crear_modelo(): \n",
    "    # modelo secuencial\n",
    "    input=keras.layers.Input(shape=X_train_full.shape[1:],name='i')\n",
    "    hidden1=keras.layers.Dense(80,name='h1',activation='relu',kernel_regularizer=keras.regularizers.l2(0.001))(input)\n",
    "    hidden2=keras.layers.Dense(50,name='h2',activation='relu',kernel_regularizer=keras.regularizers.l2(0.001))(hidden1)\n",
    "    hidden3=keras.layers.Dense(20,name='h3',activation='relu',kernel_regularizer=keras.regularizers.l2(0.001))(hidden2)\n",
    "    output=keras.layers.Dense(1,name='out')(hidden3) # salida\n",
    "    modeli=keras.models.Model(inputs=[input],outputs=[output]) ## definir entradas y salidas\n",
    "    \n",
    "    #compilar y retornar objeto modelo\n",
    "    modeli.compile(loss='mse', optimizer = 'sgd',metrics=['mse'])\n",
    "    return modeli\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# Callbacks para el entrenamiento\n",
    "direccion = 'mejor_modelo_entrenado.h5' # nombre de archivo a guardar modelo entrenado\n",
    "chk = keras.callbacks.ModelCheckpoint(direccion,save_best_only=True,verbose=2)\n",
    "stp = keras.callbacks.EarlyStopping(patience=20,mode='auto',min_delta=0,restore_best_weights=True,verbose=2)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(factor=0.9,patience=8,verbose=2)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "epochs = 100\n",
    "#batch_size = 128\n",
    "shuffle =True\n",
    "callbacks = [chk,stp,lrs]\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# wrapping... de keras a scikit model... esto convierte a model a tipo Scikite (posee .fit() , .predict() ,  .evaluate())\n",
    "model= tf.keras.wrappers.scikit_learn.KerasRegressor(crear_modelo,epochs=epochs,callbacks=callbacks,shuffle=True,validation_split=0.2)\n",
    "\n",
    "# entrenar modelo\n",
    "history = model.fit(X_train, y_train)\n",
    "\n",
    "#############################################################\n",
    "####### Ver curvas de Aprendizaje del modelo #############\n",
    "\n",
    "historia = history.history\n",
    "\n",
    "print('\\n\\nCurvas de Aprendizaje\\n')\n",
    "for nombre,valores in historia.items():\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.title('Curva de ' + nombre)\n",
    "    plt.xlabel('Interacion \"i\"')\n",
    "    plt.ylabel(nombre)\n",
    "    plt.plot(valores)\n",
    "    plt.show()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# test de rendimiento - mse\n",
    "mse_total = model.score(X_test,y_test)\n",
    "print('\\n\\nPuntaje total: ',mse_total)\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T21:57:56.707857Z",
     "start_time": "2021-02-15T21:57:56.387102Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 52us/sample - loss: 0.3159 - mse: 0.26340s - loss: 0.3155 - mse: 0.263\n",
      "\n",
      "Puntaje de modelo: -0.32\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "[2.537 1.006 2.325 0.61  1.809 2.086 0.473 1.813 1.995 1.491]\n",
      "\n",
      "Valores Orginales: \n",
      "[2.609 0.766 1.809 0.669 1.827 2.393 0.541 1.796 1.739 1.635]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediccion\n",
    "\n",
    "valor = model.score(X_test,y_test)\n",
    "print('\\nPuntaje de modelo:' ,np.round(valor,2))\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(model.predict(X_test[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Opcion 03 -  Keras  - Opcion Multicapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T03:39:10.236090Z",
     "start_time": "2021-02-18T03:38:32.851054Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dim X train:  (15480, 8)\n",
      "Dim Y train:  (15480,)\n",
      "\n",
      "\n",
      "Train on 12384 samples, validate on 3096 samples\n",
      "Epoch 1/40\n",
      "12160/12384 [============================>.] - ETA: 0s - loss: 1.2059 - mse: 1.1435\n",
      "Epoch 00001: val_loss improved from inf to 0.72979, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 103us/sample - loss: 1.2022 - mse: 1.1398 - val_loss: 0.7298 - val_mse: 0.6693\n",
      "Epoch 2/40\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.9776 - mse: 0.9184\n",
      "Epoch 00002: val_loss improved from 0.72979 to 0.70658, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 74us/sample - loss: 0.9789 - mse: 0.9197 - val_loss: 0.7066 - val_mse: 0.6484\n",
      "Epoch 3/40\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.9303 - mse: 0.8731\n",
      "Epoch 00003: val_loss improved from 0.70658 to 0.66253, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 79us/sample - loss: 0.9296 - mse: 0.8724 - val_loss: 0.6625 - val_mse: 0.6062\n",
      "Epoch 4/40\n",
      "12320/12384 [============================>.] - ETA: 0s - loss: 0.8880 - mse: 0.8325\n",
      "Epoch 00004: val_loss improved from 0.66253 to 0.64781, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 76us/sample - loss: 0.8864 - mse: 0.8309 - val_loss: 0.6478 - val_mse: 0.5931\n",
      "Epoch 5/40\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.8779 - mse: 0.8240\n",
      "Epoch 00005: val_loss did not improve from 0.64781\n",
      "12384/12384 [==============================] - 1s 75us/sample - loss: 0.8796 - mse: 0.8257 - val_loss: 0.6524 - val_mse: 0.5992\n",
      "Epoch 6/40\n",
      "11840/12384 [===========================>..] - ETA: 0s - loss: 0.8564 - mse: 0.8038\n",
      "Epoch 00006: val_loss improved from 0.64781 to 0.61925, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 75us/sample - loss: 0.8567 - mse: 0.8042 - val_loss: 0.6192 - val_mse: 0.5673\n",
      "Epoch 7/40\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.8483 - mse: 0.7970\n",
      "Epoch 00007: val_loss did not improve from 0.61925\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.8454 - mse: 0.7942 - val_loss: 0.6303 - val_mse: 0.5798\n",
      "Epoch 8/40\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.8291 - mse: 0.7791\n",
      "Epoch 00008: val_loss did not improve from 0.61925\n",
      "12384/12384 [==============================] - 1s 75us/sample - loss: 0.8288 - mse: 0.7788 - val_loss: 0.6256 - val_mse: 0.5761\n",
      "Epoch 9/40\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.8194 - mse: 0.7704\n",
      "Epoch 00009: val_loss improved from 0.61925 to 0.61474, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.8224 - mse: 0.7734 - val_loss: 0.6147 - val_mse: 0.5663\n",
      "Epoch 10/40\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.8285 - mse: 0.7806\n",
      "Epoch 00010: val_loss improved from 0.61474 to 0.60247, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.8312 - mse: 0.7833 - val_loss: 0.6025 - val_mse: 0.5549\n",
      "Epoch 11/40\n",
      "11520/12384 [==========================>...] - ETA: 0s - loss: 0.8090 - mse: 0.7619\n",
      "Epoch 00011: val_loss improved from 0.60247 to 0.59902, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.8097 - mse: 0.7626 - val_loss: 0.5990 - val_mse: 0.5524\n",
      "Epoch 12/40\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.8092 - mse: 0.7629\n",
      "Epoch 00012: val_loss improved from 0.59902 to 0.59521, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.8091 - mse: 0.7628 - val_loss: 0.5952 - val_mse: 0.5493\n",
      "Epoch 13/40\n",
      "12288/12384 [============================>.] - ETA: 0s - loss: 0.8050 - mse: 0.7595\n",
      "Epoch 00013: val_loss did not improve from 0.59521\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.8080 - mse: 0.7625 - val_loss: 0.5986 - val_mse: 0.5534\n",
      "Epoch 14/40\n",
      "12096/12384 [============================>.] - ETA: 0s - loss: 0.7972 - mse: 0.7524\n",
      "Epoch 00014: val_loss improved from 0.59521 to 0.58547, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 74us/sample - loss: 0.7968 - mse: 0.7520 - val_loss: 0.5855 - val_mse: 0.5410\n",
      "Epoch 15/40\n",
      "12192/12384 [============================>.] - ETA: 0s - loss: 0.7868 - mse: 0.7426\n",
      "Epoch 00015: val_loss did not improve from 0.58547\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.7875 - mse: 0.7433 - val_loss: 0.5858 - val_mse: 0.5419\n",
      "Epoch 16/40\n",
      "12288/12384 [============================>.] - ETA: 0s - loss: 0.7934 - mse: 0.7498- ETA: 0s - loss: 0.8148 - mse: \n",
      "Epoch 00016: val_loss improved from 0.58547 to 0.58433, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.7938 - mse: 0.7502 - val_loss: 0.5843 - val_mse: 0.5410\n",
      "Epoch 17/40\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.7782 - mse: 0.7351\n",
      "Epoch 00017: val_loss improved from 0.58433 to 0.57271, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.7778 - mse: 0.7348 - val_loss: 0.5727 - val_mse: 0.5299\n",
      "Epoch 18/40\n",
      "11936/12384 [===========================>..] - ETA: 0s - loss: 0.7728 - mse: 0.7303\n",
      "Epoch 00018: val_loss did not improve from 0.57271\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.7760 - mse: 0.7335 - val_loss: 0.5902 - val_mse: 0.5480\n",
      "Epoch 19/40\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.7873 - mse: 0.7453\n",
      "Epoch 00019: val_loss improved from 0.57271 to 0.57253, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 75us/sample - loss: 0.7794 - mse: 0.7374 - val_loss: 0.5725 - val_mse: 0.5307\n",
      "Epoch 20/40\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.7792 - mse: 0.7377\n",
      "Epoch 00020: val_loss improved from 0.57253 to 0.56893, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.7816 - mse: 0.7400 - val_loss: 0.5689 - val_mse: 0.5276\n",
      "Epoch 21/40\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.7714 - mse: 0.7303\n",
      "Epoch 00021: val_loss did not improve from 0.56893\n",
      "12384/12384 [==============================] - 1s 71us/sample - loss: 0.7700 - mse: 0.7289 - val_loss: 0.5806 - val_mse: 0.5397\n",
      "Epoch 22/40\n",
      "12352/12384 [============================>.] - ETA: 0s - loss: 0.7601 - mse: 0.7193\n",
      "Epoch 00022: val_loss did not improve from 0.56893\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.7610 - mse: 0.7203 - val_loss: 0.5869 - val_mse: 0.5464\n",
      "Epoch 23/40\n",
      "11904/12384 [===========================>..] - ETA: 0s - loss: 0.7693 - mse: 0.7289\n",
      "Epoch 00023: val_loss improved from 0.56893 to 0.54526, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.7683 - mse: 0.7280 - val_loss: 0.5453 - val_mse: 0.5050\n",
      "Epoch 24/40\n",
      "11520/12384 [==========================>...] - ETA: 0s - loss: 0.7645 - mse: 0.7245\n",
      "Epoch 00024: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 70us/sample - loss: 0.7610 - mse: 0.7210 - val_loss: 0.5677 - val_mse: 0.5279\n",
      "Epoch 25/40\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.7543 - mse: 0.7146\n",
      "Epoch 00025: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.7563 - mse: 0.7167 - val_loss: 0.5525 - val_mse: 0.5130\n",
      "Epoch 26/40\n",
      "11648/12384 [===========================>..] - ETA: 0s - loss: 0.7730 - mse: 0.7337\n",
      "Epoch 00026: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.7719 - mse: 0.7326 - val_loss: 0.5561 - val_mse: 0.5169\n",
      "Epoch 27/40\n",
      "11520/12384 [==========================>...] - ETA: 0s - loss: 0.7493 - mse: 0.7103\n",
      "Epoch 00027: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.7500 - mse: 0.7110 - val_loss: 0.5525 - val_mse: 0.5136\n",
      "Epoch 28/40\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.7532 - mse: 0.7145\n",
      "Epoch 00028: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.7586 - mse: 0.7200 - val_loss: 0.5695 - val_mse: 0.5310\n",
      "Epoch 29/40\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.7694 - mse: 0.7310\n",
      "Epoch 00029: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.7668 - mse: 0.7285 - val_loss: 0.5655 - val_mse: 0.5273\n",
      "Epoch 30/40\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.7536 - mse: 0.7155\n",
      "Epoch 00030: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 74us/sample - loss: 0.7548 - mse: 0.7168 - val_loss: 0.5521 - val_mse: 0.5142\n",
      "Epoch 31/40\n",
      "12320/12384 [============================>.] - ETA: 0s - loss: 0.7467 - mse: 0.7089\n",
      "Epoch 00031: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 75us/sample - loss: 0.7457 - mse: 0.7079 - val_loss: 0.5514 - val_mse: 0.5137\n",
      "Epoch 32/40\n",
      "11680/12384 [===========================>..] - ETA: 0s - loss: 0.7533 - mse: 0.7158\n",
      "Epoch 00032: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 74us/sample - loss: 0.7539 - mse: 0.7164 - val_loss: 0.5570 - val_mse: 0.5197\n",
      "Epoch 33/40\n",
      "11712/12384 [===========================>..] - ETA: 0s - loss: 0.7530 - mse: 0.7158\n",
      "Epoch 00033: val_loss did not improve from 0.54526\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.008999999798834325.\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.7542 - mse: 0.7170 - val_loss: 0.5650 - val_mse: 0.5280\n",
      "Epoch 34/40\n",
      "11584/12384 [===========================>..] - ETA: 0s - loss: 0.7556 - mse: 0.7186\n",
      "Epoch 00034: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 74us/sample - loss: 0.7543 - mse: 0.7173 - val_loss: 0.5751 - val_mse: 0.5382\n",
      "Epoch 35/40\n",
      "11744/12384 [===========================>..] - ETA: 0s - loss: 0.7552 - mse: 0.7184\n",
      "Epoch 00035: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.7503 - mse: 0.7135 - val_loss: 0.5465 - val_mse: 0.5098\n",
      "Epoch 36/40\n",
      "11808/12384 [===========================>..] - ETA: 0s - loss: 0.7352 - mse: 0.6985\n",
      "Epoch 00036: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.7342 - mse: 0.6975 - val_loss: 0.5627 - val_mse: 0.5262\n",
      "Epoch 37/40\n",
      "11872/12384 [===========================>..] - ETA: 0s - loss: 0.7265 - mse: 0.6900\n",
      "Epoch 00037: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 72us/sample - loss: 0.7297 - mse: 0.6932 - val_loss: 0.5518 - val_mse: 0.5154\n",
      "Epoch 38/40\n",
      "11552/12384 [==========================>...] - ETA: 0s - loss: 0.7457 - mse: 0.7093\n",
      "Epoch 00038: val_loss did not improve from 0.54526\n",
      "12384/12384 [==============================] - 1s 69us/sample - loss: 0.7471 - mse: 0.7108 - val_loss: 0.5524 - val_mse: 0.5162\n",
      "Epoch 39/40\n",
      "12256/12384 [============================>.] - ETA: 0s - loss: 0.7428 - mse: 0.7066\n",
      "Epoch 00039: val_loss improved from 0.54526 to 0.53431, saving model to mejor_modelo_entrenado.h5\n",
      "12384/12384 [==============================] - 1s 73us/sample - loss: 0.7429 - mse: 0.7067 - val_loss: 0.5343 - val_mse: 0.4982\n",
      "Epoch 40/40\n",
      "11616/12384 [===========================>..] - ETA: 0s - loss: 0.7421 - mse: 0.7061\n",
      "Epoch 00040: val_loss did not improve from 0.53431\n",
      "12384/12384 [==============================] - 1s 76us/sample - loss: 0.7385 - mse: 0.7026 - val_loss: 0.5474 - val_mse: 0.5116\n",
      "\n",
      "\n",
      "Curvas de Aprendizaje\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 50us/sample - loss: 0.5639 - mse: 0.5281\n",
      "\n",
      "\n",
      "Puntaje total:  -0.5639362792636073\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modelo keras\n",
    "\n",
    "# cargar datos\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "# estandarizar los datos ... es vital para entrenamiento inicial, en otro caso puede diverger los pesos del modelo\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # fit + transform\n",
    "X_test = scaler.transform(X_test) # transform (usando entrenado)\n",
    "\n",
    "print('\\n')\n",
    "print('Dim X train: ',X_train.shape)\n",
    "print('Dim Y train: ',y_train.shape)\n",
    "print('\\n')\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# dimension entrada...se indica en la primera capa de la NN\n",
    "dimension = X_train.shape[1:]\n",
    "\n",
    "def crear_modelo_deep(optimizer='sgd',h=[30,40,10,1],activa=['tanh','tanh','tanh',None],init ='glorot_uniform',alpha=0.001,dropout=0.2): \n",
    "    \n",
    "    # Funcion de activacion \"None\" equivale a decirle que la activacion es la funcion lineal a(x)=x ... es la funcion lineal pura\n",
    "    \n",
    "    # modelo secuencial\n",
    "    modeli = keras.Sequential()\n",
    "    \n",
    "    # definir dimension de entradas\n",
    "    modeli.add(keras.Input(shape=(dimension)))\n",
    "    \n",
    "    # De la primera capa a la penultima\n",
    "    for ix in range(len(h)-1):\n",
    "        # capa\n",
    "        modeli.add(Dense(h[ix],activation=activa[ix],kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "\n",
    "        # Agregar capa de Drop-Out a capas intermedias con un ratio igual a \"dropout\"=probabilidad desactivacion de neurona aleatoreamente\n",
    "        modeli.add(keras.layers.Dropout(rate=dropout))\n",
    "    \n",
    "    # Capa de salida o final\n",
    "    modeli.add(Dense(h[-1],activation=activa[-1]))\n",
    "\n",
    "    #compilar y retornar objeto modelo\n",
    "    modeli.compile(loss='mse', optimizer = optimizer ,metrics=['mse'])\n",
    "    return modeli\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# Callbacks para el entrenamiento\n",
    "direccion = 'mejor_modelo_entrenado.h5' # nombre de archivo a guardar modelo entrenado\n",
    "chk = keras.callbacks.ModelCheckpoint(direccion,save_best_only=True,verbose=2)\n",
    "stp = keras.callbacks.EarlyStopping(patience=20,mode='auto',min_delta=0,restore_best_weights=True,verbose=2)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(factor=0.9,patience=10,verbose=2)\n",
    "\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "epochs = 20\n",
    "dropout= 0.2 # desactivacion/activacion aleatoria de neuronas durante entrenamiento\n",
    "batch_size = 128\n",
    "shuffle =True\n",
    "callbacks = [chk,stp,lrs]\n",
    "\n",
    "# wrapping... de keras a scikit model... esto convierte a model a tipo Scikite (posee .fit() , .predict() ,  .evaluate())\n",
    "model= tf.keras.wrappers.scikit_learn.KerasRegressor(crear_modelo_deep,epochs=epochs,callbacks=callbacks,shuffle=True,validation_split=0.2,dropout=dropout)\n",
    "\n",
    "# entrenar modelo\n",
    "history = model.fit(X_train, y_train)\n",
    "\n",
    "#############################################################\n",
    "####### Ver curvas de Aprendizaje del modelo #############\n",
    "\n",
    "historia = history.history\n",
    "\n",
    "print('\\n\\nCurvas de Aprendizaje\\n')\n",
    "for nombre,valores in historia.items():\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.title('Curva de ' + nombre)\n",
    "    plt.xlabel('Interacion \"i\"')\n",
    "    plt.ylabel(nombre)\n",
    "    plt.plot(valores)\n",
    "    plt.show()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# test de rendimiento - mse\n",
    "puntaje = model.score(X_test,y_test)\n",
    "print('\\n\\nPuntaje total: ',puntaje)\n",
    "print('\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T03:39:10.594048Z",
     "start_time": "2021-02-18T03:39:10.237103Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 53us/sample - loss: 0.5639 - mse: 0.5281\n",
      "\n",
      "Puntaje de modelo: -0.564\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "[2.654 1.658 1.367 1.671 1.909 2.525 1.783 1.261 2.013 1.837]\n",
      "\n",
      "Valores Orginales: \n",
      "[1.724 1.531 1.48  1.412 2.743 3.534 1.922 1.554 2.415 1.187]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediccion\n",
    "\n",
    "valor = model.score(X_test,y_test)\n",
    "print('\\nPuntaje de modelo:' ,np.round(valor,3))\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(model.predict(X_test[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Opcion 04 - GridSearchCV + Pipeline + Keras  - Con Opcion GridSearch Multicapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T03:46:35.624134Z",
     "start_time": "2021-02-18T03:43:44.784816Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dim X train:  (15480, 8)\n",
      "Dim Y train:  (15480,)\n",
      "\n",
      "\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 150us/sample - loss: 0.8390 - mse: 0.6683 - val_loss: 0.6815 - val_mse: 0.5180\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.5707 - mse: 0.4156 - val_loss: 0.6241 - val_mse: 0.4773\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 96us/sample - loss: 0.5183 - mse: 0.3802 - val_loss: 0.5756 - val_mse: 0.4457\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.4857 - mse: 0.3628 - val_loss: 0.5312 - val_mse: 0.4155\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4678 - mse: 0.3586 - val_loss: 0.5129 - val_mse: 0.4097\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.4381 - mse: 0.3350\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 147us/sample - loss: 0.8699 - mse: 0.7007 - val_loss: 0.6678 - val_mse: 0.5056\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.5627 - mse: 0.4086 - val_loss: 0.6333 - val_mse: 0.4886\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.5116 - mse: 0.3753 - val_loss: 0.5523 - val_mse: 0.4243\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.4837 - mse: 0.3627 - val_loss: 0.5164 - val_mse: 0.4019\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4598 - mse: 0.3505 - val_loss: 0.5004 - val_mse: 0.3960\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4519 - mse: 0.3474\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 149us/sample - loss: 0.8321 - mse: 0.6616 - val_loss: 0.6475 - val_mse: 0.4831\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.5459 - mse: 0.3890 - val_loss: 0.5700 - val_mse: 0.4222\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 98us/sample - loss: 0.4990 - mse: 0.3596 - val_loss: 0.5469 - val_mse: 0.4162\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.4682 - mse: 0.3440 - val_loss: 0.5150 - val_mse: 0.3975\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.4417 - mse: 0.3295 - val_loss: 0.4907 - val_mse: 0.3837\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4356 - mse: 0.3286\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 159us/sample - loss: 0.8599 - mse: 0.6883 - val_loss: 0.6711 - val_mse: 0.5060\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.5629 - mse: 0.4065 - val_loss: 0.6242 - val_mse: 0.4773\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 105us/sample - loss: 0.5205 - mse: 0.3824 - val_loss: 0.5698 - val_mse: 0.4406\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 0.4923 - mse: 0.3705- ETA: 0s - loss: 0.5195  - 1s 102us/sample - loss: 0.4940 - mse: 0.3722 - val_loss: 0.5500 - val_mse: 0.4352\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4750 - mse: 0.3662 - val_loss: 0.5441 - val_mse: 0.4414\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.4798 - mse: 0.3772\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 147us/sample - loss: 0.8356 - mse: 0.6632 - val_loss: 0.6370 - val_mse: 0.4719\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.5906 - mse: 0.4329 - val_loss: 0.5375 - val_mse: 0.3875\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.5267 - mse: 0.3846 - val_loss: 0.5244 - val_mse: 0.3902\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4966 - mse: 0.3692 - val_loss: 0.4684 - val_mse: 0.3469 loss: 0.5029 - mse: \n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4739 - mse: 0.3578 - val_loss: 0.4474 - val_mse: 0.3364\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5139 - mse: 0.4028\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 0.9786 - mse: 0.8071 - val_loss: 0.7090 - val_mse: 0.5422\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.6074 - mse: 0.4455 - val_loss: 0.6519 - val_mse: 0.4954\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5685 - mse: 0.4177 - val_loss: 0.6209 - val_mse: 0.4753\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.5356 - mse: 0.3946 - val_loss: 0.5772 - val_mse: 0.4407\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5062 - mse: 0.3741 - val_loss: 0.5679 - val_mse: 0.4395\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4978 - mse: 0.3694\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 132us/sample - loss: 0.9341 - mse: 0.7660 - val_loss: 0.6963 - val_mse: 0.5344\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.6195 - mse: 0.4634 - val_loss: 0.6849 - val_mse: 0.5344\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5687 - mse: 0.4228 - val_loss: 0.7145 - val_mse: 0.5732\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 0.5198 - mse: 0.382 - 1s 92us/sample - loss: 0.5210 - mse: 0.3835 - val_loss: 0.5682 - val_mse: 0.4350\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.4901 - mse: 0.3605 - val_loss: 0.5435 - val_mse: 0.4177\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.4904 - mse: 0.3646\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 0.9444 - mse: 0.773 - 1s 123us/sample - loss: 0.9370 - mse: 0.7663 - val_loss: 0.6907 - val_mse: 0.5254\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5815 - mse: 0.4210 - val_loss: 0.5894 - val_mse: 0.4333\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5200 - mse: 0.3691 - val_loss: 0.5598 - val_mse: 0.4144\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 96us/sample - loss: 0.4902 - mse: 0.3503 - val_loss: 0.5445 - val_mse: 0.4099\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.4735 - mse: 0.3444 - val_loss: 0.5300 - val_mse: 0.4056\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.4724 - mse: 0.3480\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 128us/sample - loss: 0.9936 - mse: 0.8235 - val_loss: 0.7028 - val_mse: 0.5393\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.6143 - mse: 0.4568 - val_loss: 0.6816 - val_mse: 0.5294\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5499 - mse: 0.4028 - val_loss: 0.5864 - val_mse: 0.4445\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 94us/sample - loss: 0.5167 - mse: 0.3793 - val_loss: 0.5624 - val_mse: 0.4299\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.4918 - mse: 0.3638 - val_loss: 0.5587 - val_mse: 0.4352\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4920 - mse: 0.3686\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 130us/sample - loss: 0.9679 - mse: 0.7966 - val_loss: 0.6634 - val_mse: 0.4984\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5986 - mse: 0.4399 - val_loss: 0.5577 - val_mse: 0.4046\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.5398 - mse: 0.3922 - val_loss: 0.5229 - val_mse: 0.3811\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.5180 - mse: 0.3811 - val_loss: 0.4923 - val_mse: 0.3595\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.4898 - mse: 0.3610 - val_loss: 0.4611 - val_mse: 0.3366\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5214 - mse: 0.3969\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 153us/sample - loss: 0.8474 - mse: 0.6993 - val_loss: 0.6805 - val_mse: 0.5372\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 105us/sample - loss: 0.5743 - mse: 0.4372 - val_loss: 0.6075 - val_mse: 0.4771\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.5200 - mse: 0.3955 - val_loss: 0.5527 - val_mse: 0.4342\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 0.4842 - mse: 0.370 - 1s 102us/sample - loss: 0.4832 - mse: 0.3698 - val_loss: 0.5314 - val_mse: 0.4225\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.4550 - mse: 0.3499 - val_loss: 0.4993 - val_mse: 0.3977\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4324 - mse: 0.3308\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 149us/sample - loss: 0.8094 - mse: 0.6605 - val_loss: 0.6542 - val_mse: 0.5107\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.5490 - mse: 0.4116 - val_loss: 0.5717 - val_mse: 0.4409\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.4982 - mse: 0.3749 - val_loss: 0.5560 - val_mse: 0.4398\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4728 - mse: 0.3626 - val_loss: 0.5318 - val_mse: 0.4277\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 107us/sample - loss: 0.4545 - mse: 0.3556 - val_loss: 0.5064 - val_mse: 0.4125\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4549 - mse: 0.3610\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 147us/sample - loss: 0.8651 - mse: 0.7160 - val_loss: 0.6436 - val_mse: 0.5001\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.5139 - mse: 0.3786 - val_loss: 0.5647 - val_mse: 0.4380\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.4723 - mse: 0.3527 - val_loss: 0.5089 - val_mse: 0.3959\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.4453 - mse: 0.3380 - val_loss: 0.5042 - val_mse: 0.4026 loss: 0.4540 - mse: 0.344 - ETA: 0s - loss: 0.4469 - mse:\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.4298 - mse: 0.3329 - val_loss: 0.5105 - val_mse: 0.4183\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.4685 - mse: 0.3763\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 145us/sample - loss: 0.8534 - mse: 0.7045 - val_loss: 0.6517 - val_mse: 0.5079\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 96us/sample - loss: 0.5334 - mse: 0.3952 - val_loss: 0.5603 - val_mse: 0.4286\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 99us/sample - loss: 0.4922 - mse: 0.3666 - val_loss: 0.5699 - val_mse: 0.4506\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 96us/sample - loss: 0.4669 - mse: 0.3525 - val_loss: 0.5356 - val_mse: 0.4257\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.4460 - mse: 0.3403 - val_loss: 0.5493 - val_mse: 0.4474\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4844 - mse: 0.3825\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 149us/sample - loss: 0.8284 - mse: 0.6809 - val_loss: 0.7522 - val_mse: 0.6097\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.5668 - mse: 0.4297 - val_loss: 0.5531 - val_mse: 0.4215\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.5043 - mse: 0.3781 - val_loss: 0.5082 - val_mse: 0.3874\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.4688 - mse: 0.3534 - val_loss: 0.4483 - val_mse: 0.3375\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.4485 - mse: 0.3416 - val_loss: 0.4416 - val_mse: 0.3382\n",
      "3096/3096 [==============================] - 0s 50us/sample - loss: 0.4897 - mse: 0.3863\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 0.9803 - mse: 0.8314 - val_loss: 0.6963 - val_mse: 0.5513\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5867 - mse: 0.4448 - val_loss: 0.6296 - val_mse: 0.4912\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5335 - mse: 0.3985 - val_loss: 0.6111 - val_mse: 0.4800\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5030 - mse: 0.3754 - val_loss: 0.5789 - val_mse: 0.4550\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.4874 - mse: 0.3668 - val_loss: 0.5423 - val_mse: 0.4252\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4625 - mse: 0.3455\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 141us/sample - loss: 1.0009 - mse: 0.8503 - val_loss: 0.7189 - val_mse: 0.5719\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5918 - mse: 0.4484 - val_loss: 0.6184 - val_mse: 0.4788\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.5258 - mse: 0.3904 - val_loss: 0.5762 - val_mse: 0.4451\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.4959 - mse: 0.3689 - val_loss: 0.5542 - val_mse: 0.4311\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.4729 - mse: 0.3533 - val_loss: 0.5246 - val_mse: 0.4089\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4658 - mse: 0.3501\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 124us/sample - loss: 0.9691 - mse: 0.8206 - val_loss: 0.6712 - val_mse: 0.5262\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.5480 - mse: 0.4053 - val_loss: 0.5621 - val_mse: 0.4226\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.4912 - mse: 0.3556 - val_loss: 0.5372 - val_mse: 0.4060\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.4698 - mse: 0.3428 - val_loss: 0.5108 - val_mse: 0.3882\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.4490 - mse: 0.3304 - val_loss: 0.4934 - val_mse: 0.3791\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4465 - mse: 0.3322\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 127us/sample - loss: 1.0271 - mse: 0.8745 - val_loss: 0.7049 - val_mse: 0.5566\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.5840 - mse: 0.4399 - val_loss: 0.6130 - val_mse: 0.4729\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5225 - mse: 0.3862 - val_loss: 0.5816 - val_mse: 0.4495\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.4951 - mse: 0.3668 - val_loss: 0.5600 - val_mse: 0.4357\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.4703 - mse: 0.3499 - val_loss: 0.5349 - val_mse: 0.4186\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.4674 - mse: 0.3511\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 126us/sample - loss: 1.0046 - mse: 0.8510 - val_loss: 0.6167 - val_mse: 0.4677\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.5803 - mse: 0.4359 - val_loss: 0.5623 - val_mse: 0.4225\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.5235 - mse: 0.3884 - val_loss: 0.5155 - val_mse: 0.3858\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 95us/sample - loss: 0.4943 - mse: 0.3694 - val_loss: 0.4707 - val_mse: 0.3505\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.4794 - mse: 0.3634 - val_loss: 0.4677 - val_mse: 0.3558\n",
      "3096/3096 [==============================] - 0s 51us/sample - loss: 0.5312 - mse: 0.4193\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 147us/sample - loss: 1.4120 - mse: 1.2716 - val_loss: 0.8310 - val_mse: 0.7113\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.6197 - mse: 0.5115 - val_loss: 0.5980 - val_mse: 0.4991\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.4988 - mse: 0.4061 - val_loss: 0.5322 - val_mse: 0.4452\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4604 - mse: 0.3780 - val_loss: 0.5595 - val_mse: 0.4813\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.4433 - mse: 0.3690 - val_loss: 0.4779 - val_mse: 0.4065\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.4070 - mse: 0.3357\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 148us/sample - loss: 0.8974 - mse: 0.7471 - val_loss: 0.7020 - val_mse: 0.5601\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.5637 - mse: 0.4309 - val_loss: 0.5748 - val_mse: 0.4507\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.5000 - mse: 0.3835 - val_loss: 0.5511 - val_mse: 0.4407\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.4694 - mse: 0.3654 - val_loss: 0.5065 - val_mse: 0.4080\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 99us/sample - loss: 0.4480 - mse: 0.3542 - val_loss: 0.4967 - val_mse: 0.4076\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4460 - mse: 0.3569\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 148us/sample - loss: 0.8493 - mse: 0.6997 - val_loss: 0.6095 - val_mse: 0.4683\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.5185 - mse: 0.3869 - val_loss: 0.5540 - val_mse: 0.4319\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4726 - mse: 0.3588 - val_loss: 0.5175 - val_mse: 0.4118\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.4440 - mse: 0.3456 - val_loss: 0.4911 - val_mse: 0.3987\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 105us/sample - loss: 0.4280 - mse: 0.3410 - val_loss: 0.4710 - val_mse: 0.3890\n",
      "3096/3096 [==============================] - 0s 51us/sample - loss: 0.4257 - mse: 0.3437\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 170us/sample - loss: 1.0621 - mse: 0.9130 - val_loss: 0.7335 - val_mse: 0.5954\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.5917 - mse: 0.4627 - val_loss: 0.5997 - val_mse: 0.4784\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.4979 - mse: 0.3835 - val_loss: 0.5364 - val_mse: 0.4285\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.4613 - mse: 0.3586 - val_loss: 0.5037 - val_mse: 0.4058\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.4398 - mse: 0.3464 - val_loss: 0.5371 - val_mse: 0.4480\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.4680 - mse: 0.3788\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 147us/sample - loss: 0.8515 - mse: 0.7005 - val_loss: 0.6580 - val_mse: 0.5127\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.5596 - mse: 0.4228 - val_loss: 0.5707 - val_mse: 0.4428\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.4981 - mse: 0.3787 - val_loss: 0.4867 - val_mse: 0.3756\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4670 - mse: 0.3631 - val_loss: 0.4593 - val_mse: 0.3616\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.4499 - mse: 0.3579 - val_loss: 0.4318 - val_mse: 0.3445\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.4932 - mse: 0.4059\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 124us/sample - loss: 1.0506 - mse: 0.8973 - val_loss: 0.7382 - val_mse: 0.5898\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.6361 - mse: 0.4924 - val_loss: 0.6426 - val_mse: 0.5039\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5520 - mse: 0.4185 - val_loss: 0.5878 - val_mse: 0.4597\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5052 - mse: 0.3819 - val_loss: 0.5495 - val_mse: 0.4308\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.4784 - mse: 0.3639 - val_loss: 0.5274 - val_mse: 0.4176\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4493 - mse: 0.3395\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 125us/sample - loss: 1.0670 - mse: 0.9142 - val_loss: 0.7315 - val_mse: 0.5843\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.6019 - mse: 0.4609 - val_loss: 0.6196 - val_mse: 0.4850\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5249 - mse: 0.3965 - val_loss: 0.5610 - val_mse: 0.4382 - loss: 0.5304 - \n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.4872 - mse: 0.3704 - val_loss: 0.5328 - val_mse: 0.4219\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 97us/sample - loss: 0.4654 - mse: 0.3595 - val_loss: 0.5256 - val_mse: 0.4244\n",
      "3096/3096 [==============================] - 0s 50us/sample - loss: 0.4641 - mse: 0.3629\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 126us/sample - loss: 1.5189 - mse: 1.3684 - val_loss: 0.8821 - val_mse: 0.7403\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.6962 - mse: 0.5644 - val_loss: 0.6406 - val_mse: 0.5185\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5524 - mse: 0.4389 - val_loss: 0.5550 - val_mse: 0.4498\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 94us/sample - loss: 0.4872 - mse: 0.3886 - val_loss: 0.5334 - val_mse: 0.4416\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.4444 - mse: 0.3576 - val_loss: 0.4988 - val_mse: 0.4173\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.4435 - mse: 0.3620\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 128us/sample - loss: 1.0633 - mse: 0.9090 - val_loss: 0.7440 - val_mse: 0.5946\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.6157 - mse: 0.4723 - val_loss: 0.6166 - val_mse: 0.4790\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.5349 - mse: 0.4032 - val_loss: 0.5546 - val_mse: 0.4284\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.4920 - mse: 0.3715 - val_loss: 0.5420 - val_mse: 0.4270\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.4674 - mse: 0.3577 - val_loss: 0.5233 - val_mse: 0.4192\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.4676 - mse: 0.3635\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 124us/sample - loss: 0.9634 - mse: 0.8088 - val_loss: 0.6881 - val_mse: 0.5372\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.6190 - mse: 0.4718 - val_loss: 0.5976 - val_mse: 0.4543\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.5395 - mse: 0.4001 - val_loss: 0.5216 - val_mse: 0.3860\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.4986 - mse: 0.3667 - val_loss: 0.4719 - val_mse: 0.3442\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.4758 - mse: 0.3522 - val_loss: 0.4895 - val_mse: 0.3704\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5444 - mse: 0.4254\n",
      "Train on 12384 samples, validate on 3096 samples\n",
      "Epoch 1/5\n",
      "12384/12384 [==============================] - 2s 123us/sample - loss: 0.9889 - mse: 0.8412 - val_loss: 0.6741 - val_mse: 0.5397\n",
      "Epoch 2/5\n",
      "12384/12384 [==============================] - 1s 86us/sample - loss: 0.5412 - mse: 0.4194 - val_loss: 0.5557 - val_mse: 0.4446\n",
      "Epoch 3/5\n",
      "12384/12384 [==============================] - 1s 88us/sample - loss: 0.4728 - mse: 0.3705 - val_loss: 0.5018 - val_mse: 0.4072\n",
      "Epoch 4/5\n",
      "12384/12384 [==============================] - 1s 87us/sample - loss: 0.4429 - mse: 0.3544 - val_loss: 0.4982 - val_mse: 0.4154\n",
      "Epoch 5/5\n",
      "12384/12384 [==============================] - ETA: 0s - loss: 0.4277 - mse: 0.348 - 1s 86us/sample - loss: 0.4265 - mse: 0.3478 - val_loss: 0.4670 - val_mse: 0.3916\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e5901f588b67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;31m####### Ver curvas de Aprendizaje del modelo #############\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m \u001b[0mhistoria\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\nCurvas de Aprendizaje\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# modelo keras\n",
    "\n",
    "# cargar datos\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "# estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # fit + transform\n",
    "X_test = scaler.transform(X_test) # transform (usando entrenado)\n",
    "\n",
    "print('\\n')\n",
    "print('Dim X train: ',X_train.shape)\n",
    "print('Dim Y train: ',y_train.shape)\n",
    "print('\\n')\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# dimension entrada ... se indica en la primera capa de la red neuronal\n",
    "dimension = X_train.shape[1:]\n",
    "\n",
    "def crear_modelo_deep(optimizer='sgd',h=[30,40,10,1],activa=['tanh','tanh','tanh',None],init ='glorot_uniform',alpha=0.001,dropout=0.2): \n",
    "    \n",
    "    # Funcion de activacion \"None\" equivale a decirle que la activacion es la funcion lineal a(x)=x ... es la funcion lineal pura\n",
    "    \n",
    "    # modelo secuencial\n",
    "    modeli = keras.Sequential()\n",
    "    \n",
    "    # definir dimension de entradas\n",
    "    modeli.add(keras.Input(shape=(dimension)))\n",
    "    \n",
    "    # De la capa inicial a la penultima capa\n",
    "    for ix in range(len(h)-1):\n",
    "        modeli.add(Dense(h[ix],activation=activa[ix],kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "\n",
    "        # Agregar capa de Drop-Out a capas intermedias con un ratio igual a \"dropout\"=probabilidad desactivacion de neurona aleatoreamente\n",
    "        modeli.add(keras.layers.Dropout(rate=dropout))\n",
    "    \n",
    "    # Capa de salida o final\n",
    "    modeli.add(Dense(h[-1],activation=activa[-1]))\n",
    "\n",
    " \n",
    "    #compilar y retornar objeto modelo\n",
    "    modeli.compile(loss='mse', optimizer = optimizer ,metrics=['mse'])\n",
    "    return modeli\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "########  NO USAR ########\n",
    "# En gridsearch no se recomienda hacer uso de callbacks, pueden producir error\n",
    "\n",
    "# Callbacks para el entrenamiento \n",
    "direccion = 'mejor_modelo_entrenado.h5' # nombre de archivo a guardar modelo entrenado\n",
    "chk = keras.callbacks.ModelCheckpoint(direccion,save_best_only=True,verbose=2)\n",
    "stp = keras.callbacks.EarlyStopping(patience=20,mode='auto',min_delta=0,restore_best_weights=True,verbose=2)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(factor=0.9,patience=8,verbose=2)\n",
    "\n",
    "#####################################\n",
    "### Parametros de entrenamiento #####\n",
    "epochs = 4\n",
    "shuffle =True\n",
    "callbacks = [chk,stp,lrs] ## No se recomienda usar callbacks en un GridSearch\n",
    "batch_size = 128\n",
    "\n",
    "# wrapping... de keras a scikit model... esto convierte a model a tipo Scikite (posee .fit() , .predict() ,  .evaluate())\n",
    "model= tf.keras.wrappers.scikit_learn.KerasRegressor(crear_modelo_deep,epochs=epochs,shuffle=True,validation_split=0.2,verbose=1)\n",
    "\n",
    "# Definir Pipeline: Scaler + Keras Model\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('reg', model)])\n",
    "\n",
    "################################################################\n",
    "#### Grilla de busqueda de mejores parametros para la ANN ######\n",
    "\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "inits = ['glorot_uniform']\n",
    "dropouts =[0.0]\n",
    "epochss = [5]\n",
    "hs=[(80,40,20,1),(80,30,10,1),(100,20,5,1)]\n",
    "\n",
    "# grilla de parametros\n",
    "param_grid = {'reg__optimizer':optimizers,\n",
    "              'reg__epochs':epochss,\n",
    "              'reg__init':inits,\n",
    "              'reg__dropout':dropouts,\n",
    "             'reg__h': hs}\n",
    "\n",
    "# Gridsearch CV\n",
    "search = GridSearchCV(pipe, param_grid)\n",
    "\n",
    "# entrenar modelos y busqueda mediante GridSearchCV\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# ver ajuste del modelo entrenado\n",
    "valor = search.score(X_test,y_test)\n",
    "\n",
    "# Mejores parametros\n",
    "print('\\n\\n----------------------------')\n",
    "print('Mejores parametros: ')\n",
    "print(search.best_params_)\n",
    "print('Mejor Puntaje:' ,np.round(valor,3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T03:47:57.172745Z",
     "start_time": "2021-02-18T03:47:56.851370Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 51us/sample - loss: 0.4266 - mse: 0.3513\n",
      "\n",
      "Puntaje modelo: -0.427\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "10/10 [==============================] - 0s 398us/sample\n",
      "[0.723 0.682 1.758 0.906 1.411 2.075 2.021 1.627 2.347 4.68 ]\n",
      "\n",
      "Valores Orginales: \n",
      "[0.526 0.574 1.547 0.421 1.136 1.587 1.75  1.276 3.081 4.385]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediccion\n",
    "\n",
    "valor = search.score(X_test,y_test)\n",
    "print('\\nPuntaje modelo:' ,np.round(valor,3))\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(search.predict(X_test[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regresor GridSearchCV + Pipeline + Keras  - Con Profundidad Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T03:57:22.462879Z",
     "start_time": "2021-02-18T03:51:28.764079Z"
    },
    "code_folding": [
     0,
     3
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dim X train:  (15480, 8)\n",
      "Dim Y train:  (15480,)\n",
      "\n",
      "\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 121us/sample - loss: 0.8887 - mse: 0.8269 - val_loss: 0.5406 - val_mse: 0.4803\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.5591 - mse: 0.5007 - val_loss: 0.5225 - val_mse: 0.4660\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5116 - mse: 0.4551\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 117us/sample - loss: 0.9330 - mse: 0.8725 - val_loss: 0.5560 - val_mse: 0.4977\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 84us/sample - loss: 0.5556 - mse: 0.4992 - val_loss: 0.5393 - val_mse: 0.4844\n",
      "3096/3096 [==============================] - 0s 40us/sample - loss: 0.5373 - mse: 0.4825\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 121us/sample - loss: 0.8907 - mse: 0.8298 - val_loss: 0.6304 - val_mse: 0.5712\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5310 - mse: 0.4734 - val_loss: 0.5360 - val_mse: 0.4802\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5990 - mse: 0.5433\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 122us/sample - loss: 0.8700 - mse: 0.8099 - val_loss: 0.5848 - val_mse: 0.5263\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5583 - mse: 0.5018 - val_loss: 0.5269 - val_mse: 0.4724\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5030 - mse: 0.4485\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 119us/sample - loss: 0.9104 - mse: 0.8494 - val_loss: 0.5155 - val_mse: 0.4563\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5458 - mse: 0.4886 - val_loss: 0.5098 - val_mse: 0.4543\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.5517 - mse: 0.4962\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 111us/sample - loss: 1.0195 - mse: 0.9589 - val_loss: 0.5912 - val_mse: 0.5324\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 79us/sample - loss: 0.5467 - mse: 0.4899 - val_loss: 0.5371 - val_mse: 0.4821\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5121 - mse: 0.4571\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 105us/sample - loss: 1.0652 - mse: 1.0033 - val_loss: 0.5557 - val_mse: 0.4954\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 77us/sample - loss: 0.5487 - mse: 0.4902 - val_loss: 0.5322 - val_mse: 0.4755\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5259 - mse: 0.4692\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 111us/sample - loss: 1.0058 - mse: 0.9440 - val_loss: 0.5445 - val_mse: 0.4839\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 81us/sample - loss: 0.5264 - mse: 0.4675 - val_loss: 0.5261 - val_mse: 0.4685\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5947 - mse: 0.5371\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 149us/sample - loss: 1.0595 - mse: 0.9971 - val_loss: 0.5625 - val_mse: 0.5007\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 81us/sample - loss: 0.5569 - mse: 0.4962 - val_loss: 0.5385 - val_mse: 0.4791\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5110 - mse: 0.4516\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 108us/sample - loss: 1.0503 - mse: 0.9890 - val_loss: 0.5239 - val_mse: 0.4640\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 80us/sample - loss: 0.5477 - mse: 0.4895 - val_loss: 0.4870 - val_mse: 0.4303\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5247 - mse: 0.4680\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 137us/sample - loss: 1.0145 - mse: 0.9550 - val_loss: 0.5837 - val_mse: 0.5253\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 98us/sample - loss: 0.5498 - mse: 0.4923 - val_loss: 0.5105 - val_mse: 0.4540\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.5050 - mse: 0.4485\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 139us/sample - loss: 0.8499 - mse: 0.7934 - val_loss: 0.5720 - val_mse: 0.5167\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.5434 - mse: 0.4893 - val_loss: 0.5276 - val_mse: 0.4745\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5358 - mse: 0.4828\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 0.9435 - mse: 0.8867 - val_loss: 0.5381 - val_mse: 0.4820\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5198 - mse: 0.4645 - val_loss: 0.5461 - val_mse: 0.4915\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.6105 - mse: 0.5559\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 133us/sample - loss: 0.9770 - mse: 0.9207 - val_loss: 0.5460 - val_mse: 0.4909\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 98us/sample - loss: 0.5494 - mse: 0.4952 - val_loss: 0.5315 - val_mse: 0.4783\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5021 - mse: 0.4489\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 138us/sample - loss: 0.9083 - mse: 0.8526 - val_loss: 0.4891 - val_mse: 0.4345\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 95us/sample - loss: 0.5241 - mse: 0.4707 - val_loss: 0.4720 - val_mse: 0.4198\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5048 - mse: 0.4525\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 117us/sample - loss: 1.2241 - mse: 1.1669 - val_loss: 0.5437 - val_mse: 0.4872\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.5441 - mse: 0.4887 - val_loss: 0.5281 - val_mse: 0.4739\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5077 - mse: 0.4534\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 112us/sample - loss: 1.0669 - mse: 1.0099 - val_loss: 0.5399 - val_mse: 0.4834\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 0.5490 - mse: 0.493 - 1s 82us/sample - loss: 0.5451 - mse: 0.4897 - val_loss: 0.5286 - val_mse: 0.4741\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5214 - mse: 0.4669\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 117us/sample - loss: 1.1114 - mse: 1.0552 - val_loss: 0.5427 - val_mse: 0.4875\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 85us/sample - loss: 0.5199 - mse: 0.4656 - val_loss: 0.5193 - val_mse: 0.4658\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5861 - mse: 0.5326\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 116us/sample - loss: 1.1512 - mse: 1.0934 - val_loss: 0.5384 - val_mse: 0.4814\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 81us/sample - loss: 0.5553 - mse: 0.4994 - val_loss: 0.5222 - val_mse: 0.4673\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.4984 - mse: 0.4435\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 120us/sample - loss: 1.0280 - mse: 0.9717 - val_loss: 0.5168 - val_mse: 0.4610\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 85us/sample - loss: 0.5295 - mse: 0.4746 - val_loss: 0.5022 - val_mse: 0.4481\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5301 - mse: 0.4760\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 129us/sample - loss: 1.1004 - mse: 1.0414 - val_loss: 0.5404 - val_mse: 0.4839\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.6564 - mse: 0.6022 - val_loss: 0.5413 - val_mse: 0.4888\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5425 - mse: 0.4900\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 133us/sample - loss: 1.1273 - mse: 1.0687 - val_loss: 0.5411 - val_mse: 0.4855\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.6469 - mse: 0.5938 - val_loss: 0.5662 - val_mse: 0.5151\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5847 - mse: 0.5336\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 122us/sample - loss: 1.1065 - mse: 1.0466 - val_loss: 0.5658 - val_mse: 0.5086\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.6371 - mse: 0.5825 - val_loss: 0.5462 - val_mse: 0.4940\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.6089 - mse: 0.5567\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 130us/sample - loss: 1.0900 - mse: 1.0298 - val_loss: 0.5561 - val_mse: 0.4988\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.6661 - mse: 0.6112 - val_loss: 0.5370 - val_mse: 0.4844\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5129 - mse: 0.4603\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 129us/sample - loss: 1.1181 - mse: 1.0589 - val_loss: 0.5370 - val_mse: 0.4809\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.6637 - mse: 0.6107 - val_loss: 0.4930 - val_mse: 0.4425\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5255 - mse: 0.4750\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 116us/sample - loss: 1.2527 - mse: 1.1923 - val_loss: 0.5431 - val_mse: 0.4851\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 82us/sample - loss: 0.6751 - mse: 0.6196 - val_loss: 0.5497 - val_mse: 0.4967\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5326 - mse: 0.4797\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 151us/sample - loss: 1.2341 - mse: 1.1729 - val_loss: 0.5487 - val_mse: 0.4903\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 79us/sample - loss: 0.6739 - mse: 0.6182 - val_loss: 0.5360 - val_mse: 0.4827\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.5298 - mse: 0.4765\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 118us/sample - loss: 1.2159 - mse: 1.1565 - val_loss: 0.5426 - val_mse: 0.4853\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 84us/sample - loss: 0.6566 - mse: 0.6016 - val_loss: 0.5312 - val_mse: 0.4785\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5877 - mse: 0.5350\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 118us/sample - loss: 1.2867 - mse: 1.2253 - val_loss: 0.5531 - val_mse: 0.4942\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 83us/sample - loss: 0.6687 - mse: 0.6119 - val_loss: 0.5375 - val_mse: 0.4828\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.5122 - mse: 0.4576\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 118us/sample - loss: 1.2144 - mse: 1.1525 - val_loss: 0.5059 - val_mse: 0.4467\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 83us/sample - loss: 0.6782 - mse: 0.6216 - val_loss: 0.4940 - val_mse: 0.4396\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5329 - mse: 0.4786\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 139us/sample - loss: 1.4748 - mse: 1.4194 - val_loss: 0.5486 - val_mse: 0.4959\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.7994 - mse: 0.7483 - val_loss: 0.5351 - val_mse: 0.4857\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5120 - mse: 0.4626\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 140us/sample - loss: 1.5369 - mse: 1.4823 - val_loss: 0.5748 - val_mse: 0.5228\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 98us/sample - loss: 0.7865 - mse: 0.7364 - val_loss: 0.5347 - val_mse: 0.4863\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5318 - mse: 0.4833\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 144us/sample - loss: 1.4421 - mse: 1.3879 - val_loss: 0.5763 - val_mse: 0.5250\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 94us/sample - loss: 0.7569 - mse: 0.7077 - val_loss: 0.5247 - val_mse: 0.4773\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5689 - mse: 0.5215\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 142us/sample - loss: 1.4655 - mse: 1.4108 - val_loss: 0.5522 - val_mse: 0.4998\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 99us/sample - loss: 0.7754 - mse: 0.7249 - val_loss: 0.5214 - val_mse: 0.4724\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.4965 - mse: 0.4475\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 145us/sample - loss: 1.4881 - mse: 1.4350 - val_loss: 0.5481 - val_mse: 0.4980\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 95us/sample - loss: 0.7786 - mse: 0.7303 - val_loss: 0.4912 - val_mse: 0.4447\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5300 - mse: 0.4835\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 130us/sample - loss: 1.7147 - mse: 1.6593 - val_loss: 0.5725 - val_mse: 0.5193\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.8775 - mse: 0.8261 - val_loss: 0.5380 - val_mse: 0.4882\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5115 - mse: 0.4617\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 125us/sample - loss: 1.6537 - mse: 1.6002 - val_loss: 0.5539 - val_mse: 0.5025\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.8723 - mse: 0.8229 - val_loss: 0.5378 - val_mse: 0.4898\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5273 - mse: 0.4793\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 129us/sample - loss: 1.6228 - mse: 1.5674 - val_loss: 0.5619 - val_mse: 0.5082\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.8178 - mse: 0.7657 - val_loss: 0.5679 - val_mse: 0.5176\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.6147 - mse: 0.5644\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 130us/sample - loss: 1.7071 - mse: 1.6528 - val_loss: 0.5673 - val_mse: 0.5149\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.8786 - mse: 0.8283 - val_loss: 0.5629 - val_mse: 0.5143\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5323 - mse: 0.4837\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 126us/sample - loss: 1.5366 - mse: 1.4819 - val_loss: 0.5141 - val_mse: 0.4621\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 84us/sample - loss: 0.8695 - mse: 0.8193 - val_loss: 0.4988 - val_mse: 0.4501\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5372 - mse: 0.4884\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 125us/sample - loss: 0.9322 - mse: 0.8708 - val_loss: 0.5518 - val_mse: 0.4921\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.5538 - mse: 0.4959 - val_loss: 0.5244 - val_mse: 0.4683\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.5165 - mse: 0.4604\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 119us/sample - loss: 0.9082 - mse: 0.8462 - val_loss: 0.5568 - val_mse: 0.4966\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.5582 - mse: 0.5000 - val_loss: 0.5222 - val_mse: 0.4662\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5246 - mse: 0.4685\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 116us/sample - loss: 0.8967 - mse: 0.8352 - val_loss: 0.5540 - val_mse: 0.4943\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5289 - mse: 0.4710 - val_loss: 0.5274 - val_mse: 0.4714s - loss: 0.5293 - mse: 0.471\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5998 - mse: 0.5438\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 123us/sample - loss: 0.9143 - mse: 0.8523 - val_loss: 0.5512 - val_mse: 0.4908\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5556 - mse: 0.4971 - val_loss: 0.5226 - val_mse: 0.4658\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.4962 - mse: 0.4394\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 164us/sample - loss: 0.8661 - mse: 0.8032 - val_loss: 0.5068 - val_mse: 0.4454\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5447 - mse: 0.4848 - val_loss: 0.4969 - val_mse: 0.4384\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5310 - mse: 0.4724\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 108us/sample - loss: 1.0592 - mse: 0.9965 - val_loss: 0.5556 - val_mse: 0.4942\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 78us/sample - loss: 0.5496 - mse: 0.4900 - val_loss: 0.5550 - val_mse: 0.4970\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5408 - mse: 0.4828\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 110us/sample - loss: 1.0524 - mse: 0.9919 - val_loss: 0.5388 - val_mse: 0.4798\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 82us/sample - loss: 0.5459 - mse: 0.4888 - val_loss: 0.5360 - val_mse: 0.4807\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.5279 - mse: 0.4725\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 108us/sample - loss: 1.0393 - mse: 0.9766 - val_loss: 0.5516 - val_mse: 0.4902\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 78us/sample - loss: 0.5276 - mse: 0.4679 - val_loss: 0.5325 - val_mse: 0.4745\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5909 - mse: 0.5329\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 111us/sample - loss: 1.0653 - mse: 1.0036 - val_loss: 0.5928 - val_mse: 0.5323\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 80us/sample - loss: 0.5545 - mse: 0.4952 - val_loss: 0.5482 - val_mse: 0.4902\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5177 - mse: 0.4597\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 106us/sample - loss: 1.0454 - mse: 0.9841 - val_loss: 0.4999 - val_mse: 0.4405\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 85us/sample - loss: 0.5420 - mse: 0.4846 - val_loss: 0.4931 - val_mse: 0.4374\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5289 - mse: 0.4732\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 137us/sample - loss: 0.9746 - mse: 0.9193 - val_loss: 0.5317 - val_mse: 0.4772\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.5315 - mse: 0.4776 - val_loss: 0.5056 - val_mse: 0.4521\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5014 - mse: 0.4479\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 135us/sample - loss: 1.0183 - mse: 0.9621 - val_loss: 0.5451 - val_mse: 0.4892\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 97us/sample - loss: 0.5351 - mse: 0.4794 - val_loss: 0.5049 - val_mse: 0.4492\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.4970 - mse: 0.4413\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 142us/sample - loss: 1.0941 - mse: 1.0374 - val_loss: 0.5873 - val_mse: 0.5311\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 97us/sample - loss: 0.5425 - mse: 0.4868 - val_loss: 0.5203 - val_mse: 0.4651\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.5886 - mse: 0.5334\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 142us/sample - loss: 0.9397 - mse: 0.8814 - val_loss: 0.5788 - val_mse: 0.5212\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.5618 - mse: 0.5046 - val_loss: 0.5710 - val_mse: 0.5143\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.5392 - mse: 0.4825\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 139us/sample - loss: 1.0106 - mse: 0.9544 - val_loss: 0.5311 - val_mse: 0.4754\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 99us/sample - loss: 0.5343 - mse: 0.4790 - val_loss: 0.4608 - val_mse: 0.4055\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5048 - mse: 0.4496\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 114us/sample - loss: 1.1754 - mse: 1.1168 - val_loss: 0.5940 - val_mse: 0.5360\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 82us/sample - loss: 0.5756 - mse: 0.5182 - val_loss: 0.5555 - val_mse: 0.4989\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5292 - mse: 0.4726\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 116us/sample - loss: 1.1680 - mse: 1.1113 - val_loss: 0.5661 - val_mse: 0.5090\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.5513 - mse: 0.4942 - val_loss: 0.5400 - val_mse: 0.4831\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5223 - mse: 0.4653\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 122us/sample - loss: 1.1684 - mse: 1.1124 - val_loss: 0.5801 - val_mse: 0.5246\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.5325 - mse: 0.4772 - val_loss: 0.5201 - val_mse: 0.4651\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5669 - mse: 0.5119\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 115us/sample - loss: 1.0900 - mse: 1.0328 - val_loss: 0.5728 - val_mse: 0.5156\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 83us/sample - loss: 0.5533 - mse: 0.4966 - val_loss: 0.5329 - val_mse: 0.4767\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5083 - mse: 0.4520\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 123us/sample - loss: 1.0569 - mse: 1.0020 - val_loss: 0.5241 - val_mse: 0.4697\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 85us/sample - loss: 0.5484 - mse: 0.4946 - val_loss: 0.5105 - val_mse: 0.4573\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5567 - mse: 0.5035\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 125us/sample - loss: 1.0682 - mse: 1.0095 - val_loss: 0.5361 - val_mse: 0.4805\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.6703 - mse: 0.6173 - val_loss: 0.5261 - val_mse: 0.4754\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.5279 - mse: 0.4771\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 124us/sample - loss: 1.1137 - mse: 1.0529 - val_loss: 0.5539 - val_mse: 0.4959\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.6483 - mse: 0.5932 - val_loss: 0.5422 - val_mse: 0.4897\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5460 - mse: 0.4936\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 124us/sample - loss: 1.0995 - mse: 1.0391 - val_loss: 0.5648 - val_mse: 0.5075\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.6350 - mse: 0.5804 - val_loss: 0.5317 - val_mse: 0.4798\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5955 - mse: 0.54350s - loss: 0.5948 - mse: 0.54\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 123us/sample - loss: 1.0996 - mse: 1.0397 - val_loss: 0.5487 - val_mse: 0.4919\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.6634 - mse: 0.6094 - val_loss: 0.5360 - val_mse: 0.4843\n",
      "3096/3096 [==============================] - 0s 50us/sample - loss: 0.5177 - mse: 0.4661\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 128us/sample - loss: 1.0844 - mse: 1.0257 - val_loss: 0.5108 - val_mse: 0.4551\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.6515 - mse: 0.5986 - val_loss: 0.4942 - val_mse: 0.4435\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5287 - mse: 0.4781\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 115us/sample - loss: 1.3404 - mse: 1.2802 - val_loss: 0.5564 - val_mse: 0.4987\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 80us/sample - loss: 0.6705 - mse: 0.6155 - val_loss: 0.5378 - val_mse: 0.4853\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5165 - mse: 0.4640\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 117us/sample - loss: 1.2585 - mse: 1.1982 - val_loss: 0.5562 - val_mse: 0.4987\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 84us/sample - loss: 0.6652 - mse: 0.6100 - val_loss: 0.5506 - val_mse: 0.4975\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5531 - mse: 0.4999\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 116us/sample - loss: 1.2734 - mse: 1.2126 - val_loss: 0.5683 - val_mse: 0.5099\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 84us/sample - loss: 0.6550 - mse: 0.5989 - val_loss: 0.5534 - val_mse: 0.4997\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.6078 - mse: 0.5540\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 115us/sample - loss: 1.2686 - mse: 1.2097 - val_loss: 0.5537 - val_mse: 0.4973\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 85us/sample - loss: 0.6723 - mse: 0.6182 - val_loss: 0.5416 - val_mse: 0.4895\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5151 - mse: 0.4630\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 1.2903 - mse: 1.230 - 1s 114us/sample - loss: 1.2731 - mse: 1.2136 - val_loss: 0.5119 - val_mse: 0.4546\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 79us/sample - loss: 0.6664 - mse: 0.6115 - val_loss: 0.5166 - val_mse: 0.4638\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5430 - mse: 0.4901\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 143us/sample - loss: 1.4551 - mse: 1.4008 - val_loss: 0.6177 - val_mse: 0.5663\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 99us/sample - loss: 0.8426 - mse: 0.7936 - val_loss: 0.5626 - val_mse: 0.5157\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5371 - mse: 0.4902\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 144us/sample - loss: 1.4022 - mse: 1.3499 - val_loss: 0.5846 - val_mse: 0.5350\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.7976 - mse: 0.7503 - val_loss: 0.5561 - val_mse: 0.5107\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5558 - mse: 0.5103\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 141us/sample - loss: 1.4804 - mse: 1.4279 - val_loss: 0.6353 - val_mse: 0.5857\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 97us/sample - loss: 0.7998 - mse: 0.7528 - val_loss: 0.5690 - val_mse: 0.5243\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.6270 - mse: 0.5823\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 144us/sample - loss: 1.5349 - mse: 1.4802 - val_loss: 0.6119 - val_mse: 0.5598\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.8589 - mse: 0.8095 - val_loss: 0.5704 - val_mse: 0.5233\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5415 - mse: 0.4945\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 142us/sample - loss: 1.4725 - mse: 1.4180 - val_loss: 0.5660 - val_mse: 0.5146\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 99us/sample - loss: 0.8134 - mse: 0.7644 - val_loss: 0.5282 - val_mse: 0.4814\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5655 - mse: 0.5187\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 122us/sample - loss: 1.6530 - mse: 1.5994 - val_loss: 0.6450 - val_mse: 0.5933\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.8746 - mse: 0.8250 - val_loss: 0.5960 - val_mse: 0.5482\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.5664 - mse: 0.5186\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 124us/sample - loss: 1.7656 - mse: 1.7090 - val_loss: 0.6226 - val_mse: 0.5680\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.8854 - mse: 0.8329 - val_loss: 0.5802 - val_mse: 0.5298\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5709 - mse: 0.5204\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 132us/sample - loss: 1.6891 - mse: 1.6317 - val_loss: 0.6956 - val_mse: 0.6399\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.8825 - mse: 0.8291 - val_loss: 0.6141 - val_mse: 0.5630\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.6632 - mse: 0.6121\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 127us/sample - loss: 1.6100 - mse: 1.5557 - val_loss: 0.6033 - val_mse: 0.5508\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.9050 - mse: 0.8545 - val_loss: 0.5857 - val_mse: 0.5370\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5616 - mse: 0.5129\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 130us/sample - loss: 1.6955 - mse: 1.6391 - val_loss: 0.5920 - val_mse: 0.5375\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.9087 - mse: 0.8563 - val_loss: 0.5548 - val_mse: 0.5043\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5935 - mse: 0.5431\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 114us/sample - loss: 0.8860 - mse: 0.8235 - val_loss: 0.5313 - val_mse: 0.4703\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.5549 - mse: 0.4957 - val_loss: 0.5290 - val_mse: 0.4718\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5225 - mse: 0.4653\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 119us/sample - loss: 0.9452 - mse: 0.8831 - val_loss: 0.5475 - val_mse: 0.4876\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 80us/sample - loss: 0.5490 - mse: 0.4916 - val_loss: 0.5133 - val_mse: 0.4576\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.5329 - mse: 0.4772\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 119us/sample - loss: 0.9188 - mse: 0.8574 - val_loss: 0.5695 - val_mse: 0.5100\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 84us/sample - loss: 0.5475 - mse: 0.4901 - val_loss: 0.5183 - val_mse: 0.4626\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.6011 - mse: 0.5454\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 121us/sample - loss: 0.9285 - mse: 0.8669 - val_loss: 0.5412 - val_mse: 0.4809\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.5569 - mse: 0.4983 - val_loss: 0.5461 - val_mse: 0.4894\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5133 - mse: 0.4567\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 117us/sample - loss: 0.8658 - mse: 0.8053 - val_loss: 0.5025 - val_mse: 0.4439\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 83us/sample - loss: 0.5433 - mse: 0.4863 - val_loss: 0.4842 - val_mse: 0.4286\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5254 - mse: 0.4698\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 108us/sample - loss: 1.0244 - mse: 0.9623 - val_loss: 0.5493 - val_mse: 0.4887 - loss: 1.3948 - mse: \n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 82us/sample - loss: 0.5567 - mse: 0.4979 - val_loss: 0.5297 - val_mse: 0.4726\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5117 - mse: 0.4546\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 108us/sample - loss: 0.9886 - mse: 0.9264 - val_loss: 0.5550 - val_mse: 0.4940\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 82us/sample - loss: 0.5477 - mse: 0.4881 - val_loss: 0.5336 - val_mse: 0.4752\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5337 - mse: 0.4754\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 110us/sample - loss: 0.9953 - mse: 0.9346 - val_loss: 0.5363 - val_mse: 0.4770\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 83us/sample - loss: 0.5256 - mse: 0.4681 - val_loss: 0.5293 - val_mse: 0.4736\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5927 - mse: 0.5370\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 107us/sample - loss: 1.0342 - mse: 0.9708 - val_loss: 0.5650 - val_mse: 0.5031\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 79us/sample - loss: 0.5523 - mse: 0.4920 - val_loss: 0.5505 - val_mse: 0.4921\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5310 - mse: 0.4726\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 107us/sample - loss: 1.0988 - mse: 1.0381 - val_loss: 0.5073 - val_mse: 0.4482\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 77us/sample - loss: 0.5433 - mse: 0.4858 - val_loss: 0.5178 - val_mse: 0.4619\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5466 - mse: 0.4907\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 134us/sample - loss: 1.0407 - mse: 0.9856 - val_loss: 0.5552 - val_mse: 0.5004\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 97us/sample - loss: 0.5171 - mse: 0.4620 - val_loss: 0.4880 - val_mse: 0.4327\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4700 - mse: 0.4147\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 132us/sample - loss: 1.0660 - mse: 1.0090 - val_loss: 0.5454 - val_mse: 0.4886\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5043 - mse: 0.4474 - val_loss: 0.4850 - val_mse: 0.4282\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4769 - mse: 0.4200\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 135us/sample - loss: 1.0892 - mse: 1.0328 - val_loss: 0.5551 - val_mse: 0.4994\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 94us/sample - loss: 0.5076 - mse: 0.4520 - val_loss: 0.4814 - val_mse: 0.4260\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5218 - mse: 0.4664\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 136us/sample - loss: 0.9897 - mse: 0.9330 - val_loss: 0.5522 - val_mse: 0.4959\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.5193 - mse: 0.4630 - val_loss: 0.4852 - val_mse: 0.4286\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4626 - mse: 0.4061\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 137us/sample - loss: 1.1583 - mse: 1.1003 - val_loss: 0.5700 - val_mse: 0.5126\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.5392 - mse: 0.4821 - val_loss: 0.4643 - val_mse: 0.4072\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5005 - mse: 0.4434\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 115us/sample - loss: 1.1204 - mse: 1.0644 - val_loss: 0.5576 - val_mse: 0.5017\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5532 - mse: 0.4979 - val_loss: 0.5256 - val_mse: 0.4705\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5151 - mse: 0.4600\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 119us/sample - loss: 1.1500 - mse: 1.0948 - val_loss: 0.6213 - val_mse: 0.5667\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.5759 - mse: 0.5220 - val_loss: 0.5674 - val_mse: 0.5139\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5620 - mse: 0.5086\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 113us/sample - loss: 1.1688 - mse: 1.1120 - val_loss: 0.5936 - val_mse: 0.5374\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 85us/sample - loss: 0.5575 - mse: 0.5020 - val_loss: 0.5573 - val_mse: 0.5026\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.6113 - mse: 0.5567\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 121us/sample - loss: 1.1893 - mse: 1.1311 - val_loss: 0.6052 - val_mse: 0.5474\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 84us/sample - loss: 0.5676 - mse: 0.5102 - val_loss: 0.5385 - val_mse: 0.4815\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5055 - mse: 0.4486\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 122us/sample - loss: 1.1087 - mse: 1.0503 - val_loss: 0.5087 - val_mse: 0.4506\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 0.5429 - mse: 0.484 - 1s 87us/sample - loss: 0.5406 - mse: 0.4827 - val_loss: 0.4756 - val_mse: 0.4178\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5212 - mse: 0.4633\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 166us/sample - loss: 0.8995 - mse: 0.8075 - val_loss: 0.7768 - val_mse: 0.6854\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 109us/sample - loss: 0.5447 - mse: 0.4536 - val_loss: 0.5041 - val_mse: 0.4135\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.4867 - mse: 0.3961\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 161us/sample - loss: 1.1711 - mse: 1.0785 - val_loss: 0.6332 - val_mse: 0.5412\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 112us/sample - loss: 0.5532 - mse: 0.4620 - val_loss: 0.5065 - val_mse: 0.4163\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.4939 - mse: 0.4037\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 163us/sample - loss: 1.0501 - mse: 0.9569 - val_loss: 0.6678 - val_mse: 0.5756\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 111us/sample - loss: 0.5672 - mse: 0.4757 - val_loss: 0.5289 - val_mse: 0.4383\n",
      "3096/3096 [==============================] - 0s 52us/sample - loss: 0.5557 - mse: 0.4651\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 164us/sample - loss: 1.5335 - mse: 1.4395 - val_loss: 0.8021 - val_mse: 0.7096\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 107us/sample - loss: 0.6395 - mse: 0.5485 - val_loss: 0.5627 - val_mse: 0.4731\n",
      "3096/3096 [==============================] - 0s 50us/sample - loss: 0.5273 - mse: 0.4377\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 160us/sample - loss: 1.0145 - mse: 0.9221 - val_loss: 0.5684 - val_mse: 0.4763\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 109us/sample - loss: 0.5454 - mse: 0.4542 - val_loss: 0.4840 - val_mse: 0.3938\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5172 - mse: 0.4270\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 140us/sample - loss: 1.4657 - mse: 1.3709 - val_loss: 0.7947 - val_mse: 0.7004\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 97us/sample - loss: 0.6662 - mse: 0.5727 - val_loss: 0.5794 - val_mse: 0.4869\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.5459 - mse: 0.4534\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 136us/sample - loss: 1.0646 - mse: 0.9765 - val_loss: 0.6698 - val_mse: 0.5820 - loss: 1.1105 - mse: 1.02\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 94us/sample - loss: 0.5750 - mse: 0.4871 - val_loss: 0.5366 - val_mse: 0.4480\n",
      "3096/3096 [==============================] - 0s 50us/sample - loss: 0.5098 - mse: 0.4213\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 133us/sample - loss: 1.1880 - mse: 1.0964 - val_loss: 0.6737 - val_mse: 0.5823\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 99us/sample - loss: 0.5937 - mse: 0.5028 - val_loss: 0.5687 - val_mse: 0.4787\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.6089 - mse: 0.5189\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 135us/sample - loss: 1.2316 - mse: 1.1376 - val_loss: 0.7132 - val_mse: 0.6201\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.6010 - mse: 0.5078 - val_loss: 0.5411 - val_mse: 0.4480\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.5134 - mse: 0.4202\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 140us/sample - loss: 1.5123 - mse: 1.4195 - val_loss: 0.7555 - val_mse: 0.6630\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 0.6731 - mse: 0.581 - 1s 100us/sample - loss: 0.6690 - mse: 0.5769 - val_loss: 0.5508 - val_mse: 0.4599\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.6044 - mse: 0.5135\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 1.1387 - mse: 1.0786 - val_loss: 0.5678 - val_mse: 0.5108\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.6656 - mse: 0.6111 - val_loss: 0.5337 - val_mse: 0.4817\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5240 - mse: 0.4720\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 127us/sample - loss: 1.1111 - mse: 1.0508 - val_loss: 0.5606 - val_mse: 0.5039\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.6500 - mse: 0.5960 - val_loss: 0.6027 - val_mse: 0.5512\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.6093 - mse: 0.5578\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 124us/sample - loss: 1.0869 - mse: 1.0284 - val_loss: 0.5469 - val_mse: 0.4915\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 85us/sample - loss: 0.6408 - mse: 0.5877 - val_loss: 0.5383 - val_mse: 0.4876- loss: 0.6416 - mse: 0.588\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.5939 - mse: 0.5431\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 1.0929 - mse: 1.0340 - val_loss: 0.5466 - val_mse: 0.4904\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.6645 - mse: 0.6105 - val_loss: 0.5840 - val_mse: 0.5318\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5682 - mse: 0.5160\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 128us/sample - loss: 1.0521 - mse: 0.9941 - val_loss: 0.5195 - val_mse: 0.4649\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.6409 - mse: 0.5892 - val_loss: 0.4967 - val_mse: 0.4474\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5285 - mse: 0.4792\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 111us/sample - loss: 1.2952 - mse: 1.2341 - val_loss: 0.5482 - val_mse: 0.4891\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 79us/sample - loss: 0.6705 - mse: 0.6139 - val_loss: 0.5338 - val_mse: 0.4795\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5112 - mse: 0.4569\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 113us/sample - loss: 1.2280 - mse: 1.1666 - val_loss: 0.5613 - val_mse: 0.5030\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 79us/sample - loss: 0.6765 - mse: 0.6211 - val_loss: 0.5457 - val_mse: 0.4930\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.5394 - mse: 0.4867\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 115us/sample - loss: 1.2051 - mse: 1.1438 - val_loss: 0.5451 - val_mse: 0.4862\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 84us/sample - loss: 0.6531 - mse: 0.5965 - val_loss: 0.5444 - val_mse: 0.4899\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5944 - mse: 0.5399\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 112us/sample - loss: 1.2342 - mse: 1.1759 - val_loss: 0.5507 - val_mse: 0.4946\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 83us/sample - loss: 0.6706 - mse: 0.6165 - val_loss: 0.5530 - val_mse: 0.5010\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5172 - mse: 0.4651\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 112us/sample - loss: 1.2516 - mse: 1.1884 - val_loss: 0.5112 - val_mse: 0.4503\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 81us/sample - loss: 0.6672 - mse: 0.6086 - val_loss: 0.5122 - val_mse: 0.4559\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5505 - mse: 0.4942\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 146us/sample - loss: 1.5428 - mse: 1.4876 - val_loss: 0.6008 - val_mse: 0.5487\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.8312 - mse: 0.7817 - val_loss: 0.5848 - val_mse: 0.5377\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5613 - mse: 0.5142\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 146us/sample - loss: 1.5862 - mse: 1.5310 - val_loss: 0.5994 - val_mse: 0.5467\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.8221 - mse: 0.7718 - val_loss: 0.5679 - val_mse: 0.5198\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5636 - mse: 0.5155\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 146us/sample - loss: 1.4670 - mse: 1.4147 - val_loss: 0.5939 - val_mse: 0.5445\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 105us/sample - loss: 0.7810 - mse: 0.7339 - val_loss: 0.5920 - val_mse: 0.5468\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.6422 - mse: 0.5970\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 151us/sample - loss: 1.5403 - mse: 1.4854 - val_loss: 0.6150 - val_mse: 0.5628\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 99us/sample - loss: 0.8332 - mse: 0.7833 - val_loss: 0.5770 - val_mse: 0.5295\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5510 - mse: 0.5035\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 145us/sample - loss: 1.5925 - mse: 1.5376 - val_loss: 0.6007 - val_mse: 0.5483\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 98us/sample - loss: 0.8307 - mse: 0.7808 - val_loss: 0.5464 - val_mse: 0.4987\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.5941 - mse: 0.5464\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 1.8634 - mse: 1.807 - 1s 124us/sample - loss: 1.8493 - mse: 1.7930 - val_loss: 0.6765 - val_mse: 0.6221\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.9142 - mse: 0.8624 - val_loss: 0.6140 - val_mse: 0.5646\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.5813 - mse: 0.5319\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 123us/sample - loss: 1.6687 - mse: 1.6155 - val_loss: 0.6089 - val_mse: 0.5577\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.8972 - mse: 0.8482 - val_loss: 0.5773 - val_mse: 0.5303\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5676 - mse: 0.5207\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 123us/sample - loss: 1.6610 - mse: 1.6046 - val_loss: 0.6620 - val_mse: 0.6074\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.8958 - mse: 0.8432 - val_loss: 0.6050 - val_mse: 0.5546\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.6611 - mse: 0.6107\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 123us/sample - loss: 1.7317 - mse: 1.6781 - val_loss: 0.6576 - val_mse: 0.6062\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.9138 - mse: 0.8648 - val_loss: 0.6156 - val_mse: 0.5689\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.5870 - mse: 0.5403\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 128us/sample - loss: 1.7119 - mse: 1.6555 - val_loss: 0.6294 - val_mse: 0.5749\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.9442 - mse: 0.8920 - val_loss: 0.5605 - val_mse: 0.5106\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.6062 - mse: 0.5562\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 181us/sample - loss: 1.7022 - mse: 1.6143 - val_loss: 0.7547 - val_mse: 0.6708\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 120us/sample - loss: 1.0987 - mse: 1.0185 - val_loss: 0.6491 - val_mse: 0.5725\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.6072 - mse: 0.5306\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 182us/sample - loss: 1.9398 - mse: 1.8512 - val_loss: 0.8053 - val_mse: 0.7213\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 119us/sample - loss: 1.1021 - mse: 1.0222 - val_loss: 0.6846 - val_mse: 0.6086\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.6670 - mse: 0.5911\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 179us/sample - loss: 1.8226 - mse: 1.7370 - val_loss: 0.8510 - val_mse: 0.7697\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 116us/sample - loss: 1.0722 - mse: 0.9949 - val_loss: 0.6740 - val_mse: 0.6005\n",
      "3096/3096 [==============================] - 0s 49us/sample - loss: 0.7173 - mse: 0.6438\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 197us/sample - loss: 1.7555 - mse: 1.6669 - val_loss: 0.7568 - val_mse: 0.6727\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 117us/sample - loss: 1.1253 - mse: 1.0456 - val_loss: 0.6707 - val_mse: 0.5952\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.6393 - mse: 0.5638\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 180us/sample - loss: 1.9221 - mse: 1.8332 - val_loss: 0.7648 - val_mse: 0.6793\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 113us/sample - loss: 1.0989 - mse: 1.0173 - val_loss: 0.6151 - val_mse: 0.5373\n",
      "3096/3096 [==============================] - 0s 50us/sample - loss: 0.6763 - mse: 0.5985\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 156us/sample - loss: 1.9895 - mse: 1.9010 - val_loss: 0.8289 - val_mse: 0.7429\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 1.2081 - mse: 1.1246 - val_loss: 0.6979 - val_mse: 0.6172\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.6512 - mse: 0.5704\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 148us/sample - loss: 1.9576 - mse: 1.8682 - val_loss: 0.8097 - val_mse: 0.7235\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 1.2211 - mse: 1.1379 - val_loss: 0.7048 - val_mse: 0.6250\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.6932 - mse: 0.6134\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 148us/sample - loss: 2.0333 - mse: 1.9427 - val_loss: 0.9022 - val_mse: 0.8141\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 1.2201 - mse: 1.1350 - val_loss: 0.7490 - val_mse: 0.6672\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.7885 - mse: 0.7067\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 1s 151us/sample - loss: 2.1062 - mse: 2.0127 - val_loss: 0.8539 - val_mse: 0.7632\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 1.2427 - mse: 1.1551 - val_loss: 0.7288 - val_mse: 0.6443\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.7033 - mse: 0.6188\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/2\n",
      "9907/9907 [==============================] - 2s 152us/sample - loss: 2.0139 - mse: 1.9236 - val_loss: 0.8193 - val_mse: 0.7320\n",
      "Epoch 2/2\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 1.2410 - mse: 1.1565 - val_loss: 0.6813 - val_mse: 0.5995\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.7399 - mse: 0.6581\n",
      "Train on 12384 samples, validate on 3096 samples\n",
      "Epoch 1/2\n",
      "12384/12384 [==============================] - 1s 108us/sample - loss: 0.8969 - mse: 0.8413 - val_loss: 0.5700 - val_mse: 0.5152\n",
      "Epoch 2/2\n",
      "12384/12384 [==============================] - 1s 77us/sample - loss: 0.5145 - mse: 0.4601 - val_loss: 0.5030 - val_mse: 0.4486\n",
      "5160/5160 [==============================] - 0s 43us/sample - loss: 0.5007 - mse: 0.4463\n",
      "\n",
      "\n",
      "----------------------------\n",
      "Mejores parametros: \n",
      "{'reg__activa': ('tanh', 'tanh', 'tanh', 'tanh', 'tanh', None), 'reg__dropout': 0.0, 'reg__epochs': 2, 'reg__h': (50, 20, 10, 1), 'reg__init': 'glorot_uniform', 'reg__optimizer': 'rmsprop'}\n",
      "Mejor Puntaje: -0.501\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modelo keras\n",
    "\n",
    "# Definir funcion para facilitar ingreso de funciones de activacion\n",
    "def activ(funci,canti):\n",
    "    total=[]\n",
    "    if(len(canti)==1):\n",
    "        for funcion in funci:\n",
    "            for iy in range(canti):\n",
    "                total.append(funcion)\n",
    "    elif(len(funci)==len(canti)):\n",
    "        for ix in range(len(funci)):\n",
    "            for iy in range(canti[ix]):\n",
    "                total.append(funci[ix])\n",
    "    else:\n",
    "        print('Mal ingresado parametros')\n",
    "    return total\n",
    "## Ejemplo: activ(['tanh','sigm'],[2,4]) devuele: ['tanh','tanh','sigm','sigm','sigm','sigm'] .. en la cantidad [2,4] indicada\n",
    "\n",
    "\n",
    "# cargar datos\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "# estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # fit + transform\n",
    "X_test = scaler.transform(X_test) # transform (usando entrenado)\n",
    "\n",
    "print('\\n')\n",
    "print('Dim X train: ',X_train.shape)\n",
    "print('Dim Y train: ',y_train.shape)\n",
    "print('\\n')\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# dimension entrada ... se indica en la primera capa de la red neuronal\n",
    "dimension = X_train.shape[1:]\n",
    "\n",
    "def crear_modelo_deep(optimizer='sgd',h=[30,40,10,1],activa=activ(['tanh',None],[3,1]),init ='glorot_uniform',alpha=0.001,dropout=0.2): \n",
    "    \n",
    "    # Funcion de activacion \"None\" equivale a decirle que la activacion es la funcion lineal a(x)=x ... es la funcion lineal pura\n",
    "    \n",
    "    # modelo secuencial\n",
    "    modeli = keras.Sequential()\n",
    "    \n",
    "    # definir dimension de entradas\n",
    "    modeli.add(keras.Input(shape=(dimension)))\n",
    "    \n",
    "    # De la primera capa a la capa penultima layers[1:-1]\n",
    "    for ix in range(len(h)-1):\n",
    "        modeli.add(Dense(h[ix],activation=activa[ix],kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "\n",
    "        # Agregar capa de Drop-Out a capas intermedias con un ratio igual a \"dropout\"=probabilidad desactivacion de neurona aleatoreamente\n",
    "        modeli.add(keras.layers.Dropout(rate=dropout))\n",
    "    \n",
    "    # Capa de salida o final\n",
    "    modeli.add(Dense(h[-1],activation=activa[-1]))\n",
    " \n",
    "    #compilar y retornar objeto modelo\n",
    "    modeli.compile(loss='mse', optimizer = optimizer ,metrics=['mse'])\n",
    "    return modeli\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "########  NO USAR ########\n",
    "# En gridsearch no se recomienda hacer uso de callbacks, pueden producir error\n",
    "\n",
    "# Callbacks para el entrenamiento \n",
    "direccion = 'mejor_modelo_entrenado.h5' # nombre de archivo a guardar modelo entrenado\n",
    "chk = keras.callbacks.ModelCheckpoint(direccion,save_best_only=True,verbose=2)\n",
    "stp = keras.callbacks.EarlyStopping(patience=20,mode='auto',min_delta=0,restore_best_weights=True,verbose=2)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(factor=0.9,patience=8,verbose=2)\n",
    "\n",
    "#####################################\n",
    "### Parametros de entrenamiento #####\n",
    "epochs = 15\n",
    "shuffle =True\n",
    "callbacks = [chk,stp,lrs] ## No se recomienda usar callbacks en un GridSearch\n",
    "batch_size = 128\n",
    "\n",
    "# wrapping... de keras a scikit model... esto convierte a model a tipo Scikite (posee .fit() , .predict() ,  .evaluate())\n",
    "model= tf.keras.wrappers.scikit_learn.KerasRegressor(crear_modelo_deep,epochs=epochs,shuffle=True,validation_split=0.2,verbose=1)\n",
    "\n",
    "# Definir Pipeline: Scaler + Keras Model\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('reg', model)])\n",
    "\n",
    "################################################################\n",
    "#### Grilla de busqueda de mejores parametros para la ANN ######\n",
    "\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "inits = ['glorot_uniform']\n",
    "dropouts =[0.0,0.2]\n",
    "epochss = [2]\n",
    "\n",
    "# Neuronas por capa ... varias arquitecuras con profundidad variable\n",
    "hs=[(100,30,1),(50,20,10,1),(40,30,20,10,5,1)]\n",
    "\n",
    "# Generar funciones de activacion segun \"hs\"\n",
    "activas = [tuple(activ(['tanh',None],[len(hs[i])-1,1])) for i in range(len(hs))]\n",
    "\n",
    "# grilla de parametros\n",
    "param_grid = {'reg__optimizer':optimizers,\n",
    "              'reg__epochs':epochss,\n",
    "              'reg__init':inits,\n",
    "              'reg__dropout':dropouts,\n",
    "              'reg__activa':activas,\n",
    "             'reg__h': hs}\n",
    "\n",
    "# Gridsearch CV\n",
    "search = GridSearchCV(pipe, param_grid)\n",
    "\n",
    "# entrenar modelos y busqueda mediante GridSearchCV\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# ver ajuste del modelo entrenado\n",
    "valor = search.score(X_test,y_test)\n",
    "\n",
    "# Mejores parametros\n",
    "print('\\n\\n----------------------------')\n",
    "print('Mejores parametros: ')\n",
    "print(search.best_params_)\n",
    "print('Mejor Puntaje:' ,np.round(valor,3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T04:01:02.652438Z",
     "start_time": "2021-02-18T04:01:02.350184Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 48us/sample - loss: 0.5007 - mse: 0.4463\n",
      "\n",
      "Puntaje de modelo: -50.07 %\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "10/10 [==============================] - 0s 393us/sample\n",
      "[3.042 3.46  1.25  0.5   0.76  2.971 2.82  2.139 1.484 2.056]\n",
      "\n",
      "Valores Orginales: \n",
      "[2.896 3.875 0.846 0.533 0.567 5.    2.349 1.571 2.123 1.676]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediccion\n",
    "\n",
    "valor = search.score(X_test,y_test)\n",
    "print('\\nPuntaje de modelo:' ,np.round(valor*100.0,2),'%')\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(search.predict(X_test[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regresion con GridSearchCV con combinaciones restringidas entre parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T04:04:21.145056Z",
     "start_time": "2021-02-18T04:01:45.825041Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dim X train:  (15480, 8)\n",
      "Dim Y train:  (15480,)\n",
      "\n",
      "\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 123us/sample - loss: 1.1342 - mse: 1.0939 - val_loss: 0.5276 - val_mse: 0.4874\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.5145 - mse: 0.4749 - val_loss: 0.5160 - val_mse: 0.4771\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.4992 - mse: 0.4606 - val_loss: 0.5017 - val_mse: 0.4635\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.4873 - mse: 0.4493 - val_loss: 0.4949 - val_mse: 0.4571\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.4725 - mse: 0.4349 - val_loss: 0.4736 - val_mse: 0.4359\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4823 - mse: 0.4447\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 1.2376 - mse: 1.1964- ETA: 0s - loss: 1.8287 - mse: - 1s 117us/sample - loss: 1.2036 - mse: 1.1624 - val_loss: 0.5411 - val_mse: 0.5001\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 83us/sample - loss: 0.5272 - mse: 0.4870 - val_loss: 0.5046 - val_mse: 0.4650\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.5114 - mse: 0.4724 - val_loss: 0.5037 - val_mse: 0.4653\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.5028 - mse: 0.4650 - val_loss: 0.4871 - val_mse: 0.4495\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.4924 - mse: 0.4553 - val_loss: 0.4863 - val_mse: 0.4498\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4770 - mse: 0.4405\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 122us/sample - loss: 1.1466 - mse: 1.1045 - val_loss: 0.5162 - val_mse: 0.4746\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 83us/sample - loss: 0.5007 - mse: 0.4601 - val_loss: 0.5083 - val_mse: 0.4684\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.4778 - mse: 0.4384 - val_loss: 0.4765 - val_mse: 0.4375\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.4578 - mse: 0.4191 - val_loss: 0.4508 - val_mse: 0.4123\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.4405 - mse: 0.4019 - val_loss: 0.4425 - val_mse: 0.4039\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.4611 - mse: 0.4225\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 120us/sample - loss: 1.1950 - mse: 1.1537 - val_loss: 0.5478 - val_mse: 0.5068\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.5329 - mse: 0.4925 - val_loss: 0.5125 - val_mse: 0.4728\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.5115 - mse: 0.4724 - val_loss: 0.4916 - val_mse: 0.4530\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.4939 - mse: 0.4557 - val_loss: 0.4815 - val_mse: 0.4439\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.4758 - mse: 0.4385 - val_loss: 0.4679 - val_mse: 0.4309\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.4689 - mse: 0.4319\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 121us/sample - loss: 1.2068 - mse: 1.1654 - val_loss: 0.5102 - val_mse: 0.4692\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 88us/sample - loss: 0.5216 - mse: 0.4811 - val_loss: 0.4951 - val_mse: 0.4550\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.5027 - mse: 0.4632 - val_loss: 0.4750 - val_mse: 0.4359\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 85us/sample - loss: 0.4892 - mse: 0.4506 - val_loss: 0.4690 - val_mse: 0.4306\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 86us/sample - loss: 0.4788 - mse: 0.4408 - val_loss: 0.4584 - val_mse: 0.4204\n",
      "3096/3096 [==============================] - 0s 46us/sample - loss: 0.4620 - mse: 0.4240\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 129us/sample - loss: 1.4344 - mse: 1.3969 - val_loss: 0.5332 - val_mse: 0.4975\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.6752 - mse: 0.6407 - val_loss: 0.5173 - val_mse: 0.4839\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 90us/sample - loss: 0.6247 - mse: 0.5923 - val_loss: 0.5112 - val_mse: 0.4796\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.5966 - mse: 0.5658 - val_loss: 0.4996 - val_mse: 0.4693\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 0.5818 - mse: 0.552 - 1s 88us/sample - loss: 0.5796 - mse: 0.5500 - val_loss: 0.4984 - val_mse: 0.4696\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.5064 - mse: 0.4776\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 130us/sample - loss: 1.4259 - mse: 1.3869 - val_loss: 0.5296 - val_mse: 0.4928\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.6928 - mse: 0.6576 - val_loss: 0.5405 - val_mse: 0.5069\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.6313 - mse: 0.5986 - val_loss: 0.5123 - val_mse: 0.4808\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.6056 - mse: 0.5748 - val_loss: 0.5086 - val_mse: 0.4788\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.5798 - mse: 0.5507 - val_loss: 0.4949 - val_mse: 0.4664\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.4925 - mse: 0.4641\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 124us/sample - loss: 1.4452 - mse: 1.4059 - val_loss: 0.5159 - val_mse: 0.4789\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 85us/sample - loss: 0.6466 - mse: 0.6115 - val_loss: 0.4999 - val_mse: 0.4666\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5969 - mse: 0.5650 - val_loss: 0.4957 - val_mse: 0.4651\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5687 - mse: 0.5393 - val_loss: 0.4841 - val_mse: 0.4557\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5526 - mse: 0.5252 - val_loss: 0.4805 - val_mse: 0.4540\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.4964 - mse: 0.4699\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 126us/sample - loss: 1.3833 - mse: 1.3438 - val_loss: 0.5294 - val_mse: 0.4918\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 87us/sample - loss: 0.6873 - mse: 0.6511 - val_loss: 0.5245 - val_mse: 0.4895\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.6226 - mse: 0.5888 - val_loss: 0.5109 - val_mse: 0.4781\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.6121 - mse: 0.5802 - val_loss: 0.4970 - val_mse: 0.4661\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 91us/sample - loss: 0.5655 - mse: 0.5352 - val_loss: 0.4886 - val_mse: 0.4591\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.4936 - mse: 0.4641\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 132us/sample - loss: 1.6388 - mse: 1.5999 - val_loss: 0.5747 - val_mse: 0.5373\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.7078 - mse: 0.6722 - val_loss: 0.5159 - val_mse: 0.4815\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 92us/sample - loss: 0.6507 - mse: 0.6175 - val_loss: 0.5049 - val_mse: 0.4728\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 89us/sample - loss: 0.6120 - mse: 0.5807 - val_loss: 0.4953 - val_mse: 0.4649\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 93us/sample - loss: 0.5874 - mse: 0.5577 - val_loss: 0.4874 - val_mse: 0.4583\n",
      "3096/3096 [==============================] - 0s 44us/sample - loss: 0.4845 - mse: 0.4554\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 152us/sample - loss: 0.8538 - mse: 0.7317 - val_loss: 0.5826 - val_mse: 0.4632\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.5514 - mse: 0.4343 - val_loss: 0.5066 - val_mse: 0.3916\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.4961 - mse: 0.3839 - val_loss: 0.4865 - val_mse: 0.3772\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.4727 - mse: 0.3663 - val_loss: 0.4763 - val_mse: 0.3730\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.4582 - mse: 0.3577 - val_loss: 0.4595 - val_mse: 0.3616\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4730 - mse: 0.3751\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 151us/sample - loss: 0.8716 - mse: 0.7570 - val_loss: 0.5773 - val_mse: 0.4654\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.5386 - mse: 0.4287 - val_loss: 0.5700 - val_mse: 0.4619\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 106us/sample - loss: 0.4912 - mse: 0.3856 - val_loss: 0.5082 - val_mse: 0.4056\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 105us/sample - loss: 0.4727 - mse: 0.3724 - val_loss: 0.4509 - val_mse: 0.3526\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 105us/sample - loss: 0.4540 - mse: 0.3576 - val_loss: 0.4645 - val_mse: 0.3699\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4260 - mse: 0.3314\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 157us/sample - loss: 0.8942 - mse: 0.7715 - val_loss: 0.5877 - val_mse: 0.4680\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.5234 - mse: 0.4062 - val_loss: 0.5021 - val_mse: 0.3873\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.4705 - mse: 0.3589 - val_loss: 0.4559 - val_mse: 0.3473\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.4467 - mse: 0.3408 - val_loss: 0.4445 - val_mse: 0.3412\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 107us/sample - loss: 0.4297 - mse: 0.3286 - val_loss: 0.4430 - val_mse: 0.3441\n",
      "3096/3096 [==============================] - 0s 48us/sample - loss: 0.4553 - mse: 0.3564\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 151us/sample - loss: 0.8146 - mse: 0.6948 - val_loss: 0.5913 - val_mse: 0.4747\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 107us/sample - loss: 0.5610 - mse: 0.4471 - val_loss: 0.5941 - val_mse: 0.4826\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 109us/sample - loss: 0.5034 - mse: 0.3944 - val_loss: 0.5506 - val_mse: 0.4440\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 111us/sample - loss: 0.4772 - mse: 0.3731 - val_loss: 0.5104 - val_mse: 0.4090ss: 0.4781 - mse: 0.374\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 103us/sample - loss: 0.4581 - mse: 0.3588 - val_loss: 0.4700 - val_mse: 0.3730\n",
      "3096/3096 [==============================] - 0s 45us/sample - loss: 0.4762 - mse: 0.3791\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 159us/sample - loss: 0.9285 - mse: 0.8068 - val_loss: 0.5792 - val_mse: 0.4600\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.5442 - mse: 0.4275 - val_loss: 0.5203 - val_mse: 0.4067\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.5047 - mse: 0.3945 - val_loss: 0.4894 - val_mse: 0.3827\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.4816 - mse: 0.3780 - val_loss: 0.4969 - val_mse: 0.3969\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 95us/sample - loss: 0.4674 - mse: 0.3703 - val_loss: 0.4686 - val_mse: 0.3742\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.4572 - mse: 0.3628\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 151us/sample - loss: 1.2813 - mse: 1.1657 - val_loss: 0.6344 - val_mse: 0.5253\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 99us/sample - loss: 0.8113 - mse: 0.7076 - val_loss: 0.5830 - val_mse: 0.4845\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.7019 - mse: 0.6082 - val_loss: 0.5632 - val_mse: 0.4738\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.6580 - mse: 0.5729 - val_loss: 0.5313 - val_mse: 0.4502\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.6196 - mse: 0.5419 - val_loss: 0.5048 - val_mse: 0.4301\n",
      "3096/3096 [==============================] - 0s 42us/sample - loss: 0.5178 - mse: 0.4431\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 175us/sample - loss: 1.3384 - mse: 1.2211 - val_loss: 0.6471 - val_mse: 0.5359\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 113us/sample - loss: 0.8186 - mse: 0.7130 - val_loss: 0.5786 - val_mse: 0.4785\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 106us/sample - loss: 0.7399 - mse: 0.6444 - val_loss: 0.5490 - val_mse: 0.4581\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 104us/sample - loss: 0.6838 - mse: 0.5969 - val_loss: 0.5122 - val_mse: 0.4290\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.6395 - mse: 0.5599 - val_loss: 0.5002 - val_mse: 0.4240\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.4809 - mse: 0.4046\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 158us/sample - loss: 1.3339 - mse: 1.2166 - val_loss: 0.6400 - val_mse: 0.5298\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.8093 - mse: 0.7051 - val_loss: 0.5970 - val_mse: 0.4985\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 105us/sample - loss: 0.7022 - mse: 0.6091 - val_loss: 0.5618 - val_mse: 0.4737\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.6474 - mse: 0.5633 - val_loss: 0.5158 - val_mse: 0.4359\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 102us/sample - loss: 0.5996 - mse: 0.5229 - val_loss: 0.4870 - val_mse: 0.4134\n",
      "3096/3096 [==============================] - 0s 43us/sample - loss: 0.5043 - mse: 0.4308\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 1s 151us/sample - loss: 1.3571 - mse: 1.2407 - val_loss: 0.6302 - val_mse: 0.5203\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 97us/sample - loss: 0.7911 - mse: 0.6868 - val_loss: 0.5996 - val_mse: 0.5008\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 101us/sample - loss: 0.7164 - mse: 0.6224 - val_loss: 0.5480 - val_mse: 0.4584\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 100us/sample - loss: 0.6682 - mse: 0.5828 - val_loss: 0.5295 - val_mse: 0.4480\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 98us/sample - loss: 0.6205 - mse: 0.5422 - val_loss: 0.5128 - val_mse: 0.4376\n",
      "3096/3096 [==============================] - 0s 61us/sample - loss: 0.5227 - mse: 0.4475\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 167us/sample - loss: 1.2717 - mse: 1.1581 - val_loss: 0.6151 - val_mse: 0.5082\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 110us/sample - loss: 0.7966 - mse: 0.6950 - val_loss: 0.5761 - val_mse: 0.4794\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 108us/sample - loss: 0.7001 - mse: 0.6076 - val_loss: 0.5589 - val_mse: 0.4707\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 108us/sample - loss: 0.6534 - mse: 0.5689 - val_loss: 0.5318 - val_mse: 0.4507\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 108us/sample - loss: 0.6284 - mse: 0.5505 - val_loss: 0.5141 - val_mse: 0.4391 loss: 0.6379 - mse: 0\n",
      "3096/3096 [==============================] - 0s 47us/sample - loss: 0.4996 - mse: 0.4247\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 216us/sample - loss: 1.8160 - mse: 1.6313 - val_loss: 0.8864 - val_mse: 0.7101\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 141us/sample - loss: 1.2227 - mse: 1.0550 - val_loss: 0.7523 - val_mse: 0.5936\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 134us/sample - loss: 1.0068 - mse: 0.8567 - val_loss: 0.6582 - val_mse: 0.5163\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 132us/sample - loss: 0.8887 - mse: 0.7542 - val_loss: 0.6064 - val_mse: 0.4797\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 0.8040 - mse: 0.6846 - val_loss: 0.5501 - val_mse: 0.4381\n",
      "3096/3096 [==============================] - 0s 51us/sample - loss: 0.5629 - mse: 0.4509\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 206us/sample - loss: 1.8145 - mse: 1.6309 - val_loss: 0.8537 - val_mse: 0.6777\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 128us/sample - loss: 1.2133 - mse: 1.0450 - val_loss: 0.7194 - val_mse: 0.5591\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 1.0506 - mse: 0.8987 - val_loss: 0.6710 - val_mse: 0.5284\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - ETA: 0s - loss: 0.9512 - mse: 0.816 - 1s 130us/sample - loss: 0.9487 - mse: 0.8144 - val_loss: 0.6129 - val_mse: 0.4870\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 129us/sample - loss: 0.8374 - mse: 0.7190 - val_loss: 0.5962 - val_mse: 0.4850\n",
      "3096/3096 [==============================] - 0s 50us/sample - loss: 0.5760 - mse: 0.4648\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 210us/sample - loss: 1.8065 - mse: 1.6259 - val_loss: 0.8896 - val_mse: 0.7169\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 128us/sample - loss: 1.1996 - mse: 1.0356 - val_loss: 0.7230 - val_mse: 0.5676\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 0.9943 - mse: 0.8486 - val_loss: 0.6475 - val_mse: 0.5111\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 133us/sample - loss: 0.8747 - mse: 0.7473 - val_loss: 0.5918 - val_mse: 0.4729\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 0.7892 - mse: 0.6781 - val_loss: 0.5525 - val_mse: 0.4488\n",
      "3096/3096 [==============================] - 0s 52us/sample - loss: 0.5715 - mse: 0.4678\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 207us/sample - loss: 1.6811 - mse: 1.5023 - val_loss: 0.8092 - val_mse: 0.6392\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 128us/sample - loss: 1.1314 - mse: 0.9700 - val_loss: 0.7380 - val_mse: 0.5849\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 127us/sample - loss: 0.9610 - mse: 0.8159 - val_loss: 0.6296 - val_mse: 0.4922\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 129us/sample - loss: 0.8504 - mse: 0.7205 - val_loss: 0.5880 - val_mse: 0.4653\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 130us/sample - loss: 0.7822 - mse: 0.6672 - val_loss: 0.5415 - val_mse: 0.4334\n",
      "3096/3096 [==============================] - 0s 53us/sample - loss: 0.5589 - mse: 0.4508\n",
      "Train on 9907 samples, validate on 2477 samples\n",
      "Epoch 1/5\n",
      "9907/9907 [==============================] - 2s 211us/sample - loss: 1.7345 - mse: 1.5474 - val_loss: 0.8695 - val_mse: 0.6912\n",
      "Epoch 2/5\n",
      "9907/9907 [==============================] - 1s 130us/sample - loss: 1.2240 - mse: 1.0545 - val_loss: 0.7293 - val_mse: 0.5685\n",
      "Epoch 3/5\n",
      "9907/9907 [==============================] - 1s 129us/sample - loss: 1.0153 - mse: 0.8630 - val_loss: 0.7134 - val_mse: 0.5696\n",
      "Epoch 4/5\n",
      "9907/9907 [==============================] - 1s 131us/sample - loss: 0.8959 - mse: 0.7602 - val_loss: 0.5998 - val_mse: 0.4718\n",
      "Epoch 5/5\n",
      "9907/9907 [==============================] - 1s 130us/sample - loss: 0.7993 - mse: 0.6787 - val_loss: 0.5735 - val_mse: 0.4609\n",
      "3096/3096 [==============================] - 0s 51us/sample - loss: 0.5453 - mse: 0.43270s - loss: 0.5315 - mse: 0.4\n",
      "Train on 12384 samples, validate on 3096 samples\n",
      "Epoch 1/5\n",
      "12384/12384 [==============================] - 1s 116us/sample - loss: 0.7747 - mse: 0.6546 - val_loss: 0.5435 - val_mse: 0.4271\n",
      "Epoch 2/5\n",
      "12384/12384 [==============================] - 1s 81us/sample - loss: 0.5357 - mse: 0.4221 - val_loss: 0.4855 - val_mse: 0.3755\n",
      "Epoch 3/5\n",
      "12384/12384 [==============================] - 1s 80us/sample - loss: 0.4896 - mse: 0.3832 - val_loss: 0.4609 - val_mse: 0.3578\n",
      "Epoch 4/5\n",
      "12384/12384 [==============================] - 1s 81us/sample - loss: 0.4650 - mse: 0.3647 - val_loss: 0.4441 - val_mse: 0.3469\n",
      "Epoch 5/5\n",
      "12384/12384 [==============================] - 1s 81us/sample - loss: 0.4476 - mse: 0.3529 - val_loss: 0.4437 - val_mse: 0.3513\n",
      "5160/5160 [==============================] - 0s 45us/sample - loss: 0.4736 - mse: 0.3811\n",
      "\n",
      "\n",
      "----------------------------\n",
      "Mejores parametros: \n",
      "{'reg__activa': ('tanh', 'tanh', 'tanh', 'tanh', 1), 'reg__dropout': 0.0, 'reg__epochs': 5, 'reg__h': (60, 40, 30, 20, 1), 'reg__init': 'glorot_uniform', 'reg__optimizer': 'rmsprop'}\n",
      "Mejor Puntaje: -0.474\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modelo keras\n",
    "\n",
    "# Definir funcion para facilitar ingreso de funciones de activacion\n",
    "def activ(funci,canti):\n",
    "    total=[]\n",
    "    if(len(canti)==1):\n",
    "        for funcion in funci:\n",
    "            for iy in range(canti):\n",
    "                total.append(funcion)\n",
    "    elif(len(funci)==len(canti)):\n",
    "        for ix in range(len(funci)):\n",
    "            for iy in range(canti[ix]):\n",
    "                total.append(funci[ix])\n",
    "    else:\n",
    "        print('Mal ingresado parametros')\n",
    "    return total\n",
    "## Ejemplo: activ(['tanh','sigm'],[2,4]) devuele: ['tanh','tanh','sigm','sigm','sigm','sigm'] .. en la cantidad [2,4] indicada\n",
    "\n",
    "\n",
    "# cargar datos\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "# estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # fit + transform\n",
    "X_test = scaler.transform(X_test) # transform (usando entrenado)\n",
    "\n",
    "print('\\n')\n",
    "print('Dim X train: ',X_train.shape)\n",
    "print('Dim Y train: ',y_train.shape)\n",
    "print('\\n')\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# dimension entrada ... se indica en la primera capa de la red neuronal\n",
    "dimension = X_train.shape[1:]\n",
    "\n",
    "def crear_modelo_deep(optimizer='sgd',h=[30,40,10,1],activa=activ(['tanh',None],[3,1]),init ='glorot_uniform',alpha=0.001,dropout=0.2): \n",
    "    \n",
    "    # Funcion de activacion \"None\" equivale a decirle que la activacion es la funcion lineal a(x)=x ... es la funcion lineal pura\n",
    "    \n",
    "    # modelo secuencial\n",
    "    modeli = keras.Sequential()\n",
    "    \n",
    "    # definir dimension de entradas\n",
    "    modeli.add(keras.Input(shape=(dimension)))\n",
    "    \n",
    "    # De la capa 2 a la capa penultima layers[1:-1]\n",
    "    for ix in range(len(h)-1):\n",
    "        modeli.add(Dense(h[ix],activation=activa[ix],kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "\n",
    "        # Agregar capa de Drop-Out a capas intermedias con un ratio igual a \"dropout\"=probabilidad desactivacion de neurona aleatoreamente\n",
    "        modeli.add(keras.layers.Dropout(rate=dropout))\n",
    "    \n",
    "    # Capa de salida o final\n",
    "    if(activa[-1] != 1):\n",
    "        modeli.add(Dense(h[-1],activation=activa[-1]))\n",
    "    else:\n",
    "        modeli.add(Dense(h[-1]))\n",
    " \n",
    "    #compilar y retornar objeto modelo\n",
    "    modeli.compile(loss='mse', optimizer = optimizer ,metrics=['mse'])\n",
    "    return modeli\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "########  NO USAR ########\n",
    "# En gridsearch no se recomienda hacer uso de callbacks, pueden producir error\n",
    "\n",
    "# Callbacks para el entrenamiento \n",
    "direccion = 'mejor_modelo_entrenado.h5' # nombre de archivo a guardar modelo entrenado\n",
    "chk = keras.callbacks.ModelCheckpoint(direccion,save_best_only=True,verbose=2)\n",
    "stp = keras.callbacks.EarlyStopping(patience=20,mode='auto',min_delta=0,restore_best_weights=True,verbose=2)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(factor=0.9,patience=8,verbose=2)\n",
    "\n",
    "#####################################\n",
    "### Parametros de entrenamiento #####\n",
    "epochs = 15\n",
    "shuffle =True\n",
    "callbacks = [chk,stp,lrs] ## No se recomienda usar callbacks en un GridSearch\n",
    "batch_size = 128\n",
    "\n",
    "# wrapping... de keras a scikit model... esto convierte a model a tipo Scikite (posee .fit() , .predict() ,  .evaluate())\n",
    "model= tf.keras.wrappers.scikit_learn.KerasRegressor(crear_modelo_deep,epochs=epochs,shuffle=True,validation_split=0.2,verbose=1)\n",
    "\n",
    "# Definir Pipeline: Scaler + Keras Model\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('reg', model)])\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "#############################\n",
    "######## Grilla 1 ###########\n",
    "\n",
    "optimizers = ['rmsprop']\n",
    "inits = ['glorot_uniform']\n",
    "dropouts =[0.0,0.2]\n",
    "epochss = [5]\n",
    "\n",
    "# Neuronas por capa ... varias arquitecuras con profundidad variable\n",
    "hs=[(40,20,1)]\n",
    "\n",
    "# Generar funciones de activacion segun \"hs\"\n",
    "activas = [tuple(activ(['tanh',1],[len(hs[i])-1,1])) for i in range(len(hs))]\n",
    "\n",
    "# grilla de parametros\n",
    "pg1 = {'reg__optimizer':optimizers,\n",
    "              'reg__epochs':epochss,\n",
    "              'reg__init':inits,\n",
    "              'reg__dropout':dropouts,\n",
    "              'reg__activa':activas,\n",
    "             'reg__h': hs}\n",
    "\n",
    "\n",
    "#############################\n",
    "######## Grilla 2 ###########\n",
    "\n",
    "optimizers = ['rmsprop']\n",
    "inits = ['glorot_uniform']\n",
    "dropouts =[0.0,0.2]\n",
    "epochss = [5]\n",
    "\n",
    "# Neuronas por capa ... varias arquitecuras con profundidad variable\n",
    "hs=[(60,40,30,20,1)]\n",
    "\n",
    "# Generar funciones de activacion segun \"hs\"\n",
    "activas = [tuple(activ(['tanh',1],[len(hs[i])-1,1])) for i in range(len(hs))]\n",
    "\n",
    "# grilla de parametros\n",
    "pg2 = {'reg__optimizer':optimizers,\n",
    "              'reg__epochs':epochss,\n",
    "              'reg__init':inits,\n",
    "              'reg__dropout':dropouts,\n",
    "              'reg__activa':activas,\n",
    "             'reg__h': hs}\n",
    "\n",
    "#############################\n",
    "######## Grilla 3 ###########\n",
    "\n",
    "optimizers = ['rmsprop']\n",
    "inits = ['glorot_uniform']\n",
    "dropouts =[0.2]\n",
    "epochss = [5]\n",
    "\n",
    "# Neuronas por capa ... varias arquitecuras con profundidad variable\n",
    "hs=[(60,50,40,30,20,10,5,1)]\n",
    "\n",
    "# Generar funciones de activacion segun \"hs\"\n",
    "activas = [tuple(activ(['tanh',1],[len(hs[i])-1,1])) for i in range(len(hs))]\n",
    "\n",
    "# grilla de parametros\n",
    "pg3 = {'reg__optimizer':optimizers,\n",
    "              'reg__epochs':epochss,\n",
    "              'reg__init':inits,\n",
    "              'reg__dropout':dropouts,\n",
    "              'reg__activa':activas,\n",
    "             'reg__h': hs}\n",
    "\n",
    "###############################\n",
    "######## Grilla Total #########\n",
    "\n",
    "param_grid = [pg1, pg2, pg3]\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "# Gridsearch CV\n",
    "search = GridSearchCV(pipe, param_grid)\n",
    "\n",
    "# entrenar modelos y busqueda mediante GridSearchCV\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# ver ajuste del modelo entrenado\n",
    "valor = search.score(X_test,y_test)\n",
    "\n",
    "# Mejores parametros\n",
    "print('\\n\\n----------------------------')\n",
    "print('Mejores parametros: ')\n",
    "print(search.best_params_)\n",
    "print('Mejor Puntaje:' ,np.round(valor,3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T04:04:21.545731Z",
     "start_time": "2021-02-18T04:04:21.206843Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 49us/sample - loss: 0.4736 - mse: 0.3811\n",
      "\n",
      "Puntaje de modelo: -47.36 %\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "10/10 [==============================] - 0s 3ms/sample\n",
      "[1.87  0.649 2.746 2.474 1.446 1.479 1.098 1.412 0.568 1.179]\n",
      "\n",
      "Valores Orginales: \n",
      "[1.286 1.031 2.601 2.61  1.975 1.588 0.774 1.594 0.728 0.835]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediccion\n",
    "\n",
    "valor = search.score(X_test,y_test)\n",
    "print('\\nPuntaje de modelo:' ,np.round(valor*100.0,2),'%')\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(search.predict(X_test[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Clasificacion NN (Scikit Learn + Keras) / (Normal - GridSearchCV) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br><br><b>Importante !!!</b>: Es vital tener en cuenta que si lo que se quiere es convertir un modelo <b>Keras</b> a uno tipo <b>Scikit</b> para poder aplicar sus bondades como es el GridSearchCV, no se puede usar todos los modelos de Keras, el recomendado o unica opcion es usar <b>keras.Sequential()</b> esto debido a que para el Clasificador le brinda acceso al calculo mediante <b>modelo.predic_proba</b> y <b>modelo.predic</b>, estos son fundamentales en el calculo una vez entrenado (fit) el modelo a los datos.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Clasificacion Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:41:24.991414Z",
     "start_time": "2021-02-15T14:41:24.633759Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "X_train shape:  (60000, 784)\n",
      "X_test shape:  (10000, 784)\n",
      "-------------------\n",
      "y_train shape:  (60000,)\n",
      "y_test shape:  (10000,)\n",
      "-------------------\n",
      "Clases clasificacion:  10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar Datos\n",
    "\n",
    "(X_train , y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train = X_train.reshape(60000,784).astype('float32')/255.0\n",
    "X_test = X_test.reshape(10000,784).astype('float32')/255.0\n",
    "\n",
    "print('\\n\\nX_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('-------------------')\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "print('-------------------')\n",
    "print('Clases clasificacion: ',len(set(y_test)))\n",
    "print('\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### definir modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:43:05.469601Z",
     "start_time": "2021-02-15T14:41:27.304732Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.06439812\n",
      "Validation score: 0.916500\n",
      "Iteration 2, loss = 0.39500154\n",
      "Validation score: 0.942833\n",
      "Iteration 3, loss = 0.36267211\n",
      "Validation score: 0.958667\n",
      "Iteration 4, loss = 0.35363813\n",
      "Validation score: 0.936667\n",
      "Iteration 5, loss = 0.34392835\n",
      "Validation score: 0.954500\n",
      "Iteration 6, loss = 0.34126844\n",
      "Validation score: 0.958333\n",
      "Iteration 7, loss = 0.33807683\n",
      "Validation score: 0.954667\n",
      "Iteration 8, loss = 0.33417533\n",
      "Validation score: 0.963667\n",
      "Iteration 9, loss = 0.33471520\n",
      "Validation score: 0.945500\n",
      "Iteration 10, loss = 0.33292390\n",
      "Validation score: 0.955833\n",
      "Iteration 11, loss = 0.32799062\n",
      "Validation score: 0.954500\n",
      "Iteration 12, loss = 0.33150278\n",
      "Validation score: 0.950667\n",
      "Iteration 13, loss = 0.33364436\n",
      "Validation score: 0.947333\n",
      "Iteration 14, loss = 0.32739354\n",
      "Validation score: 0.961167\n",
      "Iteration 15, loss = 0.32586645\n",
      "Validation score: 0.963333\n",
      "Iteration 16, loss = 0.32727722\n",
      "Validation score: 0.962500\n",
      "Iteration 17, loss = 0.32722918\n",
      "Validation score: 0.957000\n",
      "Iteration 18, loss = 0.32773372\n",
      "Validation score: 0.957167\n",
      "Iteration 19, loss = 0.32473170\n",
      "Validation score: 0.963667\n",
      "Iteration 20, loss = 0.32693552\n",
      "Validation score: 0.964000\n",
      "Iteration 21, loss = 0.32650097\n",
      "Validation score: 0.960333\n",
      "Iteration 22, loss = 0.32451730\n",
      "Validation score: 0.964667\n",
      "Iteration 23, loss = 0.32388061\n",
      "Validation score: 0.958667\n",
      "Iteration 24, loss = 0.32467848\n",
      "Validation score: 0.959667\n",
      "Iteration 25, loss = 0.32663991\n",
      "Validation score: 0.966833\n",
      "Iteration 26, loss = 0.32549645\n",
      "Validation score: 0.944500\n",
      "Iteration 27, loss = 0.32388477\n",
      "Validation score: 0.966167\n",
      "Iteration 28, loss = 0.32133208\n",
      "Validation score: 0.938000\n",
      "Iteration 29, loss = 0.32363438\n",
      "Validation score: 0.959833\n",
      "Iteration 30, loss = 0.32366360\n",
      "Validation score: 0.960000\n",
      "Iteration 31, loss = 0.32050278\n",
      "Validation score: 0.957833\n",
      "Iteration 32, loss = 0.32306116\n",
      "Validation score: 0.949500\n",
      "Iteration 33, loss = 0.32367644\n",
      "Validation score: 0.960667\n",
      "Iteration 34, loss = 0.32194159\n",
      "Validation score: 0.959500\n",
      "Iteration 35, loss = 0.32414428\n",
      "Validation score: 0.953167\n",
      "Iteration 36, loss = 0.32417496\n",
      "Validation score: 0.964500\n",
      "Iteration 37, loss = 0.32388831\n",
      "Validation score: 0.964167\n",
      "Iteration 38, loss = 0.32601869\n",
      "Validation score: 0.960333\n",
      "Iteration 39, loss = 0.32056471\n",
      "Validation score: 0.965333\n",
      "Iteration 40, loss = 0.32104847\n",
      "Validation score: 0.952833\n",
      "Iteration 41, loss = 0.32071148\n",
      "Validation score: 0.953333\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "n\\Fin entrenamiento\n",
      "\n",
      "\n",
      "R^2 modelo: 96.57 %\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "\n",
    "model = MLPClassifier((100,60,30),\n",
    "                      activation='logistic',\n",
    "                      solver='sgd',\n",
    "                      random_state=42,\n",
    "                      batch_size = 'auto',\n",
    "                      max_iter=300,\n",
    "                      verbose=True,\n",
    "                      early_stopping=True,\n",
    "                      learning_rate_init = 0.2,\n",
    "                      power_t = 0.9,\n",
    "                      alpha = 0.01, \n",
    "                      shuffle=True,\n",
    "                      epsilon=1e-10,\n",
    "                      tol=1e-8,\n",
    "                      n_iter_no_change=15,\n",
    "                      validation_fraction=0.1)\n",
    "\n",
    "history = model.fit(X_train,y_train)\n",
    "\n",
    "#############################################################\n",
    "####### Ver curvas de Aprendizaje del modelo #############\n",
    "\n",
    "historia = history.history\n",
    "\n",
    "print('\\n\\nCurvas de Aprendizaje\\n')\n",
    "for nombre,valores in historia.items():\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.title('Curva de ' + nombre)\n",
    "    plt.xlabel('Interacion \"i\"')\n",
    "    plt.ylabel(nombre)\n",
    "    plt.plot(valores)\n",
    "    plt.show()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "valor = model.score(X_test,y_test)\n",
    "\n",
    "print('n\\Fin entrenamiento')\n",
    "print('\\n\\nR^2 modelo:' ,np.round(valor*100.0,3),'%')\n",
    "print('\\n\\n')\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### ver parametros y ajuste del modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:44:35.130028Z",
     "start_time": "2021-02-15T14:44:34.965191Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------\n",
      "R^2 del modelo:  96.57 %\n",
      "Funcion activacion:  softmax\n",
      "Cantidad de capas:  softmax\n",
      "Numero iteraciones:  41\n",
      "Cantidad datos usados:  2214000\n",
      "\n",
      "\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ver capas entrenadas: \n",
      "---------------------\n",
      "\n",
      "Capa N°  1\n",
      "Dimension:  (784, 100)\n",
      "\n",
      "\n",
      "[[-3.59441289e-14  1.29129192e-13  6.64660293e-14 ... -2.07594248e-14\n",
      "  -1.35966939e-13 -1.12338709e-13]\n",
      " [-1.34245064e-13  3.90814447e-14 -5.31868234e-14 ...  1.13771687e-13\n",
      "   1.10899868e-13  8.01840604e-14]\n",
      " [ 4.06919228e-14 -1.19143479e-13 -9.69430307e-14 ... -8.14169878e-14\n",
      "   3.52079968e-14 -1.18797531e-13]\n",
      " ...\n",
      " [ 4.19963216e-14 -4.10873138e-14 -2.47085360e-14 ... -1.02825448e-13\n",
      "  -1.22879644e-13  5.25759789e-14]\n",
      " [-5.31844197e-14 -1.37450047e-13 -4.65008668e-14 ...  9.87715533e-14\n",
      "  -4.38949773e-14  4.17747211e-14]\n",
      " [ 1.05606848e-13  7.19730839e-15  5.23719719e-14 ...  8.49548205e-14\n",
      "  -1.26371104e-13 -6.00531912e-15]]\n",
      "\n",
      "\n",
      "**********************\n",
      "Capa N°  2\n",
      "Dimension:  (100, 60)\n",
      "\n",
      "\n",
      "[[ 0.17749067  0.07217923 -0.04956509 ... -0.02725065 -0.2668488\n",
      "   0.16838155]\n",
      " [ 0.24853445  0.14391     0.06837333 ... -0.01242934  0.04559496\n",
      "   0.56915418]\n",
      " [-0.04685691 -0.18462342 -0.08998905 ...  0.00283225  0.01237699\n",
      "  -0.19129876]\n",
      " ...\n",
      " [ 0.04785564  0.08254852 -0.02336303 ... -0.10944327 -0.30239454\n",
      "   0.12362158]\n",
      " [ 0.01217398  0.0055262  -0.00067694 ... -0.00615768 -0.00919305\n",
      "   0.01950001]\n",
      " [ 0.00808947  0.16958589  0.30240678 ... -0.02149379 -0.08111387\n",
      "   0.00650351]]\n",
      "\n",
      "\n",
      "**********************\n",
      "Capa N°  3\n",
      "Dimension:  (60, 30)\n",
      "\n",
      "\n",
      "[[ 0.20712253  0.08236377  0.1570998  ...  0.12946874 -0.05869932\n",
      "   0.16721546]\n",
      " [ 0.12408902  0.08693541 -0.42042855 ... -0.23609411 -0.17908711\n",
      "  -0.22023383]\n",
      " [-0.30961522  0.43680267 -0.44641783 ... -0.23915564  0.10122184\n",
      "  -0.06123454]\n",
      " ...\n",
      " [-0.0203795   0.08700581  0.24825122 ...  0.47453012 -0.09978718\n",
      "  -0.01354981]\n",
      " [-0.58807157  0.00888832 -0.11313137 ... -0.12846964  0.16212695\n",
      "  -0.0074718 ]\n",
      " [ 0.38762504 -0.19823933 -0.31449013 ... -0.22710981 -0.23123418\n",
      "  -0.08010727]]\n",
      "\n",
      "\n",
      "**********************\n",
      "Capa N°  4\n",
      "Dimension:  (30, 10)\n",
      "\n",
      "\n",
      "[[-0.84512433  1.08239166 -0.91881839  0.98149058 -0.70015293  1.61560851\n",
      "  -0.83925193  0.85343829 -1.07791658 -0.15166487]\n",
      " [-0.68245283  1.05449622  0.9198489  -0.89751676  1.04770084 -0.59620496\n",
      "  -0.28929105  1.12300689 -0.71965842 -0.95992883]\n",
      " [-0.96436115  1.00858711  0.87979428  0.78236088 -0.84736262 -1.66668645\n",
      "  -1.1428547  -0.04676796  1.29043007  0.70686054]\n",
      " [-0.68115289  1.05234145  0.9285218  -0.89863146  1.0306258  -0.59313168\n",
      "  -0.31578091  1.1365072  -0.70191384 -0.95738548]\n",
      " [-1.19754541  0.75240698 -1.26071539  0.90275371 -0.5180522   1.20206388\n",
      "  -0.88631575 -0.68548409  1.23684316  0.4540451 ]\n",
      " [-0.95010091  0.78914584 -1.07068181 -1.05351438  0.84999123  0.80601903\n",
      "   0.99881186 -0.87086326  1.1534595  -0.65226711]\n",
      " [-0.543025   -0.5047858  -0.08153577 -0.28696633  1.53272261 -0.49138829\n",
      "   0.16418083 -0.69843596 -0.62991564  1.53914936]\n",
      " [-0.78489131  0.80340706 -0.58866356 -0.48546462  0.95494655 -0.68671371\n",
      "  -0.56723786  1.09206699 -0.61206358  0.87461403]\n",
      " [ 1.54719867 -0.5050178  -0.28116939 -0.60192303 -0.73933698 -0.34852176\n",
      "  -0.57025417  1.29203496 -0.41336646  0.62035596]\n",
      " [-0.77252674  1.04448666  0.25251524 -0.47604211  1.70551652 -0.74517493\n",
      "   1.28346151 -0.75356992 -0.59988261 -0.93878361]\n",
      " [-0.64443712  0.61374438 -0.63950782 -0.75276138  0.8382439  -0.96722595\n",
      "  -0.57216481 -0.53597453  1.83310061  0.82698271]\n",
      " [-0.60163164 -0.0364325   1.33585134  1.59081841 -0.22644985 -0.54608927\n",
      "  -0.59025964  0.09101061 -0.46631164 -0.55050582]\n",
      " [-0.4237851   1.35862945 -0.02495106 -0.47992566 -0.47277573 -0.11751483\n",
      "  -0.44773835  1.43696165 -0.37983008 -0.4490703 ]\n",
      " [-0.53775588 -0.22100638 -0.34801962 -0.44364097  1.12245408 -0.49880117\n",
      "  -0.59079157  0.95451911 -0.64624286  1.20928526]\n",
      " [-0.48113567  1.25154767  0.46097079 -0.28740051 -0.47012873 -0.56258833\n",
      "   1.46349814 -0.67750876 -0.35381241 -0.3434422 ]\n",
      " [-0.6894585   1.05321254  0.93604494 -0.89885023  1.06479923 -0.59881068\n",
      "  -0.27396327  1.11027999 -0.72929429 -0.97395973]\n",
      " [-0.68209814  0.81211819  0.75970322 -0.62713026 -0.09033647 -0.99319744\n",
      "   0.72791451 -0.53435937  1.48918653 -0.86180077]\n",
      " [-0.83402916  1.06899431 -0.90008945  0.99056125 -0.68661317  1.62637997\n",
      "  -0.8285007   0.8211944  -1.07678566 -0.18111178]\n",
      " [-0.78081305  0.8743223  -0.59181315 -0.51698408  0.94935945 -0.66741677\n",
      "  -0.55914291  1.11401412 -0.59440566  0.77287974]\n",
      " [-0.60502099 -0.11558077 -0.496628    1.08591911 -0.64064123 -0.40749675\n",
      "  -0.48586298  0.68460369 -0.48853115  1.46923909]\n",
      " [ 1.08468128  0.52054911  0.50398548 -0.76668554 -0.78552542 -0.28196509\n",
      "  -0.55149108  1.44536737 -0.43463803 -0.73427807]\n",
      " [-0.48985913  1.30754762 -0.14006817 -0.2855566  -0.48379644 -0.1737081\n",
      "   1.53752761 -0.67763941 -0.31647523 -0.27797215]\n",
      " [-1.54792514  0.72834664 -1.33554076  0.40201199  0.1091331   1.20142492\n",
      "  -1.4615476   0.61021096  1.06058394  0.23330196]\n",
      " [-0.61383484  1.13710107 -0.77974429 -0.03303584 -0.58054944  1.19417992\n",
      "   1.3344853  -0.69762661 -0.57369155 -0.38728372]\n",
      " [ 0.94389509 -0.85757843  0.77004179  0.38098501 -1.21892534 -1.08398862\n",
      "  -1.13447783  0.62043859  0.86003449  0.71957525]\n",
      " [-0.53627216 -0.21742958 -0.35204414 -0.45010109  1.12210145 -0.49879027\n",
      "  -0.5915469   0.96015362 -0.64391631  1.20784539]\n",
      " [-0.63118486 -0.28325451 -0.46700689  1.41361957 -0.16698192  0.18760292\n",
      "  -0.45577958 -0.42965769 -0.54679543  1.37943838]\n",
      " [-0.95884446  1.13320515  0.9025452   0.84979519 -0.80018107 -1.09736948\n",
      "  -0.92367339  0.21393049  1.57379637 -0.89320399]\n",
      " [-0.59617987 -0.05365953  1.33632789  1.59116072 -0.22267354 -0.54396586\n",
      "  -0.58202823  0.0730721  -0.45347107 -0.54858259]\n",
      " [-0.86384439  1.08029313  1.10839173  1.32896506 -0.535994   -0.60654292\n",
      "  -0.75814853  0.61906958 -0.7121619  -0.66002777]]\n",
      "\n",
      "\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "# Ver detalles de parametros y ajustes del modelo entrenado\n",
    "\n",
    "print('\\n\\n----------------------')\n",
    "print('R^2 del modelo: ',np.round(valor*100.0,3),'%')\n",
    "print('Funcion activacion: ',model.out_activation_)\n",
    "print('Cantidad de capas: ',model.n_layers_)\n",
    "print('Numero iteraciones: ',model.n_iter_)\n",
    "print('Cantidad datos usados: ',model.t_)\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Curvas de Error (durante entrenamiento)\n",
    "\n",
    "loss_curve = model.loss_curve_\n",
    "\n",
    "print('\\n\\n----------------------')\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.title('Curva de Error')\n",
    "plt.xlabel('Interacion \"i\"')\n",
    "plt.ylabel('Error (Loss)')\n",
    "plt.plot(loss_curve)\n",
    "plt.show()\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Pesos de capas de neuronas\n",
    "\n",
    "capas = model.coefs_\n",
    "\n",
    "i = 1\n",
    "print('\\n\\nVer capas entrenadas: ')\n",
    "print('---------------------\\n')\n",
    "for capa in capas:\n",
    "    print('Capa N° ',i)\n",
    "    print('Dimension: ',capa.shape)\n",
    "    print('\\n')\n",
    "    print(capa)\n",
    "    i +=1\n",
    "    print('\\n\\n**********************')\n",
    "    \n",
    "    \n",
    "# fin    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### uso del modelo para predecir - validar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:44:52.435792Z",
     "start_time": "2021-02-15T14:44:52.339583Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R^2 de modelo: 96.57 %\n",
      "------------------------------\n",
      "\n",
      "Probabilidades de prediccion: \n",
      "[[0.001 0.    0.    0.032 0.    0.941 0.    0.    0.003 0.022]\n",
      " [0.    0.    0.    0.001 0.    0.    0.    0.001 0.001 0.996]\n",
      " [0.    0.    0.    0.001 0.001 0.001 0.    0.    0.002 0.994]\n",
      " [0.    0.001 0.    0.    0.    0.    0.    0.998 0.    0.001]\n",
      " [0.005 0.    0.    0.02  0.001 0.852 0.003 0.    0.037 0.082]\n",
      " [0.    0.001 0.    0.    0.    0.    0.    0.998 0.    0.   ]\n",
      " [0.    0.007 0.986 0.001 0.    0.    0.    0.005 0.001 0.   ]\n",
      " [0.002 0.    0.    0.    0.001 0.001 0.996 0.    0.    0.   ]\n",
      " [0.    0.997 0.    0.    0.    0.    0.    0.001 0.001 0.   ]\n",
      " [0.    0.026 0.97  0.001 0.    0.    0.    0.002 0.    0.   ]]\n",
      "------------------------------\n",
      "\n",
      "Clases Prediccion: \n",
      "[5 9 9 7 5 7 2 6 1 2]\n",
      "\n",
      "Clases Orginales: \n",
      "[5 9 9 7 5 7 2 6 1 2]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predecir - Aplicar Modelo\n",
    "\n",
    "valor = model.score(X_test,y_test)\n",
    "print('\\nR^2 de modelo:' ,np.round(valor*100.0,2),'%')\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10 \n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "predichos = model.predict_proba(X_test[indices])\n",
    "predichos = np.round(predichos,3)\n",
    "\n",
    "print('\\nProbabilidades de prediccion: ')\n",
    "print(predichos)\n",
    "print('------------------------------')\n",
    "\n",
    "print('\\nClases Prediccion: ')\n",
    "print(model.predict(X_test[indices]))\n",
    "print('\\nClases Orginales: ')\n",
    "print(y_test[indices])\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Clasificador Pipeline + GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T16:43:21.888771Z",
     "start_time": "2021-02-15T16:24:12.338030Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dim X train:  (60000, 784)\n",
      "Dim Y train:  (10000, 784)\n",
      "\n",
      "\n",
      "Iteration 1, loss = 1.95338825\n",
      "Validation score: 0.697167\n",
      "Iteration 2, loss = 0.55263943\n",
      "Validation score: 0.896167\n",
      "Iteration 3, loss = 0.31018153\n",
      "Validation score: 0.936083\n",
      "Iteration 4, loss = 0.21675932\n",
      "Validation score: 0.951083\n",
      "Iteration 5, loss = 0.17170946\n",
      "Validation score: 0.957000\n",
      "Iteration 6, loss = 0.14925512\n",
      "Validation score: 0.962167\n",
      "Iteration 7, loss = 0.13134458\n",
      "Validation score: 0.965000\n",
      "Iteration 8, loss = 0.12018006\n",
      "Validation score: 0.968500\n",
      "Iteration 9, loss = 0.11053619\n",
      "Validation score: 0.966667\n",
      "Iteration 10, loss = 0.10304859\n",
      "Validation score: 0.970250\n",
      "Iteration 11, loss = 0.09625277\n",
      "Validation score: 0.970833\n",
      "Iteration 12, loss = 0.09172330\n",
      "Validation score: 0.968833\n",
      "Iteration 13, loss = 0.08673835\n",
      "Validation score: 0.972500\n",
      "Iteration 14, loss = 0.08299514\n",
      "Validation score: 0.971083\n",
      "Iteration 15, loss = 0.07985878\n",
      "Validation score: 0.972917\n",
      "Iteration 16, loss = 0.07727703\n",
      "Validation score: 0.973500\n",
      "Iteration 17, loss = 0.07488058\n",
      "Validation score: 0.973167\n",
      "Iteration 18, loss = 0.07248131\n",
      "Validation score: 0.973917\n",
      "Iteration 19, loss = 0.07091459\n",
      "Validation score: 0.974167\n",
      "Iteration 20, loss = 0.06838453\n",
      "Validation score: 0.973333\n",
      "Iteration 21, loss = 0.06749534\n",
      "Validation score: 0.974250\n",
      "Iteration 22, loss = 0.06547080\n",
      "Validation score: 0.974333\n",
      "Iteration 23, loss = 0.06443709\n",
      "Validation score: 0.975000\n",
      "Iteration 24, loss = 0.06477067\n",
      "Validation score: 0.973000\n",
      "Iteration 25, loss = 0.06257473\n",
      "Validation score: 0.975667\n",
      "Iteration 26, loss = 0.06205194\n",
      "Validation score: 0.975000\n",
      "Iteration 27, loss = 0.06145645\n",
      "Validation score: 0.974083\n",
      "Iteration 28, loss = 0.06181495\n",
      "Validation score: 0.974417\n",
      "Iteration 29, loss = 0.06009740\n",
      "Validation score: 0.974667\n",
      "Iteration 30, loss = 0.05909802\n",
      "Validation score: 0.975333\n",
      "Iteration 31, loss = 0.05827496\n",
      "Validation score: 0.976500\n",
      "Iteration 32, loss = 0.05921801\n",
      "Validation score: 0.977000\n",
      "Iteration 33, loss = 0.05750281\n",
      "Validation score: 0.975333\n",
      "Iteration 34, loss = 0.05711946\n",
      "Validation score: 0.974667\n",
      "Iteration 35, loss = 0.05669570\n",
      "Validation score: 0.975833\n",
      "Iteration 36, loss = 0.05628530\n",
      "Validation score: 0.976417\n",
      "Iteration 37, loss = 0.05614998\n",
      "Validation score: 0.974083\n",
      "Iteration 38, loss = 0.05551642\n",
      "Validation score: 0.976833\n",
      "Iteration 39, loss = 0.05543641\n",
      "Validation score: 0.977083\n",
      "Iteration 40, loss = 0.05564000\n",
      "Validation score: 0.976333\n",
      "Iteration 41, loss = 0.05572528\n",
      "Validation score: 0.976833\n",
      "Iteration 42, loss = 0.05455491\n",
      "Validation score: 0.976000\n",
      "Iteration 43, loss = 0.05497940\n",
      "Validation score: 0.975500\n",
      "Iteration 44, loss = 0.05500627\n",
      "Validation score: 0.975250\n",
      "Iteration 45, loss = 0.05446806\n",
      "Validation score: 0.976667\n",
      "Iteration 46, loss = 0.05456714\n",
      "Validation score: 0.976000\n",
      "Iteration 47, loss = 0.05402014\n",
      "Validation score: 0.976000\n",
      "Iteration 48, loss = 0.05436149\n",
      "Validation score: 0.976500\n",
      "Iteration 49, loss = 0.05395383\n",
      "Validation score: 0.974750\n",
      "Iteration 50, loss = 0.05426355\n",
      "Validation score: 0.977333\n",
      "Iteration 51, loss = 0.05409154\n",
      "Validation score: 0.976417\n",
      "Iteration 52, loss = 0.05410120\n",
      "Validation score: 0.976500\n",
      "Iteration 53, loss = 0.05394337\n",
      "Validation score: 0.976250\n",
      "Iteration 54, loss = 0.05341967\n",
      "Validation score: 0.974750\n",
      "Iteration 55, loss = 0.05288390\n",
      "Validation score: 0.976500\n",
      "Iteration 56, loss = 0.05291844\n",
      "Validation score: 0.975500\n",
      "Iteration 57, loss = 0.05343070\n",
      "Validation score: 0.976833\n",
      "Iteration 58, loss = 0.05318881\n",
      "Validation score: 0.975000\n",
      "Iteration 59, loss = 0.05360531\n",
      "Validation score: 0.975167\n",
      "Iteration 60, loss = 0.05390141\n",
      "Validation score: 0.976917\n",
      "Iteration 61, loss = 0.05322956\n",
      "Validation score: 0.975833\n",
      "Iteration 62, loss = 0.05241218\n",
      "Validation score: 0.975583\n",
      "Iteration 63, loss = 0.05349483\n",
      "Validation score: 0.977500\n",
      "Iteration 64, loss = 0.05269695\n",
      "Validation score: 0.977667\n",
      "Iteration 65, loss = 0.05229606\n",
      "Validation score: 0.976667\n",
      "Iteration 66, loss = 0.05184094\n",
      "Validation score: 0.977250\n",
      "Iteration 67, loss = 0.05245483\n",
      "Validation score: 0.975833\n",
      "Iteration 68, loss = 0.05186199\n",
      "Validation score: 0.974583\n",
      "Iteration 69, loss = 0.05202350\n",
      "Validation score: 0.976917\n",
      "Iteration 70, loss = 0.05221709\n",
      "Validation score: 0.977417\n",
      "Iteration 71, loss = 0.05248859\n",
      "Validation score: 0.976000\n",
      "Iteration 72, loss = 0.05215814\n",
      "Validation score: 0.977000\n",
      "Iteration 73, loss = 0.05213121\n",
      "Validation score: 0.975500\n",
      "Iteration 74, loss = 0.05219319\n",
      "Validation score: 0.977250\n",
      "Iteration 75, loss = 0.05155237\n",
      "Validation score: 0.976750\n",
      "Iteration 76, loss = 0.05166935\n",
      "Validation score: 0.977000\n",
      "Iteration 77, loss = 0.05148904\n",
      "Validation score: 0.977333\n",
      "Iteration 78, loss = 0.05089268\n",
      "Validation score: 0.977167\n",
      "Iteration 79, loss = 0.05162404\n",
      "Validation score: 0.976583\n",
      "Iteration 80, loss = 0.05096479\n",
      "Validation score: 0.976667\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48257313\n",
      "Validation score: 0.920375\n",
      "Iteration 2, loss = 0.21369623\n",
      "Validation score: 0.938000\n",
      "Iteration 3, loss = 0.17094954\n",
      "Validation score: 0.944625\n",
      "Iteration 4, loss = 0.15260255\n",
      "Validation score: 0.941500\n",
      "Iteration 5, loss = 0.13837939\n",
      "Validation score: 0.949500\n",
      "Iteration 6, loss = 0.12140733\n",
      "Validation score: 0.949875\n",
      "Iteration 7, loss = 0.11392812\n",
      "Validation score: 0.950500\n",
      "Iteration 8, loss = 0.10314131\n",
      "Validation score: 0.954375\n",
      "Iteration 9, loss = 0.09783480\n",
      "Validation score: 0.950250\n",
      "Iteration 10, loss = 0.09023143\n",
      "Validation score: 0.954875\n",
      "Iteration 11, loss = 0.08729101\n",
      "Validation score: 0.952500\n",
      "Iteration 12, loss = 0.07692294\n",
      "Validation score: 0.954875\n",
      "Iteration 13, loss = 0.07636765\n",
      "Validation score: 0.952875\n",
      "Iteration 14, loss = 0.07068394\n",
      "Validation score: 0.953875\n",
      "Iteration 15, loss = 0.07518126\n",
      "Validation score: 0.952500\n",
      "Iteration 16, loss = 0.07859713\n",
      "Validation score: 0.951000\n",
      "Iteration 17, loss = 0.06490677\n",
      "Validation score: 0.956000\n",
      "Iteration 18, loss = 0.06862665\n",
      "Validation score: 0.953625\n",
      "Iteration 19, loss = 0.06118492\n",
      "Validation score: 0.956750\n",
      "Iteration 20, loss = 0.05823799\n",
      "Validation score: 0.956375\n",
      "Iteration 21, loss = 0.05806161\n",
      "Validation score: 0.956125\n",
      "Iteration 22, loss = 0.06229706\n",
      "Validation score: 0.952250\n",
      "Iteration 23, loss = 0.05521001\n",
      "Validation score: 0.955000\n",
      "Iteration 24, loss = 0.05621109\n",
      "Validation score: 0.953750\n",
      "Iteration 25, loss = 0.05856893\n",
      "Validation score: 0.954750\n",
      "Iteration 26, loss = 0.05334028\n",
      "Validation score: 0.952000\n",
      "Iteration 27, loss = 0.05032243\n",
      "Validation score: 0.952500\n",
      "Iteration 28, loss = 0.04900649\n",
      "Validation score: 0.954125\n",
      "Iteration 29, loss = 0.04607851\n",
      "Validation score: 0.956125\n",
      "Iteration 30, loss = 0.05005080\n",
      "Validation score: 0.954125\n",
      "Iteration 31, loss = 0.04161885\n",
      "Validation score: 0.950250\n",
      "Iteration 32, loss = 0.04074379\n",
      "Validation score: 0.954750\n",
      "Iteration 33, loss = 0.04009097\n",
      "Validation score: 0.956000\n",
      "Iteration 34, loss = 0.04490648\n",
      "Validation score: 0.956625\n",
      "Iteration 35, loss = 0.03496442\n",
      "Validation score: 0.957125\n",
      "Iteration 36, loss = 0.03994835\n",
      "Validation score: 0.954625\n",
      "Iteration 37, loss = 0.05826680\n",
      "Validation score: 0.951750\n",
      "Iteration 38, loss = 0.05129638\n",
      "Validation score: 0.954750\n",
      "Iteration 39, loss = 0.04053229\n",
      "Validation score: 0.956875\n",
      "Iteration 40, loss = 0.03402926\n",
      "Validation score: 0.952500\n",
      "Iteration 41, loss = 0.03592351\n",
      "Validation score: 0.958500\n",
      "Iteration 42, loss = 0.03218195\n",
      "Validation score: 0.956250\n",
      "Iteration 43, loss = 0.03039320\n",
      "Validation score: 0.954250\n",
      "Iteration 44, loss = 0.03912711\n",
      "Validation score: 0.956375\n",
      "Iteration 45, loss = 0.03903761\n",
      "Validation score: 0.951375\n",
      "Iteration 46, loss = 0.04460861\n",
      "Validation score: 0.952625\n",
      "Iteration 47, loss = 0.03796911\n",
      "Validation score: 0.956750\n",
      "Iteration 48, loss = 0.03818003\n",
      "Validation score: 0.955875\n",
      "Iteration 49, loss = 0.03771612\n",
      "Validation score: 0.957000\n",
      "Iteration 50, loss = 0.03303043\n",
      "Validation score: 0.957500\n",
      "Iteration 51, loss = 0.03097436\n",
      "Validation score: 0.956375\n",
      "Iteration 52, loss = 0.02606341\n",
      "Validation score: 0.954750\n",
      "Iteration 53, loss = 0.02780008\n",
      "Validation score: 0.955000\n",
      "Iteration 54, loss = 0.04367322\n",
      "Validation score: 0.954750\n",
      "Iteration 55, loss = 0.03999625\n",
      "Validation score: 0.954875\n",
      "Iteration 56, loss = 0.05061466\n",
      "Validation score: 0.955750\n",
      "Iteration 57, loss = 0.03976433\n",
      "Validation score: 0.951000\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52088670\n",
      "Validation score: 0.925875\n",
      "Iteration 2, loss = 0.20291090\n",
      "Validation score: 0.943375\n",
      "Iteration 3, loss = 0.15995821\n",
      "Validation score: 0.948000\n",
      "Iteration 4, loss = 0.13921939\n",
      "Validation score: 0.947875\n",
      "Iteration 5, loss = 0.11962682\n",
      "Validation score: 0.952125\n",
      "Iteration 6, loss = 0.10557427\n",
      "Validation score: 0.956875\n",
      "Iteration 7, loss = 0.09609264\n",
      "Validation score: 0.955125\n",
      "Iteration 8, loss = 0.09699019\n",
      "Validation score: 0.954625\n",
      "Iteration 9, loss = 0.08149733\n",
      "Validation score: 0.953625\n",
      "Iteration 10, loss = 0.07788074\n",
      "Validation score: 0.954375\n",
      "Iteration 11, loss = 0.07985500\n",
      "Validation score: 0.955375\n",
      "Iteration 12, loss = 0.07509985\n",
      "Validation score: 0.954750\n",
      "Iteration 13, loss = 0.07861059\n",
      "Validation score: 0.954500\n",
      "Iteration 14, loss = 0.06570192\n",
      "Validation score: 0.956875\n",
      "Iteration 15, loss = 0.06239398\n",
      "Validation score: 0.955750\n",
      "Iteration 16, loss = 0.06093586\n",
      "Validation score: 0.957500\n",
      "Iteration 17, loss = 0.05933364\n",
      "Validation score: 0.956625\n",
      "Iteration 18, loss = 0.06353610\n",
      "Validation score: 0.956000\n",
      "Iteration 19, loss = 0.06322798\n",
      "Validation score: 0.957625\n",
      "Iteration 20, loss = 0.05934289\n",
      "Validation score: 0.954375\n",
      "Iteration 21, loss = 0.05754252\n",
      "Validation score: 0.958500\n",
      "Iteration 22, loss = 0.05434589\n",
      "Validation score: 0.953875\n",
      "Iteration 23, loss = 0.05934957\n",
      "Validation score: 0.956125\n",
      "Iteration 24, loss = 0.05533573\n",
      "Validation score: 0.957000\n",
      "Iteration 25, loss = 0.04429950\n",
      "Validation score: 0.959125\n",
      "Iteration 26, loss = 0.04182555\n",
      "Validation score: 0.957875\n",
      "Iteration 27, loss = 0.04286201\n",
      "Validation score: 0.956750\n",
      "Iteration 28, loss = 0.03988634\n",
      "Validation score: 0.958875\n",
      "Iteration 29, loss = 0.04294978\n",
      "Validation score: 0.956250\n",
      "Iteration 30, loss = 0.04206396\n",
      "Validation score: 0.956375\n",
      "Iteration 31, loss = 0.05254176\n",
      "Validation score: 0.958750\n",
      "Iteration 32, loss = 0.04046793\n",
      "Validation score: 0.950375\n",
      "Iteration 33, loss = 0.04802418\n",
      "Validation score: 0.954125\n",
      "Iteration 34, loss = 0.04798261\n",
      "Validation score: 0.955375\n",
      "Iteration 35, loss = 0.04545772\n",
      "Validation score: 0.958750\n",
      "Iteration 36, loss = 0.03798533\n",
      "Validation score: 0.959000\n",
      "Iteration 37, loss = 0.03655855\n",
      "Validation score: 0.956125\n",
      "Iteration 38, loss = 0.03832583\n",
      "Validation score: 0.953000\n",
      "Iteration 39, loss = 0.04169219\n",
      "Validation score: 0.955875\n",
      "Iteration 40, loss = 0.03520343\n",
      "Validation score: 0.953750\n",
      "Iteration 41, loss = 0.03471142\n",
      "Validation score: 0.958500\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53477635\n",
      "Validation score: 0.927000\n",
      "Iteration 2, loss = 0.21028209\n",
      "Validation score: 0.941500\n",
      "Iteration 3, loss = 0.16412099\n",
      "Validation score: 0.943250\n",
      "Iteration 4, loss = 0.13940263\n",
      "Validation score: 0.946875\n",
      "Iteration 5, loss = 0.11532009\n",
      "Validation score: 0.948500\n",
      "Iteration 6, loss = 0.11156323\n",
      "Validation score: 0.955000\n",
      "Iteration 7, loss = 0.10353813\n",
      "Validation score: 0.947375\n",
      "Iteration 8, loss = 0.09542078\n",
      "Validation score: 0.949750\n",
      "Iteration 9, loss = 0.09177408\n",
      "Validation score: 0.957625\n",
      "Iteration 10, loss = 0.07592104\n",
      "Validation score: 0.952500\n",
      "Iteration 11, loss = 0.07855559\n",
      "Validation score: 0.952375\n",
      "Iteration 12, loss = 0.07920022\n",
      "Validation score: 0.955125\n",
      "Iteration 13, loss = 0.07062253\n",
      "Validation score: 0.956000\n",
      "Iteration 14, loss = 0.06660184\n",
      "Validation score: 0.952875\n",
      "Iteration 15, loss = 0.05995318\n",
      "Validation score: 0.950375\n",
      "Iteration 16, loss = 0.06089740\n",
      "Validation score: 0.953875\n",
      "Iteration 17, loss = 0.06078387\n",
      "Validation score: 0.955125\n",
      "Iteration 18, loss = 0.06009859\n",
      "Validation score: 0.953000\n",
      "Iteration 19, loss = 0.05448770\n",
      "Validation score: 0.947750\n",
      "Iteration 20, loss = 0.05975556\n",
      "Validation score: 0.954750\n",
      "Iteration 21, loss = 0.05223307\n",
      "Validation score: 0.953375\n",
      "Iteration 22, loss = 0.04619542\n",
      "Validation score: 0.952625\n",
      "Iteration 23, loss = 0.05746754\n",
      "Validation score: 0.950625\n",
      "Iteration 24, loss = 0.05012110\n",
      "Validation score: 0.952875\n",
      "Iteration 25, loss = 0.04403211\n",
      "Validation score: 0.951375\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35061611\n",
      "Validation score: 0.943375\n",
      "Iteration 2, loss = 0.15931246\n",
      "Validation score: 0.958250\n",
      "Iteration 3, loss = 0.11761681\n",
      "Validation score: 0.953250\n",
      "Iteration 4, loss = 0.09049937\n",
      "Validation score: 0.961250\n",
      "Iteration 5, loss = 0.07436201\n",
      "Validation score: 0.959375\n",
      "Iteration 6, loss = 0.06218278\n",
      "Validation score: 0.960875\n",
      "Iteration 7, loss = 0.05491900\n",
      "Validation score: 0.963250\n",
      "Iteration 8, loss = 0.04305301\n",
      "Validation score: 0.966125\n",
      "Iteration 9, loss = 0.03639226\n",
      "Validation score: 0.964500\n",
      "Iteration 10, loss = 0.03252817\n",
      "Validation score: 0.968625\n",
      "Iteration 11, loss = 0.02261632\n",
      "Validation score: 0.964375\n",
      "Iteration 12, loss = 0.02251571\n",
      "Validation score: 0.965250\n",
      "Iteration 13, loss = 0.02304119\n",
      "Validation score: 0.959750\n",
      "Iteration 14, loss = 0.01968391\n",
      "Validation score: 0.962000\n",
      "Iteration 15, loss = 0.01792300\n",
      "Validation score: 0.967375\n",
      "Iteration 16, loss = 0.01661790\n",
      "Validation score: 0.968375\n",
      "Iteration 17, loss = 0.01332725\n",
      "Validation score: 0.966875\n",
      "Iteration 18, loss = 0.01207704\n",
      "Validation score: 0.966500\n",
      "Iteration 19, loss = 0.01102943\n",
      "Validation score: 0.966875\n",
      "Iteration 20, loss = 0.01636250\n",
      "Validation score: 0.964125\n",
      "Iteration 21, loss = 0.01900863\n",
      "Validation score: 0.967625\n",
      "Iteration 22, loss = 0.01728832\n",
      "Validation score: 0.966375\n",
      "Iteration 23, loss = 0.01060399\n",
      "Validation score: 0.966000\n",
      "Iteration 24, loss = 0.00644793\n",
      "Validation score: 0.968500\n",
      "Iteration 25, loss = 0.00348492\n",
      "Validation score: 0.969750\n",
      "Iteration 26, loss = 0.00218233\n",
      "Validation score: 0.970000\n",
      "Iteration 27, loss = 0.00183288\n",
      "Validation score: 0.970125\n",
      "Iteration 28, loss = 0.00168045\n",
      "Validation score: 0.970500\n",
      "Iteration 29, loss = 0.00157274\n",
      "Validation score: 0.970375\n",
      "Iteration 30, loss = 0.00149110\n",
      "Validation score: 0.970125\n",
      "Iteration 31, loss = 0.00142647\n",
      "Validation score: 0.970250\n",
      "Iteration 32, loss = 0.00136840\n",
      "Validation score: 0.970125\n",
      "Iteration 33, loss = 0.00131740\n",
      "Validation score: 0.970000\n",
      "Iteration 34, loss = 0.00127012\n",
      "Validation score: 0.969875\n",
      "Iteration 35, loss = 0.00122294\n",
      "Validation score: 0.970125\n",
      "Iteration 36, loss = 0.00118385\n",
      "Validation score: 0.970000\n",
      "Iteration 37, loss = 0.00114966\n",
      "Validation score: 0.970125\n",
      "Iteration 38, loss = 0.00101430\n",
      "Validation score: 0.970125\n",
      "Iteration 39, loss = 0.00096211\n",
      "Validation score: 0.970000\n",
      "Iteration 40, loss = 0.00092850\n",
      "Validation score: 0.970125\n",
      "Iteration 41, loss = 0.00090465\n",
      "Validation score: 0.970250\n",
      "Iteration 42, loss = 0.00087838\n",
      "Validation score: 0.969750\n",
      "Iteration 43, loss = 0.00086037\n",
      "Validation score: 0.970625\n",
      "Iteration 44, loss = 0.00083876\n",
      "Validation score: 0.970125\n",
      "Iteration 45, loss = 0.00082311\n",
      "Validation score: 0.970750\n",
      "Iteration 46, loss = 0.00080555\n",
      "Validation score: 0.970500\n",
      "Iteration 47, loss = 0.00079047\n",
      "Validation score: 0.970625\n",
      "Iteration 48, loss = 0.00077780\n",
      "Validation score: 0.970625\n",
      "Iteration 49, loss = 0.00076469\n",
      "Validation score: 0.970875\n",
      "Iteration 50, loss = 0.00075104\n",
      "Validation score: 0.970875\n",
      "Iteration 51, loss = 0.00073944\n",
      "Validation score: 0.970750\n",
      "Iteration 52, loss = 0.00072850\n",
      "Validation score: 0.970375\n",
      "Iteration 53, loss = 0.00071824\n",
      "Validation score: 0.970375\n",
      "Iteration 54, loss = 0.00070862\n",
      "Validation score: 0.971000\n",
      "Iteration 55, loss = 0.00070015\n",
      "Validation score: 0.970625\n",
      "Iteration 56, loss = 0.00069163\n",
      "Validation score: 0.970250\n",
      "Iteration 57, loss = 0.00068406\n",
      "Validation score: 0.971000\n",
      "Iteration 58, loss = 0.00067735\n",
      "Validation score: 0.971000\n",
      "Iteration 59, loss = 0.00067025\n",
      "Validation score: 0.971125\n",
      "Iteration 60, loss = 0.00066333\n",
      "Validation score: 0.970875\n",
      "Iteration 61, loss = 0.00065712\n",
      "Validation score: 0.970750\n",
      "Iteration 62, loss = 0.00065070\n",
      "Validation score: 0.971125\n",
      "Iteration 63, loss = 0.00064526\n",
      "Validation score: 0.970750\n",
      "Iteration 64, loss = 0.00063981\n",
      "Validation score: 0.970875\n",
      "Iteration 65, loss = 0.00063469\n",
      "Validation score: 0.970750\n",
      "Iteration 66, loss = 0.00062939\n",
      "Validation score: 0.971000\n",
      "Iteration 67, loss = 0.00062497\n",
      "Validation score: 0.971000\n",
      "Iteration 68, loss = 0.00062029\n",
      "Validation score: 0.970875\n",
      "Iteration 69, loss = 0.00061563\n",
      "Validation score: 0.971000\n",
      "Iteration 70, loss = 0.00061147\n",
      "Validation score: 0.971000\n",
      "Iteration 71, loss = 0.00060751\n",
      "Validation score: 0.971125\n",
      "Iteration 72, loss = 0.00060338\n",
      "Validation score: 0.970875\n",
      "Iteration 73, loss = 0.00059969\n",
      "Validation score: 0.970875\n",
      "Iteration 74, loss = 0.00059610\n",
      "Validation score: 0.970875\n",
      "Iteration 75, loss = 0.00059271\n",
      "Validation score: 0.970875\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34421122\n",
      "Validation score: 0.944750\n",
      "Iteration 2, loss = 0.15688669\n",
      "Validation score: 0.955500\n",
      "Iteration 3, loss = 0.11315436\n",
      "Validation score: 0.958625\n",
      "Iteration 4, loss = 0.08566782\n",
      "Validation score: 0.964750\n",
      "Iteration 5, loss = 0.06760027\n",
      "Validation score: 0.957375\n",
      "Iteration 6, loss = 0.05778149\n",
      "Validation score: 0.963375\n",
      "Iteration 7, loss = 0.04715553\n",
      "Validation score: 0.964750\n",
      "Iteration 8, loss = 0.04096999\n",
      "Validation score: 0.969625\n",
      "Iteration 9, loss = 0.03251083\n",
      "Validation score: 0.960625\n",
      "Iteration 10, loss = 0.03271586\n",
      "Validation score: 0.967875\n",
      "Iteration 11, loss = 0.02564600\n",
      "Validation score: 0.970125\n",
      "Iteration 12, loss = 0.02037667\n",
      "Validation score: 0.969000\n",
      "Iteration 13, loss = 0.01466643\n",
      "Validation score: 0.969750\n",
      "Iteration 14, loss = 0.01507551\n",
      "Validation score: 0.970875\n",
      "Iteration 15, loss = 0.01226433\n",
      "Validation score: 0.969125\n",
      "Iteration 16, loss = 0.00804324\n",
      "Validation score: 0.970875\n",
      "Iteration 17, loss = 0.00440411\n",
      "Validation score: 0.971000\n",
      "Iteration 18, loss = 0.00316229\n",
      "Validation score: 0.972375\n",
      "Iteration 19, loss = 0.00261323\n",
      "Validation score: 0.972750\n",
      "Iteration 20, loss = 0.00231296\n",
      "Validation score: 0.972125\n",
      "Iteration 21, loss = 0.00216497\n",
      "Validation score: 0.973000\n",
      "Iteration 22, loss = 0.00198695\n",
      "Validation score: 0.973000\n",
      "Iteration 23, loss = 0.00186526\n",
      "Validation score: 0.972625\n",
      "Iteration 24, loss = 0.00175917\n",
      "Validation score: 0.973125\n",
      "Iteration 25, loss = 0.00167725\n",
      "Validation score: 0.972875\n",
      "Iteration 26, loss = 0.00159787\n",
      "Validation score: 0.972750\n",
      "Iteration 27, loss = 0.00153203\n",
      "Validation score: 0.972875\n",
      "Iteration 28, loss = 0.00146695\n",
      "Validation score: 0.972125\n",
      "Iteration 29, loss = 0.00141510\n",
      "Validation score: 0.972875\n",
      "Iteration 30, loss = 0.00134618\n",
      "Validation score: 0.973125\n",
      "Iteration 31, loss = 0.00130449\n",
      "Validation score: 0.972500\n",
      "Iteration 32, loss = 0.00125729\n",
      "Validation score: 0.972875\n",
      "Iteration 33, loss = 0.00122079\n",
      "Validation score: 0.972000\n",
      "Iteration 34, loss = 0.00118351\n",
      "Validation score: 0.972625\n",
      "Iteration 35, loss = 0.00114771\n",
      "Validation score: 0.972125\n",
      "Iteration 36, loss = 0.00111125\n",
      "Validation score: 0.972625\n",
      "Iteration 37, loss = 0.00107701\n",
      "Validation score: 0.972500\n",
      "Iteration 38, loss = 0.00103575\n",
      "Validation score: 0.971875\n",
      "Iteration 39, loss = 0.00100103\n",
      "Validation score: 0.972250\n",
      "Iteration 40, loss = 0.00099228\n",
      "Validation score: 0.972250\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35919153\n",
      "Validation score: 0.942250\n",
      "Iteration 2, loss = 0.15690922\n",
      "Validation score: 0.948125\n",
      "Iteration 3, loss = 0.11607139\n",
      "Validation score: 0.960125\n",
      "Iteration 4, loss = 0.09196725\n",
      "Validation score: 0.962000\n",
      "Iteration 5, loss = 0.07349662\n",
      "Validation score: 0.958250\n",
      "Iteration 6, loss = 0.06112055\n",
      "Validation score: 0.959625\n",
      "Iteration 7, loss = 0.04909647\n",
      "Validation score: 0.965625\n",
      "Iteration 8, loss = 0.04032805\n",
      "Validation score: 0.963750\n",
      "Iteration 9, loss = 0.03095867\n",
      "Validation score: 0.969625\n",
      "Iteration 10, loss = 0.02800544\n",
      "Validation score: 0.970500\n",
      "Iteration 11, loss = 0.02143628\n",
      "Validation score: 0.965750\n",
      "Iteration 12, loss = 0.01856477\n",
      "Validation score: 0.964750\n",
      "Iteration 13, loss = 0.02276747\n",
      "Validation score: 0.968000\n",
      "Iteration 14, loss = 0.01749307\n",
      "Validation score: 0.970375\n",
      "Iteration 15, loss = 0.01314170\n",
      "Validation score: 0.968375\n",
      "Iteration 16, loss = 0.01390715\n",
      "Validation score: 0.967750\n",
      "Iteration 17, loss = 0.00964119\n",
      "Validation score: 0.972375\n",
      "Iteration 18, loss = 0.00533926\n",
      "Validation score: 0.972875\n",
      "Iteration 19, loss = 0.00298850\n",
      "Validation score: 0.972875\n",
      "Iteration 20, loss = 0.00223563\n",
      "Validation score: 0.973000\n",
      "Iteration 21, loss = 0.00188208\n",
      "Validation score: 0.973000\n",
      "Iteration 22, loss = 0.00171203\n",
      "Validation score: 0.973125\n",
      "Iteration 23, loss = 0.00158540\n",
      "Validation score: 0.972750\n",
      "Iteration 24, loss = 0.00148334\n",
      "Validation score: 0.973000\n",
      "Iteration 25, loss = 0.00139974\n",
      "Validation score: 0.973000\n",
      "Iteration 26, loss = 0.00132002\n",
      "Validation score: 0.973000\n",
      "Iteration 27, loss = 0.00126014\n",
      "Validation score: 0.972750\n",
      "Iteration 28, loss = 0.00120127\n",
      "Validation score: 0.972750\n",
      "Iteration 29, loss = 0.00114925\n",
      "Validation score: 0.973125\n",
      "Iteration 30, loss = 0.00110747\n",
      "Validation score: 0.973625\n",
      "Iteration 31, loss = 0.00106696\n",
      "Validation score: 0.973125\n",
      "Iteration 32, loss = 0.00102940\n",
      "Validation score: 0.973000\n",
      "Iteration 33, loss = 0.00099699\n",
      "Validation score: 0.973000\n",
      "Iteration 34, loss = 0.00096693\n",
      "Validation score: 0.972750\n",
      "Iteration 35, loss = 0.00093852\n",
      "Validation score: 0.972750\n",
      "Iteration 36, loss = 0.00091397\n",
      "Validation score: 0.972750\n",
      "Iteration 37, loss = 0.00088960\n",
      "Validation score: 0.972625\n",
      "Iteration 38, loss = 0.00086775\n",
      "Validation score: 0.972125\n",
      "Iteration 39, loss = 0.00084951\n",
      "Validation score: 0.972625\n",
      "Iteration 40, loss = 0.00083158\n",
      "Validation score: 0.972500\n",
      "Iteration 41, loss = 0.00081471\n",
      "Validation score: 0.972500\n",
      "Iteration 42, loss = 0.00079831\n",
      "Validation score: 0.972250\n",
      "Iteration 43, loss = 0.00078420\n",
      "Validation score: 0.972375\n",
      "Iteration 44, loss = 0.00077063\n",
      "Validation score: 0.972125\n",
      "Iteration 45, loss = 0.00075773\n",
      "Validation score: 0.972375\n",
      "Iteration 46, loss = 0.00074497\n",
      "Validation score: 0.971875\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46870554\n",
      "Validation score: 0.920250\n",
      "Iteration 2, loss = 0.24910498\n",
      "Validation score: 0.927625\n",
      "Iteration 3, loss = 0.21467991\n",
      "Validation score: 0.932625\n",
      "Iteration 4, loss = 0.20201342\n",
      "Validation score: 0.933750\n",
      "Iteration 5, loss = 0.18195843\n",
      "Validation score: 0.929625\n",
      "Iteration 6, loss = 0.17237455\n",
      "Validation score: 0.932500\n",
      "Iteration 7, loss = 0.16782316\n",
      "Validation score: 0.936750\n",
      "Iteration 8, loss = 0.16164400\n",
      "Validation score: 0.935875\n",
      "Iteration 9, loss = 0.14877837\n",
      "Validation score: 0.939375\n",
      "Iteration 10, loss = 0.15220721\n",
      "Validation score: 0.934250\n",
      "Iteration 11, loss = 0.14163762\n",
      "Validation score: 0.934625\n",
      "Iteration 12, loss = 0.13782725\n",
      "Validation score: 0.936750\n",
      "Iteration 13, loss = 0.14049309\n",
      "Validation score: 0.933375\n",
      "Iteration 14, loss = 0.13297326\n",
      "Validation score: 0.934750\n",
      "Iteration 15, loss = 0.12902309\n",
      "Validation score: 0.934000\n",
      "Iteration 16, loss = 0.12747028\n",
      "Validation score: 0.936500\n",
      "Iteration 17, loss = 0.11453181\n",
      "Validation score: 0.936875\n",
      "Iteration 18, loss = 0.11382179\n",
      "Validation score: 0.937500\n",
      "Iteration 19, loss = 0.10935438\n",
      "Validation score: 0.937375\n",
      "Iteration 20, loss = 0.11082261\n",
      "Validation score: 0.938750\n",
      "Iteration 21, loss = 0.11553490\n",
      "Validation score: 0.934625\n",
      "Iteration 22, loss = 0.10551101\n",
      "Validation score: 0.928500\n",
      "Iteration 23, loss = 0.11601713\n",
      "Validation score: 0.931375\n",
      "Iteration 24, loss = 0.11202210\n",
      "Validation score: 0.935625\n",
      "Iteration 25, loss = 0.10391501\n",
      "Validation score: 0.932500\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46484397\n",
      "Validation score: 0.912000\n",
      "Iteration 2, loss = 0.23632463\n",
      "Validation score: 0.921750\n",
      "Iteration 3, loss = 0.20840561\n",
      "Validation score: 0.928625\n",
      "Iteration 4, loss = 0.19064205\n",
      "Validation score: 0.933500\n",
      "Iteration 5, loss = 0.17678243\n",
      "Validation score: 0.930875\n",
      "Iteration 6, loss = 0.16696505\n",
      "Validation score: 0.939375\n",
      "Iteration 7, loss = 0.15519775\n",
      "Validation score: 0.936000\n",
      "Iteration 8, loss = 0.15110802\n",
      "Validation score: 0.936375\n",
      "Iteration 9, loss = 0.14335719\n",
      "Validation score: 0.939500\n",
      "Iteration 10, loss = 0.13639662\n",
      "Validation score: 0.938250\n",
      "Iteration 11, loss = 0.13146908\n",
      "Validation score: 0.943125\n",
      "Iteration 12, loss = 0.12452417\n",
      "Validation score: 0.940125\n",
      "Iteration 13, loss = 0.12537108\n",
      "Validation score: 0.939375\n",
      "Iteration 14, loss = 0.12450470\n",
      "Validation score: 0.939500\n",
      "Iteration 15, loss = 0.11461060\n",
      "Validation score: 0.941125\n",
      "Iteration 16, loss = 0.11398269\n",
      "Validation score: 0.930500\n",
      "Iteration 17, loss = 0.10767705\n",
      "Validation score: 0.941500\n",
      "Iteration 18, loss = 0.10567716\n",
      "Validation score: 0.938750\n",
      "Iteration 19, loss = 0.10535498\n",
      "Validation score: 0.932875\n",
      "Iteration 20, loss = 0.10798363\n",
      "Validation score: 0.931125\n",
      "Iteration 21, loss = 0.10400413\n",
      "Validation score: 0.932500\n",
      "Iteration 22, loss = 0.10143539\n",
      "Validation score: 0.936875\n",
      "Iteration 23, loss = 0.09688218\n",
      "Validation score: 0.939500\n",
      "Iteration 24, loss = 0.09177452\n",
      "Validation score: 0.936750\n",
      "Iteration 25, loss = 0.09359672\n",
      "Validation score: 0.933625\n",
      "Iteration 26, loss = 0.08988802\n",
      "Validation score: 0.939500\n",
      "Iteration 27, loss = 0.09041064\n",
      "Validation score: 0.937500\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47687032\n",
      "Validation score: 0.916375\n",
      "Iteration 2, loss = 0.25130262\n",
      "Validation score: 0.928375\n",
      "Iteration 3, loss = 0.20971888\n",
      "Validation score: 0.935125\n",
      "Iteration 4, loss = 0.19168050\n",
      "Validation score: 0.931375\n",
      "Iteration 5, loss = 0.18234821\n",
      "Validation score: 0.934000\n",
      "Iteration 6, loss = 0.16710265\n",
      "Validation score: 0.932625\n",
      "Iteration 7, loss = 0.16008683\n",
      "Validation score: 0.934875\n",
      "Iteration 8, loss = 0.15614385\n",
      "Validation score: 0.940125\n",
      "Iteration 9, loss = 0.15066159\n",
      "Validation score: 0.933625\n",
      "Iteration 10, loss = 0.14533876\n",
      "Validation score: 0.934750\n",
      "Iteration 11, loss = 0.14042352\n",
      "Validation score: 0.940000\n",
      "Iteration 12, loss = 0.13438272\n",
      "Validation score: 0.937125\n",
      "Iteration 13, loss = 0.12950514\n",
      "Validation score: 0.931750\n",
      "Iteration 14, loss = 0.12863562\n",
      "Validation score: 0.939125\n",
      "Iteration 15, loss = 0.12373179\n",
      "Validation score: 0.940000\n",
      "Iteration 16, loss = 0.11952640\n",
      "Validation score: 0.939125\n",
      "Iteration 17, loss = 0.11548484\n",
      "Validation score: 0.936375\n",
      "Iteration 18, loss = 0.11847569\n",
      "Validation score: 0.937125\n",
      "Iteration 19, loss = 0.11361587\n",
      "Validation score: 0.941625\n",
      "Iteration 20, loss = 0.11330940\n",
      "Validation score: 0.933125\n",
      "Iteration 21, loss = 0.11219395\n",
      "Validation score: 0.935750\n",
      "Iteration 22, loss = 0.11536207\n",
      "Validation score: 0.934875\n",
      "Iteration 23, loss = 0.10712540\n",
      "Validation score: 0.936000\n",
      "Iteration 24, loss = 0.10467828\n",
      "Validation score: 0.938375\n",
      "Iteration 25, loss = 0.10124807\n",
      "Validation score: 0.941250\n",
      "Iteration 26, loss = 0.10539202\n",
      "Validation score: 0.937375\n",
      "Iteration 27, loss = 0.09759206\n",
      "Validation score: 0.932750\n",
      "Iteration 28, loss = 0.09864992\n",
      "Validation score: 0.935625\n",
      "Iteration 29, loss = 0.09680374\n",
      "Validation score: 0.934875\n",
      "Iteration 30, loss = 0.10079947\n",
      "Validation score: 0.935375\n",
      "Iteration 31, loss = 0.09598010\n",
      "Validation score: 0.938875\n",
      "Iteration 32, loss = 0.09245145\n",
      "Validation score: 0.941000\n",
      "Iteration 33, loss = 0.08622251\n",
      "Validation score: 0.937625\n",
      "Iteration 34, loss = 0.08858534\n",
      "Validation score: 0.935875\n",
      "Iteration 35, loss = 0.09040326\n",
      "Validation score: 0.940500\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51834594\n",
      "Validation score: 0.921250\n",
      "Iteration 2, loss = 0.25536907\n",
      "Validation score: 0.944375\n",
      "Iteration 3, loss = 0.21680405\n",
      "Validation score: 0.946625\n",
      "Iteration 4, loss = 0.19922809\n",
      "Validation score: 0.943750\n",
      "Iteration 5, loss = 0.18957671\n",
      "Validation score: 0.954250\n",
      "Iteration 6, loss = 0.18127958\n",
      "Validation score: 0.953125\n",
      "Iteration 7, loss = 0.18023230\n",
      "Validation score: 0.954500\n",
      "Iteration 8, loss = 0.16510578\n",
      "Validation score: 0.953625\n",
      "Iteration 9, loss = 0.16785559\n",
      "Validation score: 0.955375\n",
      "Iteration 10, loss = 0.15645444\n",
      "Validation score: 0.952375\n",
      "Iteration 11, loss = 0.15990507\n",
      "Validation score: 0.949625\n",
      "Iteration 12, loss = 0.15568123\n",
      "Validation score: 0.960375\n",
      "Iteration 13, loss = 0.14981044\n",
      "Validation score: 0.954250\n",
      "Iteration 14, loss = 0.14657744\n",
      "Validation score: 0.957875\n",
      "Iteration 15, loss = 0.15164670\n",
      "Validation score: 0.948875\n",
      "Iteration 16, loss = 0.14811305\n",
      "Validation score: 0.960375\n",
      "Iteration 17, loss = 0.14913955\n",
      "Validation score: 0.948500\n",
      "Iteration 18, loss = 0.14189391\n",
      "Validation score: 0.961625\n",
      "Iteration 19, loss = 0.13731954\n",
      "Validation score: 0.959625\n",
      "Iteration 20, loss = 0.13758713\n",
      "Validation score: 0.959000\n",
      "Iteration 21, loss = 0.13210461\n",
      "Validation score: 0.961375\n",
      "Iteration 22, loss = 0.13329326\n",
      "Validation score: 0.958875\n",
      "Iteration 23, loss = 0.14434463\n",
      "Validation score: 0.961625\n",
      "Iteration 24, loss = 0.14391091\n",
      "Validation score: 0.955625\n",
      "Iteration 25, loss = 0.14508145\n",
      "Validation score: 0.960125\n",
      "Iteration 26, loss = 0.13346810\n",
      "Validation score: 0.960000\n",
      "Iteration 27, loss = 0.13236733\n",
      "Validation score: 0.958625\n",
      "Iteration 28, loss = 0.12977601\n",
      "Validation score: 0.958000\n",
      "Iteration 29, loss = 0.13848973\n",
      "Validation score: 0.959875\n",
      "Iteration 30, loss = 0.13744700\n",
      "Validation score: 0.956500\n",
      "Iteration 31, loss = 0.13389774\n",
      "Validation score: 0.951875\n",
      "Iteration 32, loss = 0.13732313\n",
      "Validation score: 0.960125\n",
      "Iteration 33, loss = 0.13053343\n",
      "Validation score: 0.958625\n",
      "Iteration 34, loss = 0.13074129\n",
      "Validation score: 0.960250\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55831721\n",
      "Validation score: 0.919625\n",
      "Iteration 2, loss = 0.25030755\n",
      "Validation score: 0.946625\n",
      "Iteration 3, loss = 0.21678400\n",
      "Validation score: 0.950375\n",
      "Iteration 4, loss = 0.19997664\n",
      "Validation score: 0.950875\n",
      "Iteration 5, loss = 0.18237250\n",
      "Validation score: 0.952375\n",
      "Iteration 6, loss = 0.17750036\n",
      "Validation score: 0.952000\n",
      "Iteration 7, loss = 0.16590104\n",
      "Validation score: 0.948625\n",
      "Iteration 8, loss = 0.16301326\n",
      "Validation score: 0.953625\n",
      "Iteration 9, loss = 0.16027277\n",
      "Validation score: 0.956750\n",
      "Iteration 10, loss = 0.15340065\n",
      "Validation score: 0.955750\n",
      "Iteration 11, loss = 0.15074106\n",
      "Validation score: 0.958000\n",
      "Iteration 12, loss = 0.14764336\n",
      "Validation score: 0.956125\n",
      "Iteration 13, loss = 0.14475370\n",
      "Validation score: 0.954750\n",
      "Iteration 14, loss = 0.13966569\n",
      "Validation score: 0.958875\n",
      "Iteration 15, loss = 0.13749634\n",
      "Validation score: 0.949875\n",
      "Iteration 16, loss = 0.13903482\n",
      "Validation score: 0.960000\n",
      "Iteration 17, loss = 0.14141739\n",
      "Validation score: 0.958250\n",
      "Iteration 18, loss = 0.14260371\n",
      "Validation score: 0.955375\n",
      "Iteration 19, loss = 0.14636794\n",
      "Validation score: 0.959875\n",
      "Iteration 20, loss = 0.14298520\n",
      "Validation score: 0.959375\n",
      "Iteration 21, loss = 0.13265273\n",
      "Validation score: 0.961125\n",
      "Iteration 22, loss = 0.13399398\n",
      "Validation score: 0.953250\n",
      "Iteration 23, loss = 0.12828589\n",
      "Validation score: 0.959000\n",
      "Iteration 24, loss = 0.12743457\n",
      "Validation score: 0.961125\n",
      "Iteration 25, loss = 0.13336677\n",
      "Validation score: 0.959875\n",
      "Iteration 26, loss = 0.12827445\n",
      "Validation score: 0.958500\n",
      "Iteration 27, loss = 0.12926884\n",
      "Validation score: 0.958375\n",
      "Iteration 28, loss = 0.13295490\n",
      "Validation score: 0.960375\n",
      "Iteration 29, loss = 0.13579843\n",
      "Validation score: 0.958750\n",
      "Iteration 30, loss = 0.12923841\n",
      "Validation score: 0.962125\n",
      "Iteration 31, loss = 0.12786763\n",
      "Validation score: 0.958375\n",
      "Iteration 32, loss = 0.13138476\n",
      "Validation score: 0.961500\n",
      "Iteration 33, loss = 0.11952759\n",
      "Validation score: 0.963375\n",
      "Iteration 34, loss = 0.12343216\n",
      "Validation score: 0.958250\n",
      "Iteration 35, loss = 0.12363423\n",
      "Validation score: 0.957000\n",
      "Iteration 36, loss = 0.12093506\n",
      "Validation score: 0.961250\n",
      "Iteration 37, loss = 0.12637523\n",
      "Validation score: 0.964875\n",
      "Iteration 38, loss = 0.12294486\n",
      "Validation score: 0.960375\n",
      "Iteration 39, loss = 0.12156865\n",
      "Validation score: 0.961250\n",
      "Iteration 40, loss = 0.12636414\n",
      "Validation score: 0.952625\n",
      "Iteration 41, loss = 0.12841499\n",
      "Validation score: 0.964500\n",
      "Iteration 42, loss = 0.12213651\n",
      "Validation score: 0.963875\n",
      "Iteration 43, loss = 0.12888911\n",
      "Validation score: 0.960500\n",
      "Iteration 44, loss = 0.12153538\n",
      "Validation score: 0.959625\n",
      "Iteration 45, loss = 0.11990943\n",
      "Validation score: 0.961750\n",
      "Iteration 46, loss = 0.12479737\n",
      "Validation score: 0.959750\n",
      "Iteration 47, loss = 0.12947413\n",
      "Validation score: 0.954375\n",
      "Iteration 48, loss = 0.12910716\n",
      "Validation score: 0.961500\n",
      "Iteration 49, loss = 0.11661158\n",
      "Validation score: 0.956875\n",
      "Iteration 50, loss = 0.11982387\n",
      "Validation score: 0.965250\n",
      "Iteration 51, loss = 0.11662498\n",
      "Validation score: 0.960625\n",
      "Iteration 52, loss = 0.12423327\n",
      "Validation score: 0.962375\n",
      "Iteration 53, loss = 0.12335876\n",
      "Validation score: 0.960625\n",
      "Iteration 54, loss = 0.12677962\n",
      "Validation score: 0.961125\n",
      "Iteration 55, loss = 0.12381590\n",
      "Validation score: 0.960375\n",
      "Iteration 56, loss = 0.12202913\n",
      "Validation score: 0.960625\n",
      "Iteration 57, loss = 0.12280691\n",
      "Validation score: 0.965000\n",
      "Iteration 58, loss = 0.12145691\n",
      "Validation score: 0.962625\n",
      "Iteration 59, loss = 0.12251230\n",
      "Validation score: 0.956750\n",
      "Iteration 60, loss = 0.12295743\n",
      "Validation score: 0.963750\n",
      "Iteration 61, loss = 0.12015923\n",
      "Validation score: 0.956125\n",
      "Iteration 62, loss = 0.12342315\n",
      "Validation score: 0.962000\n",
      "Iteration 63, loss = 0.11768340\n",
      "Validation score: 0.965875\n",
      "Iteration 64, loss = 0.12729067\n",
      "Validation score: 0.960125\n",
      "Iteration 65, loss = 0.12299429\n",
      "Validation score: 0.961750\n",
      "Iteration 66, loss = 0.12045458\n",
      "Validation score: 0.963250\n",
      "Iteration 67, loss = 0.11459695\n",
      "Validation score: 0.960250\n",
      "Iteration 68, loss = 0.12073596\n",
      "Validation score: 0.963750\n",
      "Iteration 69, loss = 0.11702294\n",
      "Validation score: 0.965125\n",
      "Iteration 70, loss = 0.11886467\n",
      "Validation score: 0.964375\n",
      "Iteration 71, loss = 0.12158921\n",
      "Validation score: 0.960875\n",
      "Iteration 72, loss = 0.12477991\n",
      "Validation score: 0.963250\n",
      "Iteration 73, loss = 0.11836035\n",
      "Validation score: 0.965625\n",
      "Iteration 74, loss = 0.11600219\n",
      "Validation score: 0.961000\n",
      "Iteration 75, loss = 0.11791581\n",
      "Validation score: 0.963125\n",
      "Iteration 76, loss = 0.12429305\n",
      "Validation score: 0.955875\n",
      "Iteration 77, loss = 0.12449762\n",
      "Validation score: 0.965625\n",
      "Iteration 78, loss = 0.11769844\n",
      "Validation score: 0.962750\n",
      "Iteration 79, loss = 0.11785150\n",
      "Validation score: 0.966875\n",
      "Iteration 80, loss = 0.11853652\n",
      "Validation score: 0.961625\n",
      "Iteration 81, loss = 0.12904874\n",
      "Validation score: 0.966500\n",
      "Iteration 82, loss = 0.12433298\n",
      "Validation score: 0.960750\n",
      "Iteration 83, loss = 0.12040610\n",
      "Validation score: 0.961750\n",
      "Iteration 84, loss = 0.12638721\n",
      "Validation score: 0.962625\n",
      "Iteration 85, loss = 0.12125844\n",
      "Validation score: 0.964125\n",
      "Iteration 86, loss = 0.11588759\n",
      "Validation score: 0.963750\n",
      "Iteration 87, loss = 0.11735169\n",
      "Validation score: 0.967875\n",
      "Iteration 88, loss = 0.12069369\n",
      "Validation score: 0.961250\n",
      "Iteration 89, loss = 0.12211133\n",
      "Validation score: 0.966375\n",
      "Iteration 90, loss = 0.11932513\n",
      "Validation score: 0.967125\n",
      "Iteration 91, loss = 0.12025146\n",
      "Validation score: 0.964375\n",
      "Iteration 92, loss = 0.11607298\n",
      "Validation score: 0.964125\n",
      "Iteration 93, loss = 0.11458310\n",
      "Validation score: 0.967875\n",
      "Iteration 94, loss = 0.11379695\n",
      "Validation score: 0.961875\n",
      "Iteration 95, loss = 0.11544585\n",
      "Validation score: 0.966625\n",
      "Iteration 96, loss = 0.10969045\n",
      "Validation score: 0.968875\n",
      "Iteration 97, loss = 0.10805564\n",
      "Validation score: 0.966125\n",
      "Iteration 98, loss = 0.12498541\n",
      "Validation score: 0.962500\n",
      "Iteration 99, loss = 0.12029596\n",
      "Validation score: 0.962875\n",
      "Iteration 100, loss = 0.11069778\n",
      "Validation score: 0.963750\n",
      "Iteration 101, loss = 0.11248658\n",
      "Validation score: 0.960875\n",
      "Iteration 102, loss = 0.12410628\n",
      "Validation score: 0.964875\n",
      "Iteration 103, loss = 0.12480814\n",
      "Validation score: 0.963125\n",
      "Iteration 104, loss = 0.11820857\n",
      "Validation score: 0.967625\n",
      "Iteration 105, loss = 0.11550838\n",
      "Validation score: 0.967250\n",
      "Iteration 106, loss = 0.11664876\n",
      "Validation score: 0.968250\n",
      "Iteration 107, loss = 0.11661285\n",
      "Validation score: 0.966625\n",
      "Iteration 108, loss = 0.11657059\n",
      "Validation score: 0.966375\n",
      "Iteration 109, loss = 0.12578541\n",
      "Validation score: 0.958750\n",
      "Iteration 110, loss = 0.11895513\n",
      "Validation score: 0.961750\n",
      "Iteration 111, loss = 0.11331584\n",
      "Validation score: 0.965125\n",
      "Iteration 112, loss = 0.11066470\n",
      "Validation score: 0.969875\n",
      "Iteration 113, loss = 0.11999057\n",
      "Validation score: 0.961250\n",
      "Iteration 114, loss = 0.11517030\n",
      "Validation score: 0.960875\n",
      "Iteration 115, loss = 0.11400337\n",
      "Validation score: 0.962875\n",
      "Iteration 116, loss = 0.11745708\n",
      "Validation score: 0.969625\n",
      "Iteration 117, loss = 0.12244209\n",
      "Validation score: 0.966000\n",
      "Iteration 118, loss = 0.11245005\n",
      "Validation score: 0.965625\n",
      "Iteration 119, loss = 0.11833636\n",
      "Validation score: 0.962875\n",
      "Iteration 120, loss = 0.11274537\n",
      "Validation score: 0.962000\n",
      "Iteration 121, loss = 0.11830398\n",
      "Validation score: 0.963000\n",
      "Iteration 122, loss = 0.11906798\n",
      "Validation score: 0.966375\n",
      "Iteration 123, loss = 0.11522632\n",
      "Validation score: 0.966500\n",
      "Iteration 124, loss = 0.11385617\n",
      "Validation score: 0.962625\n",
      "Iteration 125, loss = 0.11469605\n",
      "Validation score: 0.963000\n",
      "Iteration 126, loss = 0.11701144\n",
      "Validation score: 0.957625\n",
      "Iteration 127, loss = 0.11721886\n",
      "Validation score: 0.967875\n",
      "Iteration 128, loss = 0.10915409\n",
      "Validation score: 0.965125\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56982884\n",
      "Validation score: 0.927375\n",
      "Iteration 2, loss = 0.25762709\n",
      "Validation score: 0.944000\n",
      "Iteration 3, loss = 0.21632361\n",
      "Validation score: 0.942875\n",
      "Iteration 4, loss = 0.19797984\n",
      "Validation score: 0.945750\n",
      "Iteration 5, loss = 0.18025368\n",
      "Validation score: 0.953625\n",
      "Iteration 6, loss = 0.17184705\n",
      "Validation score: 0.954500\n",
      "Iteration 7, loss = 0.16702760\n",
      "Validation score: 0.949625\n",
      "Iteration 8, loss = 0.16060435\n",
      "Validation score: 0.947250\n",
      "Iteration 9, loss = 0.15938020\n",
      "Validation score: 0.955250\n",
      "Iteration 10, loss = 0.15291457\n",
      "Validation score: 0.957750\n",
      "Iteration 11, loss = 0.15009939\n",
      "Validation score: 0.952375\n",
      "Iteration 12, loss = 0.14558814\n",
      "Validation score: 0.961500\n",
      "Iteration 13, loss = 0.14481109\n",
      "Validation score: 0.958000\n",
      "Iteration 14, loss = 0.14600602\n",
      "Validation score: 0.956375\n",
      "Iteration 15, loss = 0.14523962\n",
      "Validation score: 0.957375\n",
      "Iteration 16, loss = 0.14228658\n",
      "Validation score: 0.951250\n",
      "Iteration 17, loss = 0.14002077\n",
      "Validation score: 0.960125\n",
      "Iteration 18, loss = 0.13553288\n",
      "Validation score: 0.962000\n",
      "Iteration 19, loss = 0.13159372\n",
      "Validation score: 0.956750\n",
      "Iteration 20, loss = 0.14015254\n",
      "Validation score: 0.957125\n",
      "Iteration 21, loss = 0.13496922\n",
      "Validation score: 0.960375\n",
      "Iteration 22, loss = 0.13208477\n",
      "Validation score: 0.959375\n",
      "Iteration 23, loss = 0.13768230\n",
      "Validation score: 0.956375\n",
      "Iteration 24, loss = 0.13372424\n",
      "Validation score: 0.951875\n",
      "Iteration 25, loss = 0.12513433\n",
      "Validation score: 0.957875\n",
      "Iteration 26, loss = 0.13320981\n",
      "Validation score: 0.957625\n",
      "Iteration 27, loss = 0.13415666\n",
      "Validation score: 0.959500\n",
      "Iteration 28, loss = 0.13560532\n",
      "Validation score: 0.956000\n",
      "Iteration 29, loss = 0.13226201\n",
      "Validation score: 0.955375\n",
      "Iteration 30, loss = 0.13042910\n",
      "Validation score: 0.962750\n",
      "Iteration 31, loss = 0.12739417\n",
      "Validation score: 0.955750\n",
      "Iteration 32, loss = 0.12655055\n",
      "Validation score: 0.957250\n",
      "Iteration 33, loss = 0.12909367\n",
      "Validation score: 0.961750\n",
      "Iteration 34, loss = 0.12711935\n",
      "Validation score: 0.957250\n",
      "Iteration 35, loss = 0.13066507\n",
      "Validation score: 0.958875\n",
      "Iteration 36, loss = 0.13252438\n",
      "Validation score: 0.958000\n",
      "Iteration 37, loss = 0.12726464\n",
      "Validation score: 0.960250\n",
      "Iteration 38, loss = 0.12501457\n",
      "Validation score: 0.955375\n",
      "Iteration 39, loss = 0.12828205\n",
      "Validation score: 0.960375\n",
      "Iteration 40, loss = 0.12777544\n",
      "Validation score: 0.957750\n",
      "Iteration 41, loss = 0.12537815\n",
      "Validation score: 0.956625\n",
      "Iteration 42, loss = 0.12760390\n",
      "Validation score: 0.955875\n",
      "Iteration 43, loss = 0.13508592\n",
      "Validation score: 0.955875\n",
      "Iteration 44, loss = 0.13232048\n",
      "Validation score: 0.964250\n",
      "Iteration 45, loss = 0.12414247\n",
      "Validation score: 0.960125\n",
      "Iteration 46, loss = 0.12698299\n",
      "Validation score: 0.960500\n",
      "Iteration 47, loss = 0.12727991\n",
      "Validation score: 0.965125\n",
      "Iteration 48, loss = 0.12719927\n",
      "Validation score: 0.961000\n",
      "Iteration 49, loss = 0.12569663\n",
      "Validation score: 0.958125\n",
      "Iteration 50, loss = 0.12486873\n",
      "Validation score: 0.963000\n",
      "Iteration 51, loss = 0.12345262\n",
      "Validation score: 0.955125\n",
      "Iteration 52, loss = 0.12253526\n",
      "Validation score: 0.958375\n",
      "Iteration 53, loss = 0.12499768\n",
      "Validation score: 0.958875\n",
      "Iteration 54, loss = 0.12967062\n",
      "Validation score: 0.959375\n",
      "Iteration 55, loss = 0.12142729\n",
      "Validation score: 0.961250\n",
      "Iteration 56, loss = 0.13210127\n",
      "Validation score: 0.957000\n",
      "Iteration 57, loss = 0.12408283\n",
      "Validation score: 0.958250\n",
      "Iteration 58, loss = 0.13017228\n",
      "Validation score: 0.959125\n",
      "Iteration 59, loss = 0.12800890\n",
      "Validation score: 0.956500\n",
      "Iteration 60, loss = 0.12780438\n",
      "Validation score: 0.959000\n",
      "Iteration 61, loss = 0.12288592\n",
      "Validation score: 0.957250\n",
      "Iteration 62, loss = 0.12636846\n",
      "Validation score: 0.961375\n",
      "Iteration 63, loss = 0.12223906\n",
      "Validation score: 0.960875\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38879817\n",
      "Validation score: 0.944125\n",
      "Iteration 2, loss = 0.20669886\n",
      "Validation score: 0.956500\n",
      "Iteration 3, loss = 0.17118263\n",
      "Validation score: 0.957125\n",
      "Iteration 4, loss = 0.14939469\n",
      "Validation score: 0.964500\n",
      "Iteration 5, loss = 0.13488167\n",
      "Validation score: 0.963625\n",
      "Iteration 6, loss = 0.12490821\n",
      "Validation score: 0.963375\n",
      "Iteration 7, loss = 0.12035584\n",
      "Validation score: 0.965750\n",
      "Iteration 8, loss = 0.11306130\n",
      "Validation score: 0.963875\n",
      "Iteration 9, loss = 0.10974823\n",
      "Validation score: 0.967875\n",
      "Iteration 10, loss = 0.10438177\n",
      "Validation score: 0.968250\n",
      "Iteration 11, loss = 0.10249236\n",
      "Validation score: 0.971625\n",
      "Iteration 12, loss = 0.09638503\n",
      "Validation score: 0.971750\n",
      "Iteration 13, loss = 0.09278047\n",
      "Validation score: 0.968750\n",
      "Iteration 14, loss = 0.09166646\n",
      "Validation score: 0.963875\n",
      "Iteration 15, loss = 0.08944177\n",
      "Validation score: 0.968375\n",
      "Iteration 16, loss = 0.09005541\n",
      "Validation score: 0.968750\n",
      "Iteration 17, loss = 0.08951670\n",
      "Validation score: 0.971625\n",
      "Iteration 18, loss = 0.08830145\n",
      "Validation score: 0.969500\n",
      "Iteration 19, loss = 0.08272723\n",
      "Validation score: 0.972375\n",
      "Iteration 20, loss = 0.08190741\n",
      "Validation score: 0.969250\n",
      "Iteration 21, loss = 0.08182021\n",
      "Validation score: 0.968125\n",
      "Iteration 22, loss = 0.08186460\n",
      "Validation score: 0.969750\n",
      "Iteration 23, loss = 0.08055542\n",
      "Validation score: 0.969750\n",
      "Iteration 24, loss = 0.07988554\n",
      "Validation score: 0.971000\n",
      "Iteration 25, loss = 0.08014540\n",
      "Validation score: 0.969500\n",
      "Iteration 26, loss = 0.07993026\n",
      "Validation score: 0.967500\n",
      "Iteration 27, loss = 0.07808203\n",
      "Validation score: 0.970875\n",
      "Iteration 28, loss = 0.08228448\n",
      "Validation score: 0.969500\n",
      "Iteration 29, loss = 0.08169706\n",
      "Validation score: 0.970750\n",
      "Iteration 30, loss = 0.08023798\n",
      "Validation score: 0.969125\n",
      "Iteration 31, loss = 0.07910694\n",
      "Validation score: 0.971875\n",
      "Iteration 32, loss = 0.07616614\n",
      "Validation score: 0.971375\n",
      "Iteration 33, loss = 0.07826509\n",
      "Validation score: 0.969125\n",
      "Iteration 34, loss = 0.07744180\n",
      "Validation score: 0.967000\n",
      "Iteration 35, loss = 0.07435122\n",
      "Validation score: 0.970875\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38300610\n",
      "Validation score: 0.944625\n",
      "Iteration 2, loss = 0.20480144\n",
      "Validation score: 0.957625\n",
      "Iteration 3, loss = 0.16679519\n",
      "Validation score: 0.960500\n",
      "Iteration 4, loss = 0.14388021\n",
      "Validation score: 0.965875\n",
      "Iteration 5, loss = 0.12944421\n",
      "Validation score: 0.961500\n",
      "Iteration 6, loss = 0.12349310\n",
      "Validation score: 0.968750\n",
      "Iteration 7, loss = 0.11424182\n",
      "Validation score: 0.966625\n",
      "Iteration 8, loss = 0.11070736\n",
      "Validation score: 0.966125\n",
      "Iteration 9, loss = 0.10398771\n",
      "Validation score: 0.967625\n",
      "Iteration 10, loss = 0.10016831\n",
      "Validation score: 0.968875\n",
      "Iteration 11, loss = 0.09811830\n",
      "Validation score: 0.964750\n",
      "Iteration 12, loss = 0.09650801\n",
      "Validation score: 0.968625\n",
      "Iteration 13, loss = 0.09083940\n",
      "Validation score: 0.971000\n",
      "Iteration 14, loss = 0.09228973\n",
      "Validation score: 0.970250\n",
      "Iteration 15, loss = 0.09026869\n",
      "Validation score: 0.970375\n",
      "Iteration 16, loss = 0.08791546\n",
      "Validation score: 0.971375\n",
      "Iteration 17, loss = 0.08720782\n",
      "Validation score: 0.971125\n",
      "Iteration 18, loss = 0.08595471\n",
      "Validation score: 0.970125\n",
      "Iteration 19, loss = 0.08959164\n",
      "Validation score: 0.968000\n",
      "Iteration 20, loss = 0.08853624\n",
      "Validation score: 0.972375\n",
      "Iteration 21, loss = 0.07938461\n",
      "Validation score: 0.971625\n",
      "Iteration 22, loss = 0.07732825\n",
      "Validation score: 0.974000\n",
      "Iteration 23, loss = 0.07786201\n",
      "Validation score: 0.972375\n",
      "Iteration 24, loss = 0.07717508\n",
      "Validation score: 0.970750\n",
      "Iteration 25, loss = 0.08255104\n",
      "Validation score: 0.970625\n",
      "Iteration 26, loss = 0.08350526\n",
      "Validation score: 0.972625\n",
      "Iteration 27, loss = 0.07697549\n",
      "Validation score: 0.972875\n",
      "Iteration 28, loss = 0.07244270\n",
      "Validation score: 0.971125\n",
      "Iteration 29, loss = 0.07713597\n",
      "Validation score: 0.972000\n",
      "Iteration 30, loss = 0.07648117\n",
      "Validation score: 0.972625\n",
      "Iteration 31, loss = 0.07457755\n",
      "Validation score: 0.969500\n",
      "Iteration 32, loss = 0.07827168\n",
      "Validation score: 0.969500\n",
      "Iteration 33, loss = 0.08248293\n",
      "Validation score: 0.968375\n",
      "Iteration 34, loss = 0.07714547\n",
      "Validation score: 0.971375\n",
      "Iteration 35, loss = 0.07772090\n",
      "Validation score: 0.972250\n",
      "Iteration 36, loss = 0.07594061\n",
      "Validation score: 0.973750\n",
      "Iteration 37, loss = 0.07326769\n",
      "Validation score: 0.972750\n",
      "Iteration 38, loss = 0.07637875\n",
      "Validation score: 0.973250\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.39856778\n",
      "Validation score: 0.943625\n",
      "Iteration 2, loss = 0.20615913\n",
      "Validation score: 0.949750\n",
      "Iteration 3, loss = 0.16916408\n",
      "Validation score: 0.958875\n",
      "Iteration 4, loss = 0.15028102\n",
      "Validation score: 0.961375\n",
      "Iteration 5, loss = 0.13441001\n",
      "Validation score: 0.962375\n",
      "Iteration 6, loss = 0.12582801\n",
      "Validation score: 0.962000\n",
      "Iteration 7, loss = 0.11816560\n",
      "Validation score: 0.969375\n",
      "Iteration 8, loss = 0.11087990\n",
      "Validation score: 0.965500\n",
      "Iteration 9, loss = 0.10242620\n",
      "Validation score: 0.968625\n",
      "Iteration 10, loss = 0.09898233\n",
      "Validation score: 0.966875\n",
      "Iteration 11, loss = 0.09808513\n",
      "Validation score: 0.967875\n",
      "Iteration 12, loss = 0.09775848\n",
      "Validation score: 0.966250\n",
      "Iteration 13, loss = 0.09752110\n",
      "Validation score: 0.968500\n",
      "Iteration 14, loss = 0.08764798\n",
      "Validation score: 0.971500\n",
      "Iteration 15, loss = 0.08337526\n",
      "Validation score: 0.970000\n",
      "Iteration 16, loss = 0.08195433\n",
      "Validation score: 0.969500\n",
      "Iteration 17, loss = 0.08094758\n",
      "Validation score: 0.971250\n",
      "Iteration 18, loss = 0.08641512\n",
      "Validation score: 0.967625\n",
      "Iteration 19, loss = 0.08762657\n",
      "Validation score: 0.970125\n",
      "Iteration 20, loss = 0.08291341\n",
      "Validation score: 0.967875\n",
      "Iteration 21, loss = 0.08339381\n",
      "Validation score: 0.970000\n",
      "Iteration 22, loss = 0.08058725\n",
      "Validation score: 0.970875\n",
      "Iteration 23, loss = 0.07880474\n",
      "Validation score: 0.971125\n",
      "Iteration 24, loss = 0.07699353\n",
      "Validation score: 0.972875\n",
      "Iteration 25, loss = 0.07700957\n",
      "Validation score: 0.970875\n",
      "Iteration 26, loss = 0.07601072\n",
      "Validation score: 0.970500\n",
      "Iteration 27, loss = 0.08183676\n",
      "Validation score: 0.973250\n",
      "Iteration 28, loss = 0.07549378\n",
      "Validation score: 0.969000\n",
      "Iteration 29, loss = 0.07429083\n",
      "Validation score: 0.968875\n",
      "Iteration 30, loss = 0.07410637\n",
      "Validation score: 0.965125\n",
      "Iteration 31, loss = 0.07590384\n",
      "Validation score: 0.967125\n",
      "Iteration 32, loss = 0.07689228\n",
      "Validation score: 0.970250\n",
      "Iteration 33, loss = 0.07971086\n",
      "Validation score: 0.967750\n",
      "Iteration 34, loss = 0.08582251\n",
      "Validation score: 0.969375\n",
      "Iteration 35, loss = 0.07836149\n",
      "Validation score: 0.974625\n",
      "Iteration 36, loss = 0.07192688\n",
      "Validation score: 0.972250\n",
      "Iteration 37, loss = 0.06872672\n",
      "Validation score: 0.971250\n",
      "Iteration 38, loss = 0.06903748\n",
      "Validation score: 0.970375\n",
      "Iteration 39, loss = 0.06962194\n",
      "Validation score: 0.971750\n",
      "Iteration 40, loss = 0.07219777\n",
      "Validation score: 0.967875\n",
      "Iteration 41, loss = 0.07935051\n",
      "Validation score: 0.970500\n",
      "Iteration 42, loss = 0.07414463\n",
      "Validation score: 0.972375\n",
      "Iteration 43, loss = 0.07262922\n",
      "Validation score: 0.971375\n",
      "Iteration 44, loss = 0.07171208\n",
      "Validation score: 0.970500\n",
      "Iteration 45, loss = 0.07868240\n",
      "Validation score: 0.970125\n",
      "Iteration 46, loss = 0.07915482\n",
      "Validation score: 0.971250\n",
      "Iteration 47, loss = 0.07680168\n",
      "Validation score: 0.972875\n",
      "Iteration 48, loss = 0.07322692\n",
      "Validation score: 0.972625\n",
      "Iteration 49, loss = 0.07203225\n",
      "Validation score: 0.969625\n",
      "Iteration 50, loss = 0.06711791\n",
      "Validation score: 0.972500\n",
      "Iteration 51, loss = 0.06563868\n",
      "Validation score: 0.973625\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49212585\n",
      "Validation score: 0.922250\n",
      "Iteration 2, loss = 0.28104290\n",
      "Validation score: 0.928625\n",
      "Iteration 3, loss = 0.25127263\n",
      "Validation score: 0.933375\n",
      "Iteration 4, loss = 0.23923603\n",
      "Validation score: 0.933500\n",
      "Iteration 5, loss = 0.22852865\n",
      "Validation score: 0.939750\n",
      "Iteration 6, loss = 0.22043258\n",
      "Validation score: 0.937125\n",
      "Iteration 7, loss = 0.22035708\n",
      "Validation score: 0.935875\n",
      "Iteration 8, loss = 0.21479941\n",
      "Validation score: 0.941250\n",
      "Iteration 9, loss = 0.20694752\n",
      "Validation score: 0.943125\n",
      "Iteration 10, loss = 0.20414908\n",
      "Validation score: 0.943125\n",
      "Iteration 11, loss = 0.20158127\n",
      "Validation score: 0.934875\n",
      "Iteration 12, loss = 0.20140176\n",
      "Validation score: 0.939000\n",
      "Iteration 13, loss = 0.20065625\n",
      "Validation score: 0.933500\n",
      "Iteration 14, loss = 0.20007027\n",
      "Validation score: 0.940375\n",
      "Iteration 15, loss = 0.19664198\n",
      "Validation score: 0.940250\n",
      "Iteration 16, loss = 0.19145517\n",
      "Validation score: 0.941125\n",
      "Iteration 17, loss = 0.19526237\n",
      "Validation score: 0.942875\n",
      "Iteration 18, loss = 0.18747538\n",
      "Validation score: 0.938250\n",
      "Iteration 19, loss = 0.18837745\n",
      "Validation score: 0.935125\n",
      "Iteration 20, loss = 0.19150783\n",
      "Validation score: 0.939625\n",
      "Iteration 21, loss = 0.18781508\n",
      "Validation score: 0.938750\n",
      "Iteration 22, loss = 0.18640994\n",
      "Validation score: 0.943625\n",
      "Iteration 23, loss = 0.18789501\n",
      "Validation score: 0.943750\n",
      "Iteration 24, loss = 0.18407298\n",
      "Validation score: 0.945875\n",
      "Iteration 25, loss = 0.18853402\n",
      "Validation score: 0.943625\n",
      "Iteration 26, loss = 0.18657899\n",
      "Validation score: 0.942250\n",
      "Iteration 27, loss = 0.18163476\n",
      "Validation score: 0.938750\n",
      "Iteration 28, loss = 0.19236654\n",
      "Validation score: 0.941375\n",
      "Iteration 29, loss = 0.18863476\n",
      "Validation score: 0.933500\n",
      "Iteration 30, loss = 0.18241270\n",
      "Validation score: 0.943625\n",
      "Iteration 31, loss = 0.18750117\n",
      "Validation score: 0.946625\n",
      "Iteration 32, loss = 0.17776250\n",
      "Validation score: 0.944375\n",
      "Iteration 33, loss = 0.18313942\n",
      "Validation score: 0.944875\n",
      "Iteration 34, loss = 0.18415624\n",
      "Validation score: 0.945000\n",
      "Iteration 35, loss = 0.17656681\n",
      "Validation score: 0.943250\n",
      "Iteration 36, loss = 0.18171175\n",
      "Validation score: 0.944250\n",
      "Iteration 37, loss = 0.18015735\n",
      "Validation score: 0.946125\n",
      "Iteration 38, loss = 0.17932341\n",
      "Validation score: 0.939625\n",
      "Iteration 39, loss = 0.17987900\n",
      "Validation score: 0.941375\n",
      "Iteration 40, loss = 0.18228847\n",
      "Validation score: 0.941125\n",
      "Iteration 41, loss = 0.17869955\n",
      "Validation score: 0.943250\n",
      "Iteration 42, loss = 0.18126613\n",
      "Validation score: 0.941000\n",
      "Iteration 43, loss = 0.17893332\n",
      "Validation score: 0.943875\n",
      "Iteration 44, loss = 0.18414933\n",
      "Validation score: 0.941125\n",
      "Iteration 45, loss = 0.17908728\n",
      "Validation score: 0.941500\n",
      "Iteration 46, loss = 0.17736163\n",
      "Validation score: 0.944125\n",
      "Iteration 47, loss = 0.17979123\n",
      "Validation score: 0.944250\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48783988\n",
      "Validation score: 0.913500\n",
      "Iteration 2, loss = 0.26975314\n",
      "Validation score: 0.926000\n",
      "Iteration 3, loss = 0.24313486\n",
      "Validation score: 0.928625\n",
      "Iteration 4, loss = 0.22959560\n",
      "Validation score: 0.937750\n",
      "Iteration 5, loss = 0.22337147\n",
      "Validation score: 0.940000\n",
      "Iteration 6, loss = 0.20747222\n",
      "Validation score: 0.945000\n",
      "Iteration 7, loss = 0.20110237\n",
      "Validation score: 0.940375\n",
      "Iteration 8, loss = 0.20022209\n",
      "Validation score: 0.944625\n",
      "Iteration 9, loss = 0.19570169\n",
      "Validation score: 0.941875\n",
      "Iteration 10, loss = 0.18849431\n",
      "Validation score: 0.945875\n",
      "Iteration 11, loss = 0.18824384\n",
      "Validation score: 0.945250\n",
      "Iteration 12, loss = 0.18726162\n",
      "Validation score: 0.943875\n",
      "Iteration 13, loss = 0.18430047\n",
      "Validation score: 0.943750\n",
      "Iteration 14, loss = 0.18477044\n",
      "Validation score: 0.941000\n",
      "Iteration 15, loss = 0.18506227\n",
      "Validation score: 0.941000\n",
      "Iteration 16, loss = 0.18506399\n",
      "Validation score: 0.944500\n",
      "Iteration 17, loss = 0.18630472\n",
      "Validation score: 0.944500\n",
      "Iteration 18, loss = 0.18166137\n",
      "Validation score: 0.946000\n",
      "Iteration 19, loss = 0.17728449\n",
      "Validation score: 0.943625\n",
      "Iteration 20, loss = 0.17743545\n",
      "Validation score: 0.948625\n",
      "Iteration 21, loss = 0.17974606\n",
      "Validation score: 0.942250\n",
      "Iteration 22, loss = 0.18336747\n",
      "Validation score: 0.947875\n",
      "Iteration 23, loss = 0.17823860\n",
      "Validation score: 0.944000\n",
      "Iteration 24, loss = 0.18053824\n",
      "Validation score: 0.947125\n",
      "Iteration 25, loss = 0.17534434\n",
      "Validation score: 0.945000\n",
      "Iteration 26, loss = 0.17839657\n",
      "Validation score: 0.943750\n",
      "Iteration 27, loss = 0.17613251\n",
      "Validation score: 0.948500\n",
      "Iteration 28, loss = 0.17602000\n",
      "Validation score: 0.942375\n",
      "Iteration 29, loss = 0.17687684\n",
      "Validation score: 0.940375\n",
      "Iteration 30, loss = 0.18073945\n",
      "Validation score: 0.942375\n",
      "Iteration 31, loss = 0.17527771\n",
      "Validation score: 0.947625\n",
      "Iteration 32, loss = 0.17125498\n",
      "Validation score: 0.946000\n",
      "Iteration 33, loss = 0.18149916\n",
      "Validation score: 0.946125\n",
      "Iteration 34, loss = 0.17452833\n",
      "Validation score: 0.949250\n",
      "Iteration 35, loss = 0.17291658\n",
      "Validation score: 0.949750\n",
      "Iteration 36, loss = 0.17058471\n",
      "Validation score: 0.947250\n",
      "Iteration 37, loss = 0.17374017\n",
      "Validation score: 0.947750\n",
      "Iteration 38, loss = 0.16816329\n",
      "Validation score: 0.945750\n",
      "Iteration 39, loss = 0.17423763\n",
      "Validation score: 0.944500\n",
      "Iteration 40, loss = 0.17041756\n",
      "Validation score: 0.946875\n",
      "Iteration 41, loss = 0.17148857\n",
      "Validation score: 0.940250\n",
      "Iteration 42, loss = 0.17554496\n",
      "Validation score: 0.944125\n",
      "Iteration 43, loss = 0.17434201\n",
      "Validation score: 0.944750\n",
      "Iteration 44, loss = 0.17408688\n",
      "Validation score: 0.941750\n",
      "Iteration 45, loss = 0.17338169\n",
      "Validation score: 0.946125\n",
      "Iteration 46, loss = 0.17622560\n",
      "Validation score: 0.942875\n",
      "Iteration 47, loss = 0.17420688\n",
      "Validation score: 0.945375\n",
      "Iteration 48, loss = 0.17699458\n",
      "Validation score: 0.940375\n",
      "Iteration 49, loss = 0.17558170\n",
      "Validation score: 0.946000\n",
      "Iteration 50, loss = 0.17039560\n",
      "Validation score: 0.948625\n",
      "Iteration 51, loss = 0.17349320\n",
      "Validation score: 0.946625\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50024229\n",
      "Validation score: 0.913750\n",
      "Iteration 2, loss = 0.28465795\n",
      "Validation score: 0.926500\n",
      "Iteration 3, loss = 0.24906850\n",
      "Validation score: 0.931250\n",
      "Iteration 4, loss = 0.23389657\n",
      "Validation score: 0.931500\n",
      "Iteration 5, loss = 0.22770136\n",
      "Validation score: 0.934000\n",
      "Iteration 6, loss = 0.21755473\n",
      "Validation score: 0.938375\n",
      "Iteration 7, loss = 0.21082739\n",
      "Validation score: 0.941625\n",
      "Iteration 8, loss = 0.20611459\n",
      "Validation score: 0.934625\n",
      "Iteration 9, loss = 0.20650940\n",
      "Validation score: 0.939125\n",
      "Iteration 10, loss = 0.19997512\n",
      "Validation score: 0.939875\n",
      "Iteration 11, loss = 0.19872996\n",
      "Validation score: 0.943500\n",
      "Iteration 12, loss = 0.19564617\n",
      "Validation score: 0.941875\n",
      "Iteration 13, loss = 0.19466684\n",
      "Validation score: 0.942375\n",
      "Iteration 14, loss = 0.19238920\n",
      "Validation score: 0.941250\n",
      "Iteration 15, loss = 0.19431539\n",
      "Validation score: 0.943750\n",
      "Iteration 16, loss = 0.18841028\n",
      "Validation score: 0.941625\n",
      "Iteration 17, loss = 0.19309406\n",
      "Validation score: 0.939500\n",
      "Iteration 18, loss = 0.19563792\n",
      "Validation score: 0.944750\n",
      "Iteration 19, loss = 0.19510040\n",
      "Validation score: 0.942875\n",
      "Iteration 20, loss = 0.18745181\n",
      "Validation score: 0.940625\n",
      "Iteration 21, loss = 0.18834847\n",
      "Validation score: 0.940750\n",
      "Iteration 22, loss = 0.18458716\n",
      "Validation score: 0.936875\n",
      "Iteration 23, loss = 0.18336138\n",
      "Validation score: 0.941000\n",
      "Iteration 24, loss = 0.18289564\n",
      "Validation score: 0.943750\n",
      "Iteration 25, loss = 0.18521575\n",
      "Validation score: 0.940875\n",
      "Iteration 26, loss = 0.18314835\n",
      "Validation score: 0.941750\n",
      "Iteration 27, loss = 0.18553729\n",
      "Validation score: 0.939875\n",
      "Iteration 28, loss = 0.18308116\n",
      "Validation score: 0.942875\n",
      "Iteration 29, loss = 0.17752219\n",
      "Validation score: 0.941625\n",
      "Iteration 30, loss = 0.18382412\n",
      "Validation score: 0.933750\n",
      "Iteration 31, loss = 0.17634411\n",
      "Validation score: 0.937500\n",
      "Iteration 32, loss = 0.18582115\n",
      "Validation score: 0.944000\n",
      "Iteration 33, loss = 0.17728807\n",
      "Validation score: 0.941250\n",
      "Iteration 34, loss = 0.17737936\n",
      "Validation score: 0.944125\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29695600\n",
      "Validation score: 0.230500\n",
      "Iteration 2, loss = 1.43002225\n",
      "Validation score: 0.704500\n",
      "Iteration 3, loss = 0.63653570\n",
      "Validation score: 0.880375\n",
      "Iteration 4, loss = 0.37805933\n",
      "Validation score: 0.907875\n",
      "Iteration 5, loss = 0.27997494\n",
      "Validation score: 0.916875\n",
      "Iteration 6, loss = 0.22514098\n",
      "Validation score: 0.936500\n",
      "Iteration 7, loss = 0.18559650\n",
      "Validation score: 0.942125\n",
      "Iteration 8, loss = 0.15951923\n",
      "Validation score: 0.944000\n",
      "Iteration 9, loss = 0.14017145\n",
      "Validation score: 0.947000\n",
      "Iteration 10, loss = 0.12178255\n",
      "Validation score: 0.948125\n",
      "Iteration 11, loss = 0.10677195\n",
      "Validation score: 0.949875\n",
      "Iteration 12, loss = 0.09553424\n",
      "Validation score: 0.953625\n",
      "Iteration 13, loss = 0.08569684\n",
      "Validation score: 0.956875\n",
      "Iteration 14, loss = 0.07596174\n",
      "Validation score: 0.955875\n",
      "Iteration 15, loss = 0.06926282\n",
      "Validation score: 0.956250\n",
      "Iteration 16, loss = 0.06185856\n",
      "Validation score: 0.956000\n",
      "Iteration 17, loss = 0.05674455\n",
      "Validation score: 0.956875\n",
      "Iteration 18, loss = 0.04926720\n",
      "Validation score: 0.957125\n",
      "Iteration 19, loss = 0.04473015\n",
      "Validation score: 0.957750\n",
      "Iteration 20, loss = 0.03928962\n",
      "Validation score: 0.957500\n",
      "Iteration 21, loss = 0.03586821\n",
      "Validation score: 0.958125\n",
      "Iteration 22, loss = 0.03158651\n",
      "Validation score: 0.958625\n",
      "Iteration 23, loss = 0.02996126\n",
      "Validation score: 0.957250\n",
      "Iteration 24, loss = 0.02686795\n",
      "Validation score: 0.957250\n",
      "Iteration 25, loss = 0.02363569\n",
      "Validation score: 0.959625\n",
      "Iteration 26, loss = 0.02120052\n",
      "Validation score: 0.959125\n",
      "Iteration 27, loss = 0.01973656\n",
      "Validation score: 0.958750\n",
      "Iteration 28, loss = 0.01830452\n",
      "Validation score: 0.958000\n",
      "Iteration 29, loss = 0.01687731\n",
      "Validation score: 0.954875\n",
      "Iteration 30, loss = 0.01550441\n",
      "Validation score: 0.957250\n",
      "Iteration 31, loss = 0.01270168\n",
      "Validation score: 0.957250\n",
      "Iteration 32, loss = 0.01150760\n",
      "Validation score: 0.958750\n",
      "Iteration 33, loss = 0.01046527\n",
      "Validation score: 0.959000\n",
      "Iteration 34, loss = 0.00907753\n",
      "Validation score: 0.957500\n",
      "Iteration 35, loss = 0.00873958\n",
      "Validation score: 0.957500\n",
      "Iteration 36, loss = 0.00928190\n",
      "Validation score: 0.958000\n",
      "Iteration 37, loss = 0.00785819\n",
      "Validation score: 0.958500\n",
      "Iteration 38, loss = 0.00695223\n",
      "Validation score: 0.959000\n",
      "Iteration 39, loss = 0.00651370\n",
      "Validation score: 0.959500\n",
      "Iteration 40, loss = 0.00632651\n",
      "Validation score: 0.959000\n",
      "Iteration 41, loss = 0.00594931\n",
      "Validation score: 0.959375\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29775056\n",
      "Validation score: 0.263875\n",
      "Iteration 2, loss = 1.41332268\n",
      "Validation score: 0.703875\n",
      "Iteration 3, loss = 0.62999602\n",
      "Validation score: 0.878000\n",
      "Iteration 4, loss = 0.37635937\n",
      "Validation score: 0.913375\n",
      "Iteration 5, loss = 0.27736692\n",
      "Validation score: 0.927625\n",
      "Iteration 6, loss = 0.21739825\n",
      "Validation score: 0.935750\n",
      "Iteration 7, loss = 0.18415989\n",
      "Validation score: 0.941000\n",
      "Iteration 8, loss = 0.15415076\n",
      "Validation score: 0.948875\n",
      "Iteration 9, loss = 0.13663118\n",
      "Validation score: 0.947500\n",
      "Iteration 10, loss = 0.11735943\n",
      "Validation score: 0.952000\n",
      "Iteration 11, loss = 0.10388943\n",
      "Validation score: 0.954625\n",
      "Iteration 12, loss = 0.09181523\n",
      "Validation score: 0.958000\n",
      "Iteration 13, loss = 0.08174845\n",
      "Validation score: 0.955000\n",
      "Iteration 14, loss = 0.07469822\n",
      "Validation score: 0.956875\n",
      "Iteration 15, loss = 0.06621090\n",
      "Validation score: 0.953375\n",
      "Iteration 16, loss = 0.06194520\n",
      "Validation score: 0.955250\n",
      "Iteration 17, loss = 0.05452376\n",
      "Validation score: 0.956625\n",
      "Iteration 18, loss = 0.05187271\n",
      "Validation score: 0.954875\n",
      "Iteration 19, loss = 0.04719627\n",
      "Validation score: 0.956125\n",
      "Iteration 20, loss = 0.04127193\n",
      "Validation score: 0.956625\n",
      "Iteration 21, loss = 0.03788660\n",
      "Validation score: 0.954875\n",
      "Iteration 22, loss = 0.03396105\n",
      "Validation score: 0.958250\n",
      "Iteration 23, loss = 0.02903109\n",
      "Validation score: 0.959750\n",
      "Iteration 24, loss = 0.02639853\n",
      "Validation score: 0.959375\n",
      "Iteration 25, loss = 0.02674507\n",
      "Validation score: 0.956625\n",
      "Iteration 26, loss = 0.02373207\n",
      "Validation score: 0.958000\n",
      "Iteration 27, loss = 0.01983721\n",
      "Validation score: 0.957375\n",
      "Iteration 28, loss = 0.02054736\n",
      "Validation score: 0.958625\n",
      "Iteration 29, loss = 0.01652904\n",
      "Validation score: 0.958500\n",
      "Iteration 30, loss = 0.01500313\n",
      "Validation score: 0.958625\n",
      "Iteration 31, loss = 0.01391419\n",
      "Validation score: 0.957000\n",
      "Iteration 32, loss = 0.01155579\n",
      "Validation score: 0.957375\n",
      "Iteration 33, loss = 0.01125649\n",
      "Validation score: 0.959250\n",
      "Iteration 34, loss = 0.00980402\n",
      "Validation score: 0.959625\n",
      "Iteration 35, loss = 0.00895648\n",
      "Validation score: 0.959750\n",
      "Iteration 36, loss = 0.00821393\n",
      "Validation score: 0.959000\n",
      "Iteration 37, loss = 0.00739043\n",
      "Validation score: 0.958750\n",
      "Iteration 38, loss = 0.00672900\n",
      "Validation score: 0.959500\n",
      "Iteration 39, loss = 0.00644951\n",
      "Validation score: 0.958125\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29623448\n",
      "Validation score: 0.206875\n",
      "Iteration 2, loss = 1.40828131\n",
      "Validation score: 0.713500\n",
      "Iteration 3, loss = 0.63961798\n",
      "Validation score: 0.879000\n",
      "Iteration 4, loss = 0.38240253\n",
      "Validation score: 0.899375\n",
      "Iteration 5, loss = 0.28091427\n",
      "Validation score: 0.928750\n",
      "Iteration 6, loss = 0.21928838\n",
      "Validation score: 0.933625\n",
      "Iteration 7, loss = 0.18583634\n",
      "Validation score: 0.939750\n",
      "Iteration 8, loss = 0.15251181\n",
      "Validation score: 0.949250\n",
      "Iteration 9, loss = 0.13231305\n",
      "Validation score: 0.947125\n",
      "Iteration 10, loss = 0.11570134\n",
      "Validation score: 0.954500\n",
      "Iteration 11, loss = 0.10213586\n",
      "Validation score: 0.954375\n",
      "Iteration 12, loss = 0.08905767\n",
      "Validation score: 0.950750\n",
      "Iteration 13, loss = 0.08117314\n",
      "Validation score: 0.957625\n",
      "Iteration 14, loss = 0.07439939\n",
      "Validation score: 0.956875\n",
      "Iteration 15, loss = 0.06874313\n",
      "Validation score: 0.956000\n",
      "Iteration 16, loss = 0.06084129\n",
      "Validation score: 0.955750\n",
      "Iteration 17, loss = 0.05369721\n",
      "Validation score: 0.955375\n",
      "Iteration 18, loss = 0.05045366\n",
      "Validation score: 0.957750\n",
      "Iteration 19, loss = 0.04403855\n",
      "Validation score: 0.955750\n",
      "Iteration 20, loss = 0.04190401\n",
      "Validation score: 0.955250\n",
      "Iteration 21, loss = 0.03785058\n",
      "Validation score: 0.957125\n",
      "Iteration 22, loss = 0.03397259\n",
      "Validation score: 0.957125\n",
      "Iteration 23, loss = 0.02938775\n",
      "Validation score: 0.955375\n",
      "Iteration 24, loss = 0.02767714\n",
      "Validation score: 0.959250\n",
      "Iteration 25, loss = 0.02882590\n",
      "Validation score: 0.956250\n",
      "Iteration 26, loss = 0.02557033\n",
      "Validation score: 0.956375\n",
      "Iteration 27, loss = 0.02153108\n",
      "Validation score: 0.957125\n",
      "Iteration 28, loss = 0.01994251\n",
      "Validation score: 0.957250\n",
      "Iteration 29, loss = 0.01798870\n",
      "Validation score: 0.954500\n",
      "Iteration 30, loss = 0.01670609\n",
      "Validation score: 0.954500\n",
      "Iteration 31, loss = 0.01573691\n",
      "Validation score: 0.958625\n",
      "Iteration 32, loss = 0.01620156\n",
      "Validation score: 0.959125\n",
      "Iteration 33, loss = 0.01408339\n",
      "Validation score: 0.958750\n",
      "Iteration 34, loss = 0.01216349\n",
      "Validation score: 0.958500\n",
      "Iteration 35, loss = 0.01056913\n",
      "Validation score: 0.958875\n",
      "Iteration 36, loss = 0.01012730\n",
      "Validation score: 0.960250\n",
      "Iteration 37, loss = 0.00978624\n",
      "Validation score: 0.958750\n",
      "Iteration 38, loss = 0.00889619\n",
      "Validation score: 0.959250\n",
      "Iteration 39, loss = 0.00845029\n",
      "Validation score: 0.959375\n",
      "Iteration 40, loss = 0.00771652\n",
      "Validation score: 0.959375\n",
      "Iteration 41, loss = 0.00733780\n",
      "Validation score: 0.958000\n",
      "Iteration 42, loss = 0.00703736\n",
      "Validation score: 0.959125\n",
      "Iteration 43, loss = 0.00656884\n",
      "Validation score: 0.959250\n",
      "Iteration 44, loss = 0.00631967\n",
      "Validation score: 0.959250\n",
      "Iteration 45, loss = 0.00622724\n",
      "Validation score: 0.960250\n",
      "Iteration 46, loss = 0.00591799\n",
      "Validation score: 0.959125\n",
      "Iteration 47, loss = 0.00578823\n",
      "Validation score: 0.960625\n",
      "Iteration 48, loss = 0.00559631\n",
      "Validation score: 0.957875\n",
      "Iteration 49, loss = 0.00538101\n",
      "Validation score: 0.958375\n",
      "Iteration 50, loss = 0.00520251\n",
      "Validation score: 0.958500\n",
      "Iteration 51, loss = 0.00498036\n",
      "Validation score: 0.958750\n",
      "Iteration 52, loss = 0.00477023\n",
      "Validation score: 0.957125\n",
      "Iteration 53, loss = 0.00457692\n",
      "Validation score: 0.958375\n",
      "Iteration 54, loss = 0.00443685\n",
      "Validation score: 0.957750\n",
      "Iteration 55, loss = 0.00427323\n",
      "Validation score: 0.957375\n",
      "Iteration 56, loss = 0.00417853\n",
      "Validation score: 0.958500\n",
      "Iteration 57, loss = 0.00401506\n",
      "Validation score: 0.958125\n",
      "Iteration 58, loss = 0.00383374\n",
      "Validation score: 0.957250\n",
      "Iteration 59, loss = 0.00378209\n",
      "Validation score: 0.958750\n",
      "Iteration 60, loss = 0.00361546\n",
      "Validation score: 0.958625\n",
      "Iteration 61, loss = 0.00354043\n",
      "Validation score: 0.958125\n",
      "Iteration 62, loss = 0.00342710\n",
      "Validation score: 0.957625\n",
      "Iteration 63, loss = 0.00339597\n",
      "Validation score: 0.958500\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25004838\n",
      "Validation score: 0.870375\n",
      "Iteration 2, loss = 0.35774340\n",
      "Validation score: 0.916625\n",
      "Iteration 3, loss = 0.25123224\n",
      "Validation score: 0.939000\n",
      "Iteration 4, loss = 0.19403018\n",
      "Validation score: 0.945125\n",
      "Iteration 5, loss = 0.15806113\n",
      "Validation score: 0.953000\n",
      "Iteration 6, loss = 0.13342429\n",
      "Validation score: 0.958125\n",
      "Iteration 7, loss = 0.11428605\n",
      "Validation score: 0.959000\n",
      "Iteration 8, loss = 0.09971862\n",
      "Validation score: 0.961000\n",
      "Iteration 9, loss = 0.08732514\n",
      "Validation score: 0.963875\n",
      "Iteration 10, loss = 0.07628705\n",
      "Validation score: 0.965875\n",
      "Iteration 11, loss = 0.06621229\n",
      "Validation score: 0.966250\n",
      "Iteration 12, loss = 0.05928988\n",
      "Validation score: 0.965250\n",
      "Iteration 13, loss = 0.05263239\n",
      "Validation score: 0.963875\n",
      "Iteration 14, loss = 0.04623861\n",
      "Validation score: 0.967500\n",
      "Iteration 15, loss = 0.04218259\n",
      "Validation score: 0.968000\n",
      "Iteration 16, loss = 0.03753630\n",
      "Validation score: 0.967250\n",
      "Iteration 17, loss = 0.03264184\n",
      "Validation score: 0.968250\n",
      "Iteration 18, loss = 0.02951987\n",
      "Validation score: 0.968250\n",
      "Iteration 19, loss = 0.02589084\n",
      "Validation score: 0.969750\n",
      "Iteration 20, loss = 0.02321826\n",
      "Validation score: 0.969000\n",
      "Iteration 21, loss = 0.02099900\n",
      "Validation score: 0.970000\n",
      "Iteration 22, loss = 0.01861588\n",
      "Validation score: 0.968875\n",
      "Iteration 23, loss = 0.01665395\n",
      "Validation score: 0.970250\n",
      "Iteration 24, loss = 0.01509014\n",
      "Validation score: 0.970250\n",
      "Iteration 25, loss = 0.01333303\n",
      "Validation score: 0.971625\n",
      "Iteration 26, loss = 0.01222641\n",
      "Validation score: 0.970500\n",
      "Iteration 27, loss = 0.01084782\n",
      "Validation score: 0.970000\n",
      "Iteration 28, loss = 0.00969602\n",
      "Validation score: 0.970125\n",
      "Iteration 29, loss = 0.00903839\n",
      "Validation score: 0.971375\n",
      "Iteration 30, loss = 0.00810871\n",
      "Validation score: 0.970625\n",
      "Iteration 31, loss = 0.00756247\n",
      "Validation score: 0.970500\n",
      "Iteration 32, loss = 0.00694491\n",
      "Validation score: 0.971125\n",
      "Iteration 33, loss = 0.00642757\n",
      "Validation score: 0.970500\n",
      "Iteration 34, loss = 0.00594208\n",
      "Validation score: 0.970125\n",
      "Iteration 35, loss = 0.00559851\n",
      "Validation score: 0.970500\n",
      "Iteration 36, loss = 0.00523823\n",
      "Validation score: 0.971750\n",
      "Iteration 37, loss = 0.00494423\n",
      "Validation score: 0.970125\n",
      "Iteration 38, loss = 0.00470573\n",
      "Validation score: 0.970625\n",
      "Iteration 39, loss = 0.00448589\n",
      "Validation score: 0.970000\n",
      "Iteration 40, loss = 0.00419752\n",
      "Validation score: 0.970750\n",
      "Iteration 41, loss = 0.00401739\n",
      "Validation score: 0.970125\n",
      "Iteration 42, loss = 0.00386514\n",
      "Validation score: 0.970625\n",
      "Iteration 43, loss = 0.00374526\n",
      "Validation score: 0.970125\n",
      "Iteration 44, loss = 0.00355138\n",
      "Validation score: 0.970375\n",
      "Iteration 45, loss = 0.00341876\n",
      "Validation score: 0.970250\n",
      "Iteration 46, loss = 0.00328498\n",
      "Validation score: 0.970250\n",
      "Iteration 47, loss = 0.00317348\n",
      "Validation score: 0.970125\n",
      "Iteration 48, loss = 0.00307259\n",
      "Validation score: 0.970875\n",
      "Iteration 49, loss = 0.00296812\n",
      "Validation score: 0.971000\n",
      "Iteration 50, loss = 0.00292098\n",
      "Validation score: 0.970500\n",
      "Iteration 51, loss = 0.00281076\n",
      "Validation score: 0.970750\n",
      "Iteration 52, loss = 0.00275170\n",
      "Validation score: 0.970750\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25605640\n",
      "Validation score: 0.874125\n",
      "Iteration 2, loss = 0.35483530\n",
      "Validation score: 0.916625\n",
      "Iteration 3, loss = 0.24851853\n",
      "Validation score: 0.939625\n",
      "Iteration 4, loss = 0.19187525\n",
      "Validation score: 0.947375\n",
      "Iteration 5, loss = 0.15483796\n",
      "Validation score: 0.952875\n",
      "Iteration 6, loss = 0.12994652\n",
      "Validation score: 0.957250\n",
      "Iteration 7, loss = 0.11214107\n",
      "Validation score: 0.961625\n",
      "Iteration 8, loss = 0.09670707\n",
      "Validation score: 0.963000\n",
      "Iteration 9, loss = 0.08438050\n",
      "Validation score: 0.962875\n",
      "Iteration 10, loss = 0.07461060\n",
      "Validation score: 0.966500\n",
      "Iteration 11, loss = 0.06599064\n",
      "Validation score: 0.967250\n",
      "Iteration 12, loss = 0.05798186\n",
      "Validation score: 0.965750\n",
      "Iteration 13, loss = 0.05186317\n",
      "Validation score: 0.965375\n",
      "Iteration 14, loss = 0.04578777\n",
      "Validation score: 0.964875\n",
      "Iteration 15, loss = 0.04067743\n",
      "Validation score: 0.967875\n",
      "Iteration 16, loss = 0.03565836\n",
      "Validation score: 0.967375\n",
      "Iteration 17, loss = 0.03151356\n",
      "Validation score: 0.969250\n",
      "Iteration 18, loss = 0.02783311\n",
      "Validation score: 0.967375\n",
      "Iteration 19, loss = 0.02516196\n",
      "Validation score: 0.967750\n",
      "Iteration 20, loss = 0.02295843\n",
      "Validation score: 0.968000\n",
      "Iteration 21, loss = 0.02003768\n",
      "Validation score: 0.968125\n",
      "Iteration 22, loss = 0.01750919\n",
      "Validation score: 0.970625\n",
      "Iteration 23, loss = 0.01599581\n",
      "Validation score: 0.969625\n",
      "Iteration 24, loss = 0.01401298\n",
      "Validation score: 0.967625\n",
      "Iteration 25, loss = 0.01275304\n",
      "Validation score: 0.969500\n",
      "Iteration 26, loss = 0.01157415\n",
      "Validation score: 0.970625\n",
      "Iteration 27, loss = 0.01035496\n",
      "Validation score: 0.969250\n",
      "Iteration 28, loss = 0.00960628\n",
      "Validation score: 0.969250\n",
      "Iteration 29, loss = 0.00885566\n",
      "Validation score: 0.970000\n",
      "Iteration 30, loss = 0.00786010\n",
      "Validation score: 0.970375\n",
      "Iteration 31, loss = 0.00743092\n",
      "Validation score: 0.969000\n",
      "Iteration 32, loss = 0.00664013\n",
      "Validation score: 0.969125\n",
      "Iteration 33, loss = 0.00633197\n",
      "Validation score: 0.969375\n",
      "Iteration 34, loss = 0.00587156\n",
      "Validation score: 0.970000\n",
      "Iteration 35, loss = 0.00565773\n",
      "Validation score: 0.969125\n",
      "Iteration 36, loss = 0.00512617\n",
      "Validation score: 0.970000\n",
      "Iteration 37, loss = 0.00478852\n",
      "Validation score: 0.970125\n",
      "Iteration 38, loss = 0.00456877\n",
      "Validation score: 0.969875\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25533956\n",
      "Validation score: 0.877500\n",
      "Iteration 2, loss = 0.35592700\n",
      "Validation score: 0.914750\n",
      "Iteration 3, loss = 0.25165797\n",
      "Validation score: 0.931250\n",
      "Iteration 4, loss = 0.19360282\n",
      "Validation score: 0.941500\n",
      "Iteration 5, loss = 0.15500126\n",
      "Validation score: 0.949000\n",
      "Iteration 6, loss = 0.12984159\n",
      "Validation score: 0.953500\n",
      "Iteration 7, loss = 0.10976829\n",
      "Validation score: 0.958625\n",
      "Iteration 8, loss = 0.09510105\n",
      "Validation score: 0.957000\n",
      "Iteration 9, loss = 0.08195366\n",
      "Validation score: 0.961875\n",
      "Iteration 10, loss = 0.07264503\n",
      "Validation score: 0.963750\n",
      "Iteration 11, loss = 0.06286042\n",
      "Validation score: 0.963250\n",
      "Iteration 12, loss = 0.05598049\n",
      "Validation score: 0.964500\n",
      "Iteration 13, loss = 0.04945108\n",
      "Validation score: 0.967000\n",
      "Iteration 14, loss = 0.04340172\n",
      "Validation score: 0.968000\n",
      "Iteration 15, loss = 0.03757211\n",
      "Validation score: 0.966500\n",
      "Iteration 16, loss = 0.03385555\n",
      "Validation score: 0.967125\n",
      "Iteration 17, loss = 0.02991840\n",
      "Validation score: 0.967625\n",
      "Iteration 18, loss = 0.02665291\n",
      "Validation score: 0.968625\n",
      "Iteration 19, loss = 0.02392164\n",
      "Validation score: 0.967375\n",
      "Iteration 20, loss = 0.02114921\n",
      "Validation score: 0.968500\n",
      "Iteration 21, loss = 0.01894174\n",
      "Validation score: 0.968750\n",
      "Iteration 22, loss = 0.01713397\n",
      "Validation score: 0.969625\n",
      "Iteration 23, loss = 0.01532901\n",
      "Validation score: 0.969000\n",
      "Iteration 24, loss = 0.01431839\n",
      "Validation score: 0.968875\n",
      "Iteration 25, loss = 0.01247755\n",
      "Validation score: 0.968625\n",
      "Iteration 26, loss = 0.01158586\n",
      "Validation score: 0.968875\n",
      "Iteration 27, loss = 0.01049049\n",
      "Validation score: 0.969500\n",
      "Iteration 28, loss = 0.00943235\n",
      "Validation score: 0.969500\n",
      "Iteration 29, loss = 0.00854704\n",
      "Validation score: 0.969500\n",
      "Iteration 30, loss = 0.00768104\n",
      "Validation score: 0.969250\n",
      "Iteration 31, loss = 0.00714342\n",
      "Validation score: 0.969875\n",
      "Iteration 32, loss = 0.00655585\n",
      "Validation score: 0.969625\n",
      "Iteration 33, loss = 0.00629528\n",
      "Validation score: 0.968875\n",
      "Iteration 34, loss = 0.00587449\n",
      "Validation score: 0.969125\n",
      "Iteration 35, loss = 0.00545219\n",
      "Validation score: 0.970250\n",
      "Iteration 36, loss = 0.00517753\n",
      "Validation score: 0.970375\n",
      "Iteration 37, loss = 0.00486945\n",
      "Validation score: 0.970000\n",
      "Iteration 38, loss = 0.00461310\n",
      "Validation score: 0.969500\n",
      "Iteration 39, loss = 0.00434139\n",
      "Validation score: 0.970250\n",
      "Iteration 40, loss = 0.00412981\n",
      "Validation score: 0.970250\n",
      "Iteration 41, loss = 0.00394599\n",
      "Validation score: 0.970000\n",
      "Iteration 42, loss = 0.00378272\n",
      "Validation score: 0.970125\n",
      "Iteration 43, loss = 0.00360582\n",
      "Validation score: 0.970750\n",
      "Iteration 44, loss = 0.00345700\n",
      "Validation score: 0.970375\n",
      "Iteration 45, loss = 0.00336335\n",
      "Validation score: 0.970500\n",
      "Iteration 46, loss = 0.00322572\n",
      "Validation score: 0.970375\n",
      "Iteration 47, loss = 0.00314298\n",
      "Validation score: 0.970875\n",
      "Iteration 48, loss = 0.00305083\n",
      "Validation score: 0.970000\n",
      "Iteration 49, loss = 0.00294914\n",
      "Validation score: 0.970500\n",
      "Iteration 50, loss = 0.00287578\n",
      "Validation score: 0.970625\n",
      "Iteration 51, loss = 0.00277826\n",
      "Validation score: 0.970625\n",
      "Iteration 52, loss = 0.00270275\n",
      "Validation score: 0.970750\n",
      "Iteration 53, loss = 0.00264977\n",
      "Validation score: 0.970625\n",
      "Iteration 54, loss = 0.00258061\n",
      "Validation score: 0.970750\n",
      "Iteration 55, loss = 0.00252938\n",
      "Validation score: 0.970500\n",
      "Iteration 56, loss = 0.00247422\n",
      "Validation score: 0.970500\n",
      "Iteration 57, loss = 0.00242017\n",
      "Validation score: 0.970750\n",
      "Iteration 58, loss = 0.00237028\n",
      "Validation score: 0.970625\n",
      "Iteration 59, loss = 0.00233283\n",
      "Validation score: 0.970500\n",
      "Iteration 60, loss = 0.00228675\n",
      "Validation score: 0.970625\n",
      "Iteration 61, loss = 0.00225756\n",
      "Validation score: 0.970750\n",
      "Iteration 62, loss = 0.00220740\n",
      "Validation score: 0.970875\n",
      "Iteration 63, loss = 0.00216620\n",
      "Validation score: 0.970125\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.44861429\n",
      "Validation score: 0.834750\n",
      "Iteration 2, loss = 0.45400947\n",
      "Validation score: 0.907125\n",
      "Iteration 3, loss = 0.29671963\n",
      "Validation score: 0.925375\n",
      "Iteration 4, loss = 0.23584686\n",
      "Validation score: 0.932125\n",
      "Iteration 5, loss = 0.20243723\n",
      "Validation score: 0.934875\n",
      "Iteration 6, loss = 0.18208477\n",
      "Validation score: 0.940250\n",
      "Iteration 7, loss = 0.16575669\n",
      "Validation score: 0.937000\n",
      "Iteration 8, loss = 0.15513238\n",
      "Validation score: 0.944125\n",
      "Iteration 9, loss = 0.14360079\n",
      "Validation score: 0.945000\n",
      "Iteration 10, loss = 0.13537650\n",
      "Validation score: 0.944625\n",
      "Iteration 11, loss = 0.12860397\n",
      "Validation score: 0.946250\n",
      "Iteration 12, loss = 0.12434785\n",
      "Validation score: 0.944875\n",
      "Iteration 13, loss = 0.11884807\n",
      "Validation score: 0.945500\n",
      "Iteration 14, loss = 0.11337313\n",
      "Validation score: 0.944625\n",
      "Iteration 15, loss = 0.10758021\n",
      "Validation score: 0.947125\n",
      "Iteration 16, loss = 0.10316391\n",
      "Validation score: 0.941750\n",
      "Iteration 17, loss = 0.09912930\n",
      "Validation score: 0.948875\n",
      "Iteration 18, loss = 0.09763391\n",
      "Validation score: 0.949125\n",
      "Iteration 19, loss = 0.09196617\n",
      "Validation score: 0.948875\n",
      "Iteration 20, loss = 0.08959127\n",
      "Validation score: 0.948125\n",
      "Iteration 21, loss = 0.08614341\n",
      "Validation score: 0.946000\n",
      "Iteration 22, loss = 0.08282228\n",
      "Validation score: 0.947250\n",
      "Iteration 23, loss = 0.08173424\n",
      "Validation score: 0.949875\n",
      "Iteration 24, loss = 0.07731990\n",
      "Validation score: 0.947500\n",
      "Iteration 25, loss = 0.07563116\n",
      "Validation score: 0.950125\n",
      "Iteration 26, loss = 0.07401954\n",
      "Validation score: 0.948000\n",
      "Iteration 27, loss = 0.06987154\n",
      "Validation score: 0.944000\n",
      "Iteration 28, loss = 0.06968673\n",
      "Validation score: 0.945125\n",
      "Iteration 29, loss = 0.06865731\n",
      "Validation score: 0.947750\n",
      "Iteration 30, loss = 0.06482154\n",
      "Validation score: 0.947750\n",
      "Iteration 31, loss = 0.06129611\n",
      "Validation score: 0.947125\n",
      "Iteration 32, loss = 0.06093057\n",
      "Validation score: 0.947750\n",
      "Iteration 33, loss = 0.05923256\n",
      "Validation score: 0.946500\n",
      "Iteration 34, loss = 0.05826220\n",
      "Validation score: 0.946250\n",
      "Iteration 35, loss = 0.05570468\n",
      "Validation score: 0.946250\n",
      "Iteration 36, loss = 0.05377724\n",
      "Validation score: 0.946625\n",
      "Iteration 37, loss = 0.05282190\n",
      "Validation score: 0.945375\n",
      "Iteration 38, loss = 0.05148026\n",
      "Validation score: 0.945125\n",
      "Iteration 39, loss = 0.05118253\n",
      "Validation score: 0.944000\n",
      "Iteration 40, loss = 0.04932860\n",
      "Validation score: 0.943625\n",
      "Iteration 41, loss = 0.04809377\n",
      "Validation score: 0.945125\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.44231547\n",
      "Validation score: 0.829625\n",
      "Iteration 2, loss = 0.44539823\n",
      "Validation score: 0.902000\n",
      "Iteration 3, loss = 0.29103481\n",
      "Validation score: 0.924500\n",
      "Iteration 4, loss = 0.23014854\n",
      "Validation score: 0.932625\n",
      "Iteration 5, loss = 0.19755218\n",
      "Validation score: 0.938375\n",
      "Iteration 6, loss = 0.17621447\n",
      "Validation score: 0.937375\n",
      "Iteration 7, loss = 0.15919773\n",
      "Validation score: 0.939750\n",
      "Iteration 8, loss = 0.14680324\n",
      "Validation score: 0.947000\n",
      "Iteration 9, loss = 0.13668779\n",
      "Validation score: 0.946250\n",
      "Iteration 10, loss = 0.12809190\n",
      "Validation score: 0.947500\n",
      "Iteration 11, loss = 0.12184017\n",
      "Validation score: 0.944000\n",
      "Iteration 12, loss = 0.11583307\n",
      "Validation score: 0.946125\n",
      "Iteration 13, loss = 0.11139002\n",
      "Validation score: 0.949875\n",
      "Iteration 14, loss = 0.10557844\n",
      "Validation score: 0.948125\n",
      "Iteration 15, loss = 0.10075286\n",
      "Validation score: 0.950250\n",
      "Iteration 16, loss = 0.09691761\n",
      "Validation score: 0.946500\n",
      "Iteration 17, loss = 0.09094979\n",
      "Validation score: 0.949875\n",
      "Iteration 18, loss = 0.09024052\n",
      "Validation score: 0.948750\n",
      "Iteration 19, loss = 0.08718327\n",
      "Validation score: 0.948875\n",
      "Iteration 20, loss = 0.08403476\n",
      "Validation score: 0.950625\n",
      "Iteration 21, loss = 0.08128810\n",
      "Validation score: 0.950875\n",
      "Iteration 22, loss = 0.07800081\n",
      "Validation score: 0.950375\n",
      "Iteration 23, loss = 0.07483787\n",
      "Validation score: 0.946750\n",
      "Iteration 24, loss = 0.07301351\n",
      "Validation score: 0.949125\n",
      "Iteration 25, loss = 0.06990771\n",
      "Validation score: 0.948000\n",
      "Iteration 26, loss = 0.06765391\n",
      "Validation score: 0.949625\n",
      "Iteration 27, loss = 0.06521952\n",
      "Validation score: 0.949750\n",
      "Iteration 28, loss = 0.06479118\n",
      "Validation score: 0.947750\n",
      "Iteration 29, loss = 0.06279482\n",
      "Validation score: 0.946625\n",
      "Iteration 30, loss = 0.06071995\n",
      "Validation score: 0.947000\n",
      "Iteration 31, loss = 0.05927707\n",
      "Validation score: 0.948375\n",
      "Iteration 32, loss = 0.05717345\n",
      "Validation score: 0.949250\n",
      "Iteration 33, loss = 0.05484196\n",
      "Validation score: 0.947250\n",
      "Iteration 34, loss = 0.05360083\n",
      "Validation score: 0.949500\n",
      "Iteration 35, loss = 0.05171743\n",
      "Validation score: 0.945750\n",
      "Iteration 36, loss = 0.05147321\n",
      "Validation score: 0.948000\n",
      "Iteration 37, loss = 0.05078275\n",
      "Validation score: 0.945625\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.45125551\n",
      "Validation score: 0.825375\n",
      "Iteration 2, loss = 0.45453478\n",
      "Validation score: 0.903125\n",
      "Iteration 3, loss = 0.29627864\n",
      "Validation score: 0.918875\n",
      "Iteration 4, loss = 0.23547955\n",
      "Validation score: 0.927375\n",
      "Iteration 5, loss = 0.20128851\n",
      "Validation score: 0.938125\n",
      "Iteration 6, loss = 0.17697755\n",
      "Validation score: 0.935000\n",
      "Iteration 7, loss = 0.16020349\n",
      "Validation score: 0.938250\n",
      "Iteration 8, loss = 0.14811892\n",
      "Validation score: 0.944125\n",
      "Iteration 9, loss = 0.13531017\n",
      "Validation score: 0.943500\n",
      "Iteration 10, loss = 0.12739850\n",
      "Validation score: 0.944625\n",
      "Iteration 11, loss = 0.11984263\n",
      "Validation score: 0.946000\n",
      "Iteration 12, loss = 0.11372237\n",
      "Validation score: 0.943125\n",
      "Iteration 13, loss = 0.10845821\n",
      "Validation score: 0.944750\n",
      "Iteration 14, loss = 0.10300043\n",
      "Validation score: 0.945375\n",
      "Iteration 15, loss = 0.09744723\n",
      "Validation score: 0.948500\n",
      "Iteration 16, loss = 0.09374575\n",
      "Validation score: 0.947375\n",
      "Iteration 17, loss = 0.09029868\n",
      "Validation score: 0.945875\n",
      "Iteration 18, loss = 0.08622999\n",
      "Validation score: 0.946500\n",
      "Iteration 19, loss = 0.08210350\n",
      "Validation score: 0.948250\n",
      "Iteration 20, loss = 0.07954455\n",
      "Validation score: 0.946750\n",
      "Iteration 21, loss = 0.07652653\n",
      "Validation score: 0.948625\n",
      "Iteration 22, loss = 0.07307907\n",
      "Validation score: 0.947250\n",
      "Iteration 23, loss = 0.06838706\n",
      "Validation score: 0.948000\n",
      "Iteration 24, loss = 0.06707282\n",
      "Validation score: 0.949125\n",
      "Iteration 25, loss = 0.06649512\n",
      "Validation score: 0.947375\n",
      "Iteration 26, loss = 0.06442101\n",
      "Validation score: 0.949625\n",
      "Iteration 27, loss = 0.06053772\n",
      "Validation score: 0.946375\n",
      "Iteration 28, loss = 0.06042400\n",
      "Validation score: 0.947125\n",
      "Iteration 29, loss = 0.05707542\n",
      "Validation score: 0.948625\n",
      "Iteration 30, loss = 0.05577309\n",
      "Validation score: 0.944125\n",
      "Iteration 31, loss = 0.05351540\n",
      "Validation score: 0.945500\n",
      "Iteration 32, loss = 0.05166991\n",
      "Validation score: 0.946000\n",
      "Iteration 33, loss = 0.04970824\n",
      "Validation score: 0.948000\n",
      "Iteration 34, loss = 0.04979367\n",
      "Validation score: 0.946375\n",
      "Iteration 35, loss = 0.04722579\n",
      "Validation score: 0.945875\n",
      "Iteration 36, loss = 0.04635082\n",
      "Validation score: 0.946375\n",
      "Iteration 37, loss = 0.04552647\n",
      "Validation score: 0.943875\n",
      "Iteration 38, loss = 0.04445820\n",
      "Validation score: 0.947375\n",
      "Iteration 39, loss = 0.04377779\n",
      "Validation score: 0.945875\n",
      "Iteration 40, loss = 0.04144628\n",
      "Validation score: 0.945625\n",
      "Iteration 41, loss = 0.03984038\n",
      "Validation score: 0.943000\n",
      "Iteration 42, loss = 0.03973939\n",
      "Validation score: 0.945500\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30750771\n",
      "Validation score: 0.112875\n",
      "Iteration 2, loss = 1.58435738\n",
      "Validation score: 0.659625\n",
      "Iteration 3, loss = 0.81000793\n",
      "Validation score: 0.862125\n",
      "Iteration 4, loss = 0.52333155\n",
      "Validation score: 0.900750\n",
      "Iteration 5, loss = 0.42587881\n",
      "Validation score: 0.910375\n",
      "Iteration 6, loss = 0.37724504\n",
      "Validation score: 0.930875\n",
      "Iteration 7, loss = 0.34573985\n",
      "Validation score: 0.941000\n",
      "Iteration 8, loss = 0.32505375\n",
      "Validation score: 0.941625\n",
      "Iteration 9, loss = 0.31077733\n",
      "Validation score: 0.946750\n",
      "Iteration 10, loss = 0.29825170\n",
      "Validation score: 0.944875\n",
      "Iteration 11, loss = 0.28523200\n",
      "Validation score: 0.952875\n",
      "Iteration 12, loss = 0.27668331\n",
      "Validation score: 0.952875\n",
      "Iteration 13, loss = 0.27339272\n",
      "Validation score: 0.955875\n",
      "Iteration 14, loss = 0.26515498\n",
      "Validation score: 0.957375\n",
      "Iteration 15, loss = 0.26211788\n",
      "Validation score: 0.954125\n",
      "Iteration 16, loss = 0.25635681\n",
      "Validation score: 0.954875\n",
      "Iteration 17, loss = 0.25280334\n",
      "Validation score: 0.959500\n",
      "Iteration 18, loss = 0.24809308\n",
      "Validation score: 0.959250\n",
      "Iteration 19, loss = 0.24627253\n",
      "Validation score: 0.960000\n",
      "Iteration 20, loss = 0.24576493\n",
      "Validation score: 0.961750\n",
      "Iteration 21, loss = 0.24067592\n",
      "Validation score: 0.961500\n",
      "Iteration 22, loss = 0.23888342\n",
      "Validation score: 0.963875\n",
      "Iteration 23, loss = 0.23805125\n",
      "Validation score: 0.964500\n",
      "Iteration 24, loss = 0.23531448\n",
      "Validation score: 0.964125\n",
      "Iteration 25, loss = 0.23311867\n",
      "Validation score: 0.959500\n",
      "Iteration 26, loss = 0.23300968\n",
      "Validation score: 0.963125\n",
      "Iteration 27, loss = 0.23109222\n",
      "Validation score: 0.961750\n",
      "Iteration 28, loss = 0.23027072\n",
      "Validation score: 0.959625\n",
      "Iteration 29, loss = 0.22908911\n",
      "Validation score: 0.963375\n",
      "Iteration 30, loss = 0.22946506\n",
      "Validation score: 0.962875\n",
      "Iteration 31, loss = 0.22744950\n",
      "Validation score: 0.963250\n",
      "Iteration 32, loss = 0.22574534\n",
      "Validation score: 0.962625\n",
      "Iteration 33, loss = 0.22531847\n",
      "Validation score: 0.959875\n",
      "Iteration 34, loss = 0.22447346\n",
      "Validation score: 0.964375\n",
      "Iteration 35, loss = 0.22458395\n",
      "Validation score: 0.963250\n",
      "Iteration 36, loss = 0.22446846\n",
      "Validation score: 0.963875\n",
      "Iteration 37, loss = 0.22259102\n",
      "Validation score: 0.960125\n",
      "Iteration 38, loss = 0.22225501\n",
      "Validation score: 0.962375\n",
      "Iteration 39, loss = 0.22207400\n",
      "Validation score: 0.966000\n",
      "Iteration 40, loss = 0.22035781\n",
      "Validation score: 0.962750\n",
      "Iteration 41, loss = 0.22310718\n",
      "Validation score: 0.961250\n",
      "Iteration 42, loss = 0.22015480\n",
      "Validation score: 0.965750\n",
      "Iteration 43, loss = 0.21901713\n",
      "Validation score: 0.963500\n",
      "Iteration 44, loss = 0.22108274\n",
      "Validation score: 0.960750\n",
      "Iteration 45, loss = 0.21855280\n",
      "Validation score: 0.966500\n",
      "Iteration 46, loss = 0.21800922\n",
      "Validation score: 0.964375\n",
      "Iteration 47, loss = 0.21945032\n",
      "Validation score: 0.964250\n",
      "Iteration 48, loss = 0.21742169\n",
      "Validation score: 0.962000\n",
      "Iteration 49, loss = 0.21771755\n",
      "Validation score: 0.964500\n",
      "Iteration 50, loss = 0.21597575\n",
      "Validation score: 0.961750\n",
      "Iteration 51, loss = 0.21633033\n",
      "Validation score: 0.964125\n",
      "Iteration 52, loss = 0.21489022\n",
      "Validation score: 0.961250\n",
      "Iteration 53, loss = 0.21584178\n",
      "Validation score: 0.964500\n",
      "Iteration 54, loss = 0.21705353\n",
      "Validation score: 0.963875\n",
      "Iteration 55, loss = 0.21472849\n",
      "Validation score: 0.964875\n",
      "Iteration 56, loss = 0.21360936\n",
      "Validation score: 0.961875\n",
      "Iteration 57, loss = 0.21310787\n",
      "Validation score: 0.963250\n",
      "Iteration 58, loss = 0.21287429\n",
      "Validation score: 0.965625\n",
      "Iteration 59, loss = 0.21383701\n",
      "Validation score: 0.964500\n",
      "Iteration 60, loss = 0.21243493\n",
      "Validation score: 0.966000\n",
      "Iteration 61, loss = 0.21325169\n",
      "Validation score: 0.963625\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30785909\n",
      "Validation score: 0.194625\n",
      "Iteration 2, loss = 1.56962230\n",
      "Validation score: 0.645625\n",
      "Iteration 3, loss = 0.79965849\n",
      "Validation score: 0.864375\n",
      "Iteration 4, loss = 0.52276136\n",
      "Validation score: 0.907125\n",
      "Iteration 5, loss = 0.42197488\n",
      "Validation score: 0.921250\n",
      "Iteration 6, loss = 0.36749633\n",
      "Validation score: 0.931375\n",
      "Iteration 7, loss = 0.34197215\n",
      "Validation score: 0.938500\n",
      "Iteration 8, loss = 0.31688954\n",
      "Validation score: 0.946250\n",
      "Iteration 9, loss = 0.30498923\n",
      "Validation score: 0.951250\n",
      "Iteration 10, loss = 0.29094697\n",
      "Validation score: 0.949375\n",
      "Iteration 11, loss = 0.28253019\n",
      "Validation score: 0.955750\n",
      "Iteration 12, loss = 0.27266918\n",
      "Validation score: 0.955875\n",
      "Iteration 13, loss = 0.26762456\n",
      "Validation score: 0.956750\n",
      "Iteration 14, loss = 0.26154866\n",
      "Validation score: 0.956625\n",
      "Iteration 15, loss = 0.25768574\n",
      "Validation score: 0.957875\n",
      "Iteration 16, loss = 0.25322133\n",
      "Validation score: 0.957125\n",
      "Iteration 17, loss = 0.25013437\n",
      "Validation score: 0.961500\n",
      "Iteration 18, loss = 0.24673911\n",
      "Validation score: 0.957625\n",
      "Iteration 19, loss = 0.24467265\n",
      "Validation score: 0.959375\n",
      "Iteration 20, loss = 0.24230962\n",
      "Validation score: 0.959375\n",
      "Iteration 21, loss = 0.23922406\n",
      "Validation score: 0.961375\n",
      "Iteration 22, loss = 0.23676727\n",
      "Validation score: 0.965625\n",
      "Iteration 23, loss = 0.23526211\n",
      "Validation score: 0.955875\n",
      "Iteration 24, loss = 0.23379736\n",
      "Validation score: 0.965625\n",
      "Iteration 25, loss = 0.23187533\n",
      "Validation score: 0.966000\n",
      "Iteration 26, loss = 0.23015128\n",
      "Validation score: 0.964375\n",
      "Iteration 27, loss = 0.22922517\n",
      "Validation score: 0.961750\n",
      "Iteration 28, loss = 0.22681409\n",
      "Validation score: 0.963375\n",
      "Iteration 29, loss = 0.22778207\n",
      "Validation score: 0.964000\n",
      "Iteration 30, loss = 0.22607917\n",
      "Validation score: 0.965625\n",
      "Iteration 31, loss = 0.22432033\n",
      "Validation score: 0.966250\n",
      "Iteration 32, loss = 0.22340474\n",
      "Validation score: 0.966125\n",
      "Iteration 33, loss = 0.22316571\n",
      "Validation score: 0.962625\n",
      "Iteration 34, loss = 0.22064900\n",
      "Validation score: 0.965625\n",
      "Iteration 35, loss = 0.22078362\n",
      "Validation score: 0.967125\n",
      "Iteration 36, loss = 0.22015776\n",
      "Validation score: 0.965625\n",
      "Iteration 37, loss = 0.21957117\n",
      "Validation score: 0.965000\n",
      "Iteration 38, loss = 0.21978169\n",
      "Validation score: 0.965125\n",
      "Iteration 39, loss = 0.21884016\n",
      "Validation score: 0.963875\n",
      "Iteration 40, loss = 0.21751855\n",
      "Validation score: 0.966125\n",
      "Iteration 41, loss = 0.21720148\n",
      "Validation score: 0.964750\n",
      "Iteration 42, loss = 0.21812874\n",
      "Validation score: 0.967625\n",
      "Iteration 43, loss = 0.21643320\n",
      "Validation score: 0.967000\n",
      "Iteration 44, loss = 0.21648689\n",
      "Validation score: 0.965875\n",
      "Iteration 45, loss = 0.21590967\n",
      "Validation score: 0.965750\n",
      "Iteration 46, loss = 0.21473295\n",
      "Validation score: 0.968750\n",
      "Iteration 47, loss = 0.21569887\n",
      "Validation score: 0.963250\n",
      "Iteration 48, loss = 0.21406680\n",
      "Validation score: 0.962875\n",
      "Iteration 49, loss = 0.21308472\n",
      "Validation score: 0.965500\n",
      "Iteration 50, loss = 0.21309585\n",
      "Validation score: 0.966250\n",
      "Iteration 51, loss = 0.21389149\n",
      "Validation score: 0.966750\n",
      "Iteration 52, loss = 0.21197077\n",
      "Validation score: 0.966500\n",
      "Iteration 53, loss = 0.21327595\n",
      "Validation score: 0.964750\n",
      "Iteration 54, loss = 0.21166855\n",
      "Validation score: 0.964375\n",
      "Iteration 55, loss = 0.21288447\n",
      "Validation score: 0.967625\n",
      "Iteration 56, loss = 0.21180416\n",
      "Validation score: 0.965625\n",
      "Iteration 57, loss = 0.21096188\n",
      "Validation score: 0.966375\n",
      "Iteration 58, loss = 0.21025798\n",
      "Validation score: 0.965500\n",
      "Iteration 59, loss = 0.21127277\n",
      "Validation score: 0.965375\n",
      "Iteration 60, loss = 0.21125752\n",
      "Validation score: 0.967250\n",
      "Iteration 61, loss = 0.21020605\n",
      "Validation score: 0.966250\n",
      "Iteration 62, loss = 0.21131601\n",
      "Validation score: 0.966375\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30715564\n",
      "Validation score: 0.206625\n",
      "Iteration 2, loss = 1.55814285\n",
      "Validation score: 0.658500\n",
      "Iteration 3, loss = 0.80775779\n",
      "Validation score: 0.863875\n",
      "Iteration 4, loss = 0.52200270\n",
      "Validation score: 0.890250\n",
      "Iteration 5, loss = 0.42618927\n",
      "Validation score: 0.923000\n",
      "Iteration 6, loss = 0.37350982\n",
      "Validation score: 0.927875\n",
      "Iteration 7, loss = 0.34608844\n",
      "Validation score: 0.938250\n",
      "Iteration 8, loss = 0.31878456\n",
      "Validation score: 0.944750\n",
      "Iteration 9, loss = 0.30318703\n",
      "Validation score: 0.947375\n",
      "Iteration 10, loss = 0.29121331\n",
      "Validation score: 0.949625\n",
      "Iteration 11, loss = 0.28164463\n",
      "Validation score: 0.952125\n",
      "Iteration 12, loss = 0.27294042\n",
      "Validation score: 0.952875\n",
      "Iteration 13, loss = 0.26657673\n",
      "Validation score: 0.957125\n",
      "Iteration 14, loss = 0.26251891\n",
      "Validation score: 0.956625\n",
      "Iteration 15, loss = 0.25890186\n",
      "Validation score: 0.958000\n",
      "Iteration 16, loss = 0.25475686\n",
      "Validation score: 0.956500\n",
      "Iteration 17, loss = 0.25124306\n",
      "Validation score: 0.960500\n",
      "Iteration 18, loss = 0.24843462\n",
      "Validation score: 0.959625\n",
      "Iteration 19, loss = 0.24457031\n",
      "Validation score: 0.959000\n",
      "Iteration 20, loss = 0.24254859\n",
      "Validation score: 0.959875\n",
      "Iteration 21, loss = 0.24126821\n",
      "Validation score: 0.957875\n",
      "Iteration 22, loss = 0.24265263\n",
      "Validation score: 0.961125\n",
      "Iteration 23, loss = 0.23845005\n",
      "Validation score: 0.962625\n",
      "Iteration 24, loss = 0.23634729\n",
      "Validation score: 0.961625\n",
      "Iteration 25, loss = 0.23491600\n",
      "Validation score: 0.961000\n",
      "Iteration 26, loss = 0.23291582\n",
      "Validation score: 0.963125\n",
      "Iteration 27, loss = 0.23285928\n",
      "Validation score: 0.961875\n",
      "Iteration 28, loss = 0.22944107\n",
      "Validation score: 0.961500\n",
      "Iteration 29, loss = 0.22900940\n",
      "Validation score: 0.963375\n",
      "Iteration 30, loss = 0.22835915\n",
      "Validation score: 0.964000\n",
      "Iteration 31, loss = 0.22726779\n",
      "Validation score: 0.963750\n",
      "Iteration 32, loss = 0.22683023\n",
      "Validation score: 0.962250\n",
      "Iteration 33, loss = 0.22683711\n",
      "Validation score: 0.963250\n",
      "Iteration 34, loss = 0.22565882\n",
      "Validation score: 0.963000\n",
      "Iteration 35, loss = 0.22504656\n",
      "Validation score: 0.963750\n",
      "Iteration 36, loss = 0.22390276\n",
      "Validation score: 0.963875\n",
      "Iteration 37, loss = 0.22433080\n",
      "Validation score: 0.963875\n",
      "Iteration 38, loss = 0.22344253\n",
      "Validation score: 0.966000\n",
      "Iteration 39, loss = 0.22149548\n",
      "Validation score: 0.965750\n",
      "Iteration 40, loss = 0.22280063\n",
      "Validation score: 0.964500\n",
      "Iteration 41, loss = 0.22192148\n",
      "Validation score: 0.964250\n",
      "Iteration 42, loss = 0.22161828\n",
      "Validation score: 0.965125\n",
      "Iteration 43, loss = 0.21892057\n",
      "Validation score: 0.964375\n",
      "Iteration 44, loss = 0.21985538\n",
      "Validation score: 0.965750\n",
      "Iteration 45, loss = 0.21971127\n",
      "Validation score: 0.966625\n",
      "Iteration 46, loss = 0.21871305\n",
      "Validation score: 0.964875\n",
      "Iteration 47, loss = 0.21853800\n",
      "Validation score: 0.965375\n",
      "Iteration 48, loss = 0.21874491\n",
      "Validation score: 0.966250\n",
      "Iteration 49, loss = 0.21828546\n",
      "Validation score: 0.966000\n",
      "Iteration 50, loss = 0.21710573\n",
      "Validation score: 0.966625\n",
      "Iteration 51, loss = 0.21722120\n",
      "Validation score: 0.965500\n",
      "Iteration 52, loss = 0.21644691\n",
      "Validation score: 0.966250\n",
      "Iteration 53, loss = 0.21623852\n",
      "Validation score: 0.961875\n",
      "Iteration 54, loss = 0.21570496\n",
      "Validation score: 0.964375\n",
      "Iteration 55, loss = 0.21599850\n",
      "Validation score: 0.966875\n",
      "Iteration 56, loss = 0.21535821\n",
      "Validation score: 0.965000\n",
      "Iteration 57, loss = 0.21474210\n",
      "Validation score: 0.962500\n",
      "Iteration 58, loss = 0.21396163\n",
      "Validation score: 0.968125\n",
      "Iteration 59, loss = 0.21551494\n",
      "Validation score: 0.965000\n",
      "Iteration 60, loss = 0.21387604\n",
      "Validation score: 0.967125\n",
      "Iteration 61, loss = 0.21264855\n",
      "Validation score: 0.963750\n",
      "Iteration 62, loss = 0.21282982\n",
      "Validation score: 0.962625\n",
      "Iteration 63, loss = 0.21274334\n",
      "Validation score: 0.965000\n",
      "Iteration 64, loss = 0.21411229\n",
      "Validation score: 0.967750\n",
      "Iteration 65, loss = 0.21189539\n",
      "Validation score: 0.966250\n",
      "Iteration 66, loss = 0.21331182\n",
      "Validation score: 0.964750\n",
      "Iteration 67, loss = 0.21189612\n",
      "Validation score: 0.964750\n",
      "Iteration 68, loss = 0.21242056\n",
      "Validation score: 0.967500\n",
      "Iteration 69, loss = 0.21131172\n",
      "Validation score: 0.965125\n",
      "Iteration 70, loss = 0.21256116\n",
      "Validation score: 0.967625\n",
      "Iteration 71, loss = 0.20997570\n",
      "Validation score: 0.966000\n",
      "Iteration 72, loss = 0.21145256\n",
      "Validation score: 0.969625\n",
      "Iteration 73, loss = 0.21076388\n",
      "Validation score: 0.968125\n",
      "Iteration 74, loss = 0.21095851\n",
      "Validation score: 0.964750\n",
      "Iteration 75, loss = 0.20920442\n",
      "Validation score: 0.966500\n",
      "Iteration 76, loss = 0.20923857\n",
      "Validation score: 0.966750\n",
      "Iteration 77, loss = 0.20959283\n",
      "Validation score: 0.968250\n",
      "Iteration 78, loss = 0.20907642\n",
      "Validation score: 0.966500\n",
      "Iteration 79, loss = 0.20986096\n",
      "Validation score: 0.966625\n",
      "Iteration 80, loss = 0.20983238\n",
      "Validation score: 0.967375\n",
      "Iteration 81, loss = 0.20826164\n",
      "Validation score: 0.966500\n",
      "Iteration 82, loss = 0.20920296\n",
      "Validation score: 0.969000\n",
      "Iteration 83, loss = 0.21083356\n",
      "Validation score: 0.965750\n",
      "Iteration 84, loss = 0.20902913\n",
      "Validation score: 0.964500\n",
      "Iteration 85, loss = 0.20800567\n",
      "Validation score: 0.966625\n",
      "Iteration 86, loss = 0.20783439\n",
      "Validation score: 0.965250\n",
      "Iteration 87, loss = 0.20680103\n",
      "Validation score: 0.967625\n",
      "Iteration 88, loss = 0.20833217\n",
      "Validation score: 0.966500\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.28888444\n",
      "Validation score: 0.868875\n",
      "Iteration 2, loss = 0.43089035\n",
      "Validation score: 0.914000\n",
      "Iteration 3, loss = 0.34185502\n",
      "Validation score: 0.935375\n",
      "Iteration 4, loss = 0.29720880\n",
      "Validation score: 0.942500\n",
      "Iteration 5, loss = 0.27223318\n",
      "Validation score: 0.949125\n",
      "Iteration 6, loss = 0.25659193\n",
      "Validation score: 0.954875\n",
      "Iteration 7, loss = 0.24508710\n",
      "Validation score: 0.956375\n",
      "Iteration 8, loss = 0.23680775\n",
      "Validation score: 0.958875\n",
      "Iteration 9, loss = 0.23000316\n",
      "Validation score: 0.959500\n",
      "Iteration 10, loss = 0.22432243\n",
      "Validation score: 0.962250\n",
      "Iteration 11, loss = 0.21885001\n",
      "Validation score: 0.965125\n",
      "Iteration 12, loss = 0.21542990\n",
      "Validation score: 0.963000\n",
      "Iteration 13, loss = 0.21242175\n",
      "Validation score: 0.964125\n",
      "Iteration 14, loss = 0.20948912\n",
      "Validation score: 0.966500\n",
      "Iteration 15, loss = 0.20761422\n",
      "Validation score: 0.963875\n",
      "Iteration 16, loss = 0.20572323\n",
      "Validation score: 0.969250\n",
      "Iteration 17, loss = 0.20319275\n",
      "Validation score: 0.968000\n",
      "Iteration 18, loss = 0.20256583\n",
      "Validation score: 0.968750\n",
      "Iteration 19, loss = 0.20017422\n",
      "Validation score: 0.968500\n",
      "Iteration 20, loss = 0.19924172\n",
      "Validation score: 0.967125\n",
      "Iteration 21, loss = 0.19870923\n",
      "Validation score: 0.968625\n",
      "Iteration 22, loss = 0.19817829\n",
      "Validation score: 0.969125\n",
      "Iteration 23, loss = 0.19669287\n",
      "Validation score: 0.968250\n",
      "Iteration 24, loss = 0.19657920\n",
      "Validation score: 0.970000\n",
      "Iteration 25, loss = 0.19561076\n",
      "Validation score: 0.969250\n",
      "Iteration 26, loss = 0.19486232\n",
      "Validation score: 0.968375\n",
      "Iteration 27, loss = 0.19410583\n",
      "Validation score: 0.970875\n",
      "Iteration 28, loss = 0.19299158\n",
      "Validation score: 0.969500\n",
      "Iteration 29, loss = 0.19306764\n",
      "Validation score: 0.969500\n",
      "Iteration 30, loss = 0.19270093\n",
      "Validation score: 0.970250\n",
      "Iteration 31, loss = 0.19173757\n",
      "Validation score: 0.970375\n",
      "Iteration 32, loss = 0.19164143\n",
      "Validation score: 0.971000\n",
      "Iteration 33, loss = 0.19133308\n",
      "Validation score: 0.969750\n",
      "Iteration 34, loss = 0.19090653\n",
      "Validation score: 0.971125\n",
      "Iteration 35, loss = 0.19106117\n",
      "Validation score: 0.969750\n",
      "Iteration 36, loss = 0.19081637\n",
      "Validation score: 0.969875\n",
      "Iteration 37, loss = 0.19027507\n",
      "Validation score: 0.969500\n",
      "Iteration 38, loss = 0.18980821\n",
      "Validation score: 0.970750\n",
      "Iteration 39, loss = 0.19001412\n",
      "Validation score: 0.969875\n",
      "Iteration 40, loss = 0.18997595\n",
      "Validation score: 0.971375\n",
      "Iteration 41, loss = 0.18927649\n",
      "Validation score: 0.969750\n",
      "Iteration 42, loss = 0.19003721\n",
      "Validation score: 0.971250\n",
      "Iteration 43, loss = 0.18861554\n",
      "Validation score: 0.969875\n",
      "Iteration 44, loss = 0.18876329\n",
      "Validation score: 0.969000\n",
      "Iteration 45, loss = 0.18881544\n",
      "Validation score: 0.970750\n",
      "Iteration 46, loss = 0.18818760\n",
      "Validation score: 0.971500\n",
      "Iteration 47, loss = 0.18907776\n",
      "Validation score: 0.972500\n",
      "Iteration 48, loss = 0.18777350\n",
      "Validation score: 0.971500\n",
      "Iteration 49, loss = 0.18755905\n",
      "Validation score: 0.969750\n",
      "Iteration 50, loss = 0.18788720\n",
      "Validation score: 0.971000\n",
      "Iteration 51, loss = 0.18754418\n",
      "Validation score: 0.968875\n",
      "Iteration 52, loss = 0.18754533\n",
      "Validation score: 0.970625\n",
      "Iteration 53, loss = 0.18682145\n",
      "Validation score: 0.970500\n",
      "Iteration 54, loss = 0.18748148\n",
      "Validation score: 0.970625\n",
      "Iteration 55, loss = 0.18671415\n",
      "Validation score: 0.970375\n",
      "Iteration 56, loss = 0.18695154\n",
      "Validation score: 0.972000\n",
      "Iteration 57, loss = 0.18656336\n",
      "Validation score: 0.971375\n",
      "Iteration 58, loss = 0.18660197\n",
      "Validation score: 0.971750\n",
      "Iteration 59, loss = 0.18664841\n",
      "Validation score: 0.971000\n",
      "Iteration 60, loss = 0.18670404\n",
      "Validation score: 0.971375\n",
      "Iteration 61, loss = 0.18625914\n",
      "Validation score: 0.969750\n",
      "Iteration 62, loss = 0.18660507\n",
      "Validation score: 0.971250\n",
      "Iteration 63, loss = 0.18606546\n",
      "Validation score: 0.972250\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.29481129\n",
      "Validation score: 0.871500\n",
      "Iteration 2, loss = 0.42856538\n",
      "Validation score: 0.914125\n",
      "Iteration 3, loss = 0.33950963\n",
      "Validation score: 0.936000\n",
      "Iteration 4, loss = 0.29592792\n",
      "Validation score: 0.943375\n",
      "Iteration 5, loss = 0.26953067\n",
      "Validation score: 0.950500\n",
      "Iteration 6, loss = 0.25335208\n",
      "Validation score: 0.954500\n",
      "Iteration 7, loss = 0.24270136\n",
      "Validation score: 0.959625\n",
      "Iteration 8, loss = 0.23488754\n",
      "Validation score: 0.957500\n",
      "Iteration 9, loss = 0.22773454\n",
      "Validation score: 0.960000\n",
      "Iteration 10, loss = 0.22288682\n",
      "Validation score: 0.965125\n",
      "Iteration 11, loss = 0.21959881\n",
      "Validation score: 0.965625\n",
      "Iteration 12, loss = 0.21436097\n",
      "Validation score: 0.963625\n",
      "Iteration 13, loss = 0.21249666\n",
      "Validation score: 0.964500\n",
      "Iteration 14, loss = 0.20976001\n",
      "Validation score: 0.962750\n",
      "Iteration 15, loss = 0.20793016\n",
      "Validation score: 0.966250\n",
      "Iteration 16, loss = 0.20577584\n",
      "Validation score: 0.965750\n",
      "Iteration 17, loss = 0.20374875\n",
      "Validation score: 0.968125\n",
      "Iteration 18, loss = 0.20281393\n",
      "Validation score: 0.969250\n",
      "Iteration 19, loss = 0.20161852\n",
      "Validation score: 0.967000\n",
      "Iteration 20, loss = 0.20039367\n",
      "Validation score: 0.968750\n",
      "Iteration 21, loss = 0.19895824\n",
      "Validation score: 0.969500\n",
      "Iteration 22, loss = 0.19838578\n",
      "Validation score: 0.970000\n",
      "Iteration 23, loss = 0.19746082\n",
      "Validation score: 0.969875\n",
      "Iteration 24, loss = 0.19602099\n",
      "Validation score: 0.971125\n",
      "Iteration 25, loss = 0.19597428\n",
      "Validation score: 0.970250\n",
      "Iteration 26, loss = 0.19514224\n",
      "Validation score: 0.970875\n",
      "Iteration 27, loss = 0.19452102\n",
      "Validation score: 0.970250\n",
      "Iteration 28, loss = 0.19472719\n",
      "Validation score: 0.970250\n",
      "Iteration 29, loss = 0.19378031\n",
      "Validation score: 0.970500\n",
      "Iteration 30, loss = 0.19295025\n",
      "Validation score: 0.971000\n",
      "Iteration 31, loss = 0.19281479\n",
      "Validation score: 0.971750\n",
      "Iteration 32, loss = 0.19213681\n",
      "Validation score: 0.972000\n",
      "Iteration 33, loss = 0.19158169\n",
      "Validation score: 0.970500\n",
      "Iteration 34, loss = 0.19108278\n",
      "Validation score: 0.971375\n",
      "Iteration 35, loss = 0.19090406\n",
      "Validation score: 0.972500\n",
      "Iteration 36, loss = 0.19072511\n",
      "Validation score: 0.970625\n",
      "Iteration 37, loss = 0.18990541\n",
      "Validation score: 0.973125\n",
      "Iteration 38, loss = 0.19099105\n",
      "Validation score: 0.970875\n",
      "Iteration 39, loss = 0.18993210\n",
      "Validation score: 0.971375\n",
      "Iteration 40, loss = 0.18974928\n",
      "Validation score: 0.970500\n",
      "Iteration 41, loss = 0.18950993\n",
      "Validation score: 0.972625\n",
      "Iteration 42, loss = 0.18924229\n",
      "Validation score: 0.974000\n",
      "Iteration 43, loss = 0.18851719\n",
      "Validation score: 0.972125\n",
      "Iteration 44, loss = 0.18899071\n",
      "Validation score: 0.972750\n",
      "Iteration 45, loss = 0.18784738\n",
      "Validation score: 0.972125\n",
      "Iteration 46, loss = 0.18782171\n",
      "Validation score: 0.973625\n",
      "Iteration 47, loss = 0.18792643\n",
      "Validation score: 0.973375\n",
      "Iteration 48, loss = 0.18771487\n",
      "Validation score: 0.970625\n",
      "Iteration 49, loss = 0.18767745\n",
      "Validation score: 0.973625\n",
      "Iteration 50, loss = 0.18709268\n",
      "Validation score: 0.973625\n",
      "Iteration 51, loss = 0.18684413\n",
      "Validation score: 0.972875\n",
      "Iteration 52, loss = 0.18757867\n",
      "Validation score: 0.972750\n",
      "Iteration 53, loss = 0.18645074\n",
      "Validation score: 0.973375\n",
      "Iteration 54, loss = 0.18666551\n",
      "Validation score: 0.973750\n",
      "Iteration 55, loss = 0.18663546\n",
      "Validation score: 0.973500\n",
      "Iteration 56, loss = 0.18559582\n",
      "Validation score: 0.974750\n",
      "Iteration 57, loss = 0.18614776\n",
      "Validation score: 0.973500\n",
      "Iteration 58, loss = 0.18610972\n",
      "Validation score: 0.974625\n",
      "Iteration 59, loss = 0.18498532\n",
      "Validation score: 0.973750\n",
      "Iteration 60, loss = 0.18571770\n",
      "Validation score: 0.972875\n",
      "Iteration 61, loss = 0.18556311\n",
      "Validation score: 0.973875\n",
      "Iteration 62, loss = 0.18562599\n",
      "Validation score: 0.974375\n",
      "Iteration 63, loss = 0.18497952\n",
      "Validation score: 0.973375\n",
      "Iteration 64, loss = 0.18482138\n",
      "Validation score: 0.973875\n",
      "Iteration 65, loss = 0.18481168\n",
      "Validation score: 0.974125\n",
      "Iteration 66, loss = 0.18526687\n",
      "Validation score: 0.973000\n",
      "Iteration 67, loss = 0.18557392\n",
      "Validation score: 0.973500\n",
      "Iteration 68, loss = 0.18500968\n",
      "Validation score: 0.975125\n",
      "Iteration 69, loss = 0.18433051\n",
      "Validation score: 0.974500\n",
      "Iteration 70, loss = 0.18406596\n",
      "Validation score: 0.975250\n",
      "Iteration 71, loss = 0.18437943\n",
      "Validation score: 0.973250\n",
      "Iteration 72, loss = 0.18434654\n",
      "Validation score: 0.973000\n",
      "Iteration 73, loss = 0.18382347\n",
      "Validation score: 0.970875\n",
      "Iteration 74, loss = 0.18424002\n",
      "Validation score: 0.974250\n",
      "Iteration 75, loss = 0.18362363\n",
      "Validation score: 0.973625\n",
      "Iteration 76, loss = 0.18407014\n",
      "Validation score: 0.975625\n",
      "Iteration 77, loss = 0.18365076\n",
      "Validation score: 0.974500\n",
      "Iteration 78, loss = 0.18380964\n",
      "Validation score: 0.974000\n",
      "Iteration 79, loss = 0.18349966\n",
      "Validation score: 0.974125\n",
      "Iteration 80, loss = 0.18340926\n",
      "Validation score: 0.976000\n",
      "Iteration 81, loss = 0.18356721\n",
      "Validation score: 0.975500\n",
      "Iteration 82, loss = 0.18351229\n",
      "Validation score: 0.974625\n",
      "Iteration 83, loss = 0.18318102\n",
      "Validation score: 0.976625\n",
      "Iteration 84, loss = 0.18300078\n",
      "Validation score: 0.976250\n",
      "Iteration 85, loss = 0.18300817\n",
      "Validation score: 0.975750\n",
      "Iteration 86, loss = 0.18327337\n",
      "Validation score: 0.975250\n",
      "Iteration 87, loss = 0.18348395\n",
      "Validation score: 0.975625\n",
      "Iteration 88, loss = 0.18266549\n",
      "Validation score: 0.974875\n",
      "Iteration 89, loss = 0.18255058\n",
      "Validation score: 0.975625\n",
      "Iteration 90, loss = 0.18289608\n",
      "Validation score: 0.975125\n",
      "Iteration 91, loss = 0.18291752\n",
      "Validation score: 0.974875\n",
      "Iteration 92, loss = 0.18308858\n",
      "Validation score: 0.975125\n",
      "Iteration 93, loss = 0.18328010\n",
      "Validation score: 0.974875\n",
      "Iteration 94, loss = 0.18324503\n",
      "Validation score: 0.974875\n",
      "Iteration 95, loss = 0.18294706\n",
      "Validation score: 0.974750\n",
      "Iteration 96, loss = 0.18283616\n",
      "Validation score: 0.975125\n",
      "Iteration 97, loss = 0.18216242\n",
      "Validation score: 0.975125\n",
      "Iteration 98, loss = 0.18209385\n",
      "Validation score: 0.974750\n",
      "Iteration 99, loss = 0.18174585\n",
      "Validation score: 0.974250\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.29403190\n",
      "Validation score: 0.874125\n",
      "Iteration 2, loss = 0.42920814\n",
      "Validation score: 0.911625\n",
      "Iteration 3, loss = 0.34214683\n",
      "Validation score: 0.928375\n",
      "Iteration 4, loss = 0.29763927\n",
      "Validation score: 0.936875\n",
      "Iteration 5, loss = 0.27023847\n",
      "Validation score: 0.943250\n",
      "Iteration 6, loss = 0.25414548\n",
      "Validation score: 0.949125\n",
      "Iteration 7, loss = 0.24156346\n",
      "Validation score: 0.952750\n",
      "Iteration 8, loss = 0.23377226\n",
      "Validation score: 0.954875\n",
      "Iteration 9, loss = 0.22668572\n",
      "Validation score: 0.959375\n",
      "Iteration 10, loss = 0.22176880\n",
      "Validation score: 0.959750\n",
      "Iteration 11, loss = 0.21707785\n",
      "Validation score: 0.959750\n",
      "Iteration 12, loss = 0.21431526\n",
      "Validation score: 0.959750\n",
      "Iteration 13, loss = 0.21104331\n",
      "Validation score: 0.962250\n",
      "Iteration 14, loss = 0.20885377\n",
      "Validation score: 0.964000\n",
      "Iteration 15, loss = 0.20592517\n",
      "Validation score: 0.962125\n",
      "Iteration 16, loss = 0.20423321\n",
      "Validation score: 0.964000\n",
      "Iteration 17, loss = 0.20221213\n",
      "Validation score: 0.965250\n",
      "Iteration 18, loss = 0.20126724\n",
      "Validation score: 0.966250\n",
      "Iteration 19, loss = 0.19956366\n",
      "Validation score: 0.967125\n",
      "Iteration 20, loss = 0.19856076\n",
      "Validation score: 0.966000\n",
      "Iteration 21, loss = 0.19820837\n",
      "Validation score: 0.965250\n",
      "Iteration 22, loss = 0.19680441\n",
      "Validation score: 0.967250\n",
      "Iteration 23, loss = 0.19641641\n",
      "Validation score: 0.964125\n",
      "Iteration 24, loss = 0.19575112\n",
      "Validation score: 0.966000\n",
      "Iteration 25, loss = 0.19521618\n",
      "Validation score: 0.967625\n",
      "Iteration 26, loss = 0.19454260\n",
      "Validation score: 0.966625\n",
      "Iteration 27, loss = 0.19380777\n",
      "Validation score: 0.967375\n",
      "Iteration 28, loss = 0.19279687\n",
      "Validation score: 0.968625\n",
      "Iteration 29, loss = 0.19218834\n",
      "Validation score: 0.968375\n",
      "Iteration 30, loss = 0.19174642\n",
      "Validation score: 0.968625\n",
      "Iteration 31, loss = 0.19113574\n",
      "Validation score: 0.967375\n",
      "Iteration 32, loss = 0.19053872\n",
      "Validation score: 0.968500\n",
      "Iteration 33, loss = 0.19103035\n",
      "Validation score: 0.967750\n",
      "Iteration 34, loss = 0.19033504\n",
      "Validation score: 0.970625\n",
      "Iteration 35, loss = 0.19001348\n",
      "Validation score: 0.969625\n",
      "Iteration 36, loss = 0.18921293\n",
      "Validation score: 0.970125\n",
      "Iteration 37, loss = 0.18922671\n",
      "Validation score: 0.970500\n",
      "Iteration 38, loss = 0.18876068\n",
      "Validation score: 0.970625\n",
      "Iteration 39, loss = 0.18840849\n",
      "Validation score: 0.969625\n",
      "Iteration 40, loss = 0.18795321\n",
      "Validation score: 0.971000\n",
      "Iteration 41, loss = 0.18832012\n",
      "Validation score: 0.968875\n",
      "Iteration 42, loss = 0.18774071\n",
      "Validation score: 0.969250\n",
      "Iteration 43, loss = 0.18757914\n",
      "Validation score: 0.969375\n",
      "Iteration 44, loss = 0.18711270\n",
      "Validation score: 0.970000\n",
      "Iteration 45, loss = 0.18748460\n",
      "Validation score: 0.970875\n",
      "Iteration 46, loss = 0.18690401\n",
      "Validation score: 0.968625\n",
      "Iteration 47, loss = 0.18699144\n",
      "Validation score: 0.970375\n",
      "Iteration 48, loss = 0.18645477\n",
      "Validation score: 0.970500\n",
      "Iteration 49, loss = 0.18690009\n",
      "Validation score: 0.971000\n",
      "Iteration 50, loss = 0.18637707\n",
      "Validation score: 0.972250\n",
      "Iteration 51, loss = 0.18625676\n",
      "Validation score: 0.971250\n",
      "Iteration 52, loss = 0.18626697\n",
      "Validation score: 0.971125\n",
      "Iteration 53, loss = 0.18628989\n",
      "Validation score: 0.970875\n",
      "Iteration 54, loss = 0.18523749\n",
      "Validation score: 0.969750\n",
      "Iteration 55, loss = 0.18550639\n",
      "Validation score: 0.972250\n",
      "Iteration 56, loss = 0.18504758\n",
      "Validation score: 0.971750\n",
      "Iteration 57, loss = 0.18537152\n",
      "Validation score: 0.971125\n",
      "Iteration 58, loss = 0.18552033\n",
      "Validation score: 0.970625\n",
      "Iteration 59, loss = 0.18464098\n",
      "Validation score: 0.972750\n",
      "Iteration 60, loss = 0.18493361\n",
      "Validation score: 0.971500\n",
      "Iteration 61, loss = 0.18440115\n",
      "Validation score: 0.970875\n",
      "Iteration 62, loss = 0.18484865\n",
      "Validation score: 0.971875\n",
      "Iteration 63, loss = 0.18426433\n",
      "Validation score: 0.972750\n",
      "Iteration 64, loss = 0.18451884\n",
      "Validation score: 0.972625\n",
      "Iteration 65, loss = 0.18434906\n",
      "Validation score: 0.972375\n",
      "Iteration 66, loss = 0.18385895\n",
      "Validation score: 0.973125\n",
      "Iteration 67, loss = 0.18467583\n",
      "Validation score: 0.969375\n",
      "Iteration 68, loss = 0.18420407\n",
      "Validation score: 0.972875\n",
      "Iteration 69, loss = 0.18365944\n",
      "Validation score: 0.972125\n",
      "Iteration 70, loss = 0.18354515\n",
      "Validation score: 0.972750\n",
      "Iteration 71, loss = 0.18370117\n",
      "Validation score: 0.972125\n",
      "Iteration 72, loss = 0.18391013\n",
      "Validation score: 0.970875\n",
      "Iteration 73, loss = 0.18362402\n",
      "Validation score: 0.972500\n",
      "Iteration 74, loss = 0.18385391\n",
      "Validation score: 0.973625\n",
      "Iteration 75, loss = 0.18307586\n",
      "Validation score: 0.972625\n",
      "Iteration 76, loss = 0.18317349\n",
      "Validation score: 0.974000\n",
      "Iteration 77, loss = 0.18302764\n",
      "Validation score: 0.972750\n",
      "Iteration 78, loss = 0.18367118\n",
      "Validation score: 0.972750\n",
      "Iteration 79, loss = 0.18343769\n",
      "Validation score: 0.974125\n",
      "Iteration 80, loss = 0.18282391\n",
      "Validation score: 0.972500\n",
      "Iteration 81, loss = 0.18332713\n",
      "Validation score: 0.972875\n",
      "Iteration 82, loss = 0.18278096\n",
      "Validation score: 0.974125\n",
      "Iteration 83, loss = 0.18234751\n",
      "Validation score: 0.971375\n",
      "Iteration 84, loss = 0.18230878\n",
      "Validation score: 0.972875\n",
      "Iteration 85, loss = 0.18256335\n",
      "Validation score: 0.973250\n",
      "Iteration 86, loss = 0.18256154\n",
      "Validation score: 0.972875\n",
      "Iteration 87, loss = 0.18239197\n",
      "Validation score: 0.973875\n",
      "Iteration 88, loss = 0.18278669\n",
      "Validation score: 0.972750\n",
      "Iteration 89, loss = 0.18240491\n",
      "Validation score: 0.973750\n",
      "Iteration 90, loss = 0.18178614\n",
      "Validation score: 0.973250\n",
      "Iteration 91, loss = 0.18210957\n",
      "Validation score: 0.973000\n",
      "Iteration 92, loss = 0.18224720\n",
      "Validation score: 0.972750\n",
      "Iteration 93, loss = 0.18212039\n",
      "Validation score: 0.974250\n",
      "Iteration 94, loss = 0.18265056\n",
      "Validation score: 0.974250\n",
      "Iteration 95, loss = 0.18203719\n",
      "Validation score: 0.974375\n",
      "Iteration 96, loss = 0.18212241\n",
      "Validation score: 0.973125\n",
      "Iteration 97, loss = 0.18170373\n",
      "Validation score: 0.973000\n",
      "Iteration 98, loss = 0.18198704\n",
      "Validation score: 0.972375\n",
      "Iteration 99, loss = 0.18205817\n",
      "Validation score: 0.974125\n",
      "Iteration 100, loss = 0.18168524\n",
      "Validation score: 0.973250\n",
      "Iteration 101, loss = 0.18175114\n",
      "Validation score: 0.972000\n",
      "Iteration 102, loss = 0.18195941\n",
      "Validation score: 0.973000\n",
      "Iteration 103, loss = 0.18212055\n",
      "Validation score: 0.974125\n",
      "Iteration 104, loss = 0.18135779\n",
      "Validation score: 0.974875\n",
      "Iteration 105, loss = 0.18087388\n",
      "Validation score: 0.974125\n",
      "Iteration 106, loss = 0.18157452\n",
      "Validation score: 0.973000\n",
      "Iteration 107, loss = 0.18145343\n",
      "Validation score: 0.973375\n",
      "Iteration 108, loss = 0.18218635\n",
      "Validation score: 0.974000\n",
      "Iteration 109, loss = 0.18084631\n",
      "Validation score: 0.973250\n",
      "Iteration 110, loss = 0.18114611\n",
      "Validation score: 0.973250\n",
      "Iteration 111, loss = 0.18123511\n",
      "Validation score: 0.972875\n",
      "Iteration 112, loss = 0.18164158\n",
      "Validation score: 0.973000\n",
      "Iteration 113, loss = 0.18115602\n",
      "Validation score: 0.973250\n",
      "Iteration 114, loss = 0.18099231\n",
      "Validation score: 0.973750\n",
      "Iteration 115, loss = 0.18157439\n",
      "Validation score: 0.974375\n",
      "Iteration 116, loss = 0.18084803\n",
      "Validation score: 0.974750\n",
      "Iteration 117, loss = 0.18127076\n",
      "Validation score: 0.974375\n",
      "Iteration 118, loss = 0.18123424\n",
      "Validation score: 0.973250\n",
      "Iteration 119, loss = 0.18082378\n",
      "Validation score: 0.974750\n",
      "Iteration 120, loss = 0.18123736\n",
      "Validation score: 0.972375\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48020866\n",
      "Validation score: 0.830375\n",
      "Iteration 2, loss = 0.53287037\n",
      "Validation score: 0.905125\n",
      "Iteration 3, loss = 0.39609865\n",
      "Validation score: 0.922500\n",
      "Iteration 4, loss = 0.34843737\n",
      "Validation score: 0.929875\n",
      "Iteration 5, loss = 0.32381357\n",
      "Validation score: 0.932875\n",
      "Iteration 6, loss = 0.30961660\n",
      "Validation score: 0.938875\n",
      "Iteration 7, loss = 0.29947970\n",
      "Validation score: 0.938625\n",
      "Iteration 8, loss = 0.29257342\n",
      "Validation score: 0.941750\n",
      "Iteration 9, loss = 0.28519416\n",
      "Validation score: 0.941625\n",
      "Iteration 10, loss = 0.28098836\n",
      "Validation score: 0.943500\n",
      "Iteration 11, loss = 0.27772465\n",
      "Validation score: 0.945000\n",
      "Iteration 12, loss = 0.27595773\n",
      "Validation score: 0.943000\n",
      "Iteration 13, loss = 0.27281015\n",
      "Validation score: 0.944375\n",
      "Iteration 14, loss = 0.27095248\n",
      "Validation score: 0.944875\n",
      "Iteration 15, loss = 0.26852839\n",
      "Validation score: 0.945250\n",
      "Iteration 16, loss = 0.26605854\n",
      "Validation score: 0.943625\n",
      "Iteration 17, loss = 0.26378841\n",
      "Validation score: 0.947250\n",
      "Iteration 18, loss = 0.26432758\n",
      "Validation score: 0.948625\n",
      "Iteration 19, loss = 0.26301991\n",
      "Validation score: 0.947375\n",
      "Iteration 20, loss = 0.26150542\n",
      "Validation score: 0.947375\n",
      "Iteration 21, loss = 0.25949065\n",
      "Validation score: 0.948750\n",
      "Iteration 22, loss = 0.25832415\n",
      "Validation score: 0.949000\n",
      "Iteration 23, loss = 0.25814245\n",
      "Validation score: 0.949000\n",
      "Iteration 24, loss = 0.25809831\n",
      "Validation score: 0.947375\n",
      "Iteration 25, loss = 0.25614197\n",
      "Validation score: 0.950375\n",
      "Iteration 26, loss = 0.25534621\n",
      "Validation score: 0.948500\n",
      "Iteration 27, loss = 0.25520292\n",
      "Validation score: 0.947375\n",
      "Iteration 28, loss = 0.25479552\n",
      "Validation score: 0.950125\n",
      "Iteration 29, loss = 0.25454806\n",
      "Validation score: 0.950750\n",
      "Iteration 30, loss = 0.25416289\n",
      "Validation score: 0.944875\n",
      "Iteration 31, loss = 0.25244212\n",
      "Validation score: 0.950125\n",
      "Iteration 32, loss = 0.25152063\n",
      "Validation score: 0.950250\n",
      "Iteration 33, loss = 0.25215508\n",
      "Validation score: 0.950625\n",
      "Iteration 34, loss = 0.25133645\n",
      "Validation score: 0.949875\n",
      "Iteration 35, loss = 0.25118855\n",
      "Validation score: 0.948875\n",
      "Iteration 36, loss = 0.25047045\n",
      "Validation score: 0.948000\n",
      "Iteration 37, loss = 0.25049970\n",
      "Validation score: 0.951500\n",
      "Iteration 38, loss = 0.24964699\n",
      "Validation score: 0.949125\n",
      "Iteration 39, loss = 0.24939857\n",
      "Validation score: 0.950625\n",
      "Iteration 40, loss = 0.24841646\n",
      "Validation score: 0.951625\n",
      "Iteration 41, loss = 0.24752544\n",
      "Validation score: 0.950875\n",
      "Iteration 42, loss = 0.24761516\n",
      "Validation score: 0.950375\n",
      "Iteration 43, loss = 0.24755210\n",
      "Validation score: 0.950875\n",
      "Iteration 44, loss = 0.24662598\n",
      "Validation score: 0.950125\n",
      "Iteration 45, loss = 0.24759694\n",
      "Validation score: 0.953750\n",
      "Iteration 46, loss = 0.24682324\n",
      "Validation score: 0.948250\n",
      "Iteration 47, loss = 0.24611103\n",
      "Validation score: 0.952000\n",
      "Iteration 48, loss = 0.24615299\n",
      "Validation score: 0.950875\n",
      "Iteration 49, loss = 0.24607948\n",
      "Validation score: 0.952750\n",
      "Iteration 50, loss = 0.24541739\n",
      "Validation score: 0.951750\n",
      "Iteration 51, loss = 0.24434420\n",
      "Validation score: 0.952375\n",
      "Iteration 52, loss = 0.24444705\n",
      "Validation score: 0.950875\n",
      "Iteration 53, loss = 0.24303044\n",
      "Validation score: 0.953125\n",
      "Iteration 54, loss = 0.24446361\n",
      "Validation score: 0.953500\n",
      "Iteration 55, loss = 0.24391521\n",
      "Validation score: 0.951375\n",
      "Iteration 56, loss = 0.24313227\n",
      "Validation score: 0.952250\n",
      "Iteration 57, loss = 0.24293650\n",
      "Validation score: 0.952375\n",
      "Iteration 58, loss = 0.24374485\n",
      "Validation score: 0.954875\n",
      "Iteration 59, loss = 0.24378391\n",
      "Validation score: 0.955125\n",
      "Iteration 60, loss = 0.24333020\n",
      "Validation score: 0.952125\n",
      "Iteration 61, loss = 0.24256076\n",
      "Validation score: 0.954625\n",
      "Iteration 62, loss = 0.24252456\n",
      "Validation score: 0.948875\n",
      "Iteration 63, loss = 0.24261445\n",
      "Validation score: 0.953500\n",
      "Iteration 64, loss = 0.24184350\n",
      "Validation score: 0.952625\n",
      "Iteration 65, loss = 0.24133700\n",
      "Validation score: 0.954750\n",
      "Iteration 66, loss = 0.24135589\n",
      "Validation score: 0.952500\n",
      "Iteration 67, loss = 0.24152238\n",
      "Validation score: 0.950375\n",
      "Iteration 68, loss = 0.24100310\n",
      "Validation score: 0.951625\n",
      "Iteration 69, loss = 0.24161277\n",
      "Validation score: 0.953750\n",
      "Iteration 70, loss = 0.24027990\n",
      "Validation score: 0.951625\n",
      "Iteration 71, loss = 0.24086015\n",
      "Validation score: 0.953500\n",
      "Iteration 72, loss = 0.24052621\n",
      "Validation score: 0.952875\n",
      "Iteration 73, loss = 0.23984610\n",
      "Validation score: 0.954375\n",
      "Iteration 74, loss = 0.23991355\n",
      "Validation score: 0.954750\n",
      "Iteration 75, loss = 0.24117002\n",
      "Validation score: 0.952500\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47410734\n",
      "Validation score: 0.826750\n",
      "Iteration 2, loss = 0.52473085\n",
      "Validation score: 0.901125\n",
      "Iteration 3, loss = 0.39107908\n",
      "Validation score: 0.921250\n",
      "Iteration 4, loss = 0.34286818\n",
      "Validation score: 0.927750\n",
      "Iteration 5, loss = 0.31949145\n",
      "Validation score: 0.934875\n",
      "Iteration 6, loss = 0.30443584\n",
      "Validation score: 0.936000\n",
      "Iteration 7, loss = 0.29259667\n",
      "Validation score: 0.936250\n",
      "Iteration 8, loss = 0.28562038\n",
      "Validation score: 0.944625\n",
      "Iteration 9, loss = 0.28062202\n",
      "Validation score: 0.944125\n",
      "Iteration 10, loss = 0.27653131\n",
      "Validation score: 0.947625\n",
      "Iteration 11, loss = 0.27293754\n",
      "Validation score: 0.946250\n",
      "Iteration 12, loss = 0.27065577\n",
      "Validation score: 0.945750\n",
      "Iteration 13, loss = 0.26819728\n",
      "Validation score: 0.947375\n",
      "Iteration 14, loss = 0.26616836\n",
      "Validation score: 0.947750\n",
      "Iteration 15, loss = 0.26480363\n",
      "Validation score: 0.947250\n",
      "Iteration 16, loss = 0.26295754\n",
      "Validation score: 0.948375\n",
      "Iteration 17, loss = 0.26105669\n",
      "Validation score: 0.947500\n",
      "Iteration 18, loss = 0.26097178\n",
      "Validation score: 0.944875\n",
      "Iteration 19, loss = 0.25939903\n",
      "Validation score: 0.948750\n",
      "Iteration 20, loss = 0.25857998\n",
      "Validation score: 0.949250\n",
      "Iteration 21, loss = 0.25731945\n",
      "Validation score: 0.949875\n",
      "Iteration 22, loss = 0.25651977\n",
      "Validation score: 0.949750\n",
      "Iteration 23, loss = 0.25577308\n",
      "Validation score: 0.950250\n",
      "Iteration 24, loss = 0.25475781\n",
      "Validation score: 0.948750\n",
      "Iteration 25, loss = 0.25380627\n",
      "Validation score: 0.948000\n",
      "Iteration 26, loss = 0.25374305\n",
      "Validation score: 0.952875\n",
      "Iteration 27, loss = 0.25349587\n",
      "Validation score: 0.951125\n",
      "Iteration 28, loss = 0.25333289\n",
      "Validation score: 0.951250\n",
      "Iteration 29, loss = 0.25314100\n",
      "Validation score: 0.951375\n",
      "Iteration 30, loss = 0.25214761\n",
      "Validation score: 0.948000\n",
      "Iteration 31, loss = 0.25156473\n",
      "Validation score: 0.949500\n",
      "Iteration 32, loss = 0.25060639\n",
      "Validation score: 0.950625\n",
      "Iteration 33, loss = 0.24976772\n",
      "Validation score: 0.951125\n",
      "Iteration 34, loss = 0.25086775\n",
      "Validation score: 0.950875\n",
      "Iteration 35, loss = 0.24958020\n",
      "Validation score: 0.951625\n",
      "Iteration 36, loss = 0.24867698\n",
      "Validation score: 0.951750\n",
      "Iteration 37, loss = 0.24901385\n",
      "Validation score: 0.951125\n",
      "Iteration 38, loss = 0.24885174\n",
      "Validation score: 0.952500\n",
      "Iteration 39, loss = 0.24890497\n",
      "Validation score: 0.953750\n",
      "Iteration 40, loss = 0.24749020\n",
      "Validation score: 0.950000\n",
      "Iteration 41, loss = 0.24697914\n",
      "Validation score: 0.953250\n",
      "Iteration 42, loss = 0.24605487\n",
      "Validation score: 0.949625\n",
      "Iteration 43, loss = 0.24649569\n",
      "Validation score: 0.951000\n",
      "Iteration 44, loss = 0.24769820\n",
      "Validation score: 0.952625\n",
      "Iteration 45, loss = 0.24596729\n",
      "Validation score: 0.953125\n",
      "Iteration 46, loss = 0.24604973\n",
      "Validation score: 0.949250\n",
      "Iteration 47, loss = 0.24487640\n",
      "Validation score: 0.949500\n",
      "Iteration 48, loss = 0.24571728\n",
      "Validation score: 0.952000\n",
      "Iteration 49, loss = 0.24505235\n",
      "Validation score: 0.952375\n",
      "Iteration 50, loss = 0.24500354\n",
      "Validation score: 0.952375\n",
      "Iteration 51, loss = 0.24447601\n",
      "Validation score: 0.953500\n",
      "Iteration 52, loss = 0.24386843\n",
      "Validation score: 0.953625\n",
      "Iteration 53, loss = 0.24378677\n",
      "Validation score: 0.952750\n",
      "Iteration 54, loss = 0.24369641\n",
      "Validation score: 0.951250\n",
      "Iteration 55, loss = 0.24373920\n",
      "Validation score: 0.954125\n",
      "Iteration 56, loss = 0.24237375\n",
      "Validation score: 0.954625\n",
      "Iteration 57, loss = 0.24235452\n",
      "Validation score: 0.950500\n",
      "Iteration 58, loss = 0.24295239\n",
      "Validation score: 0.952375\n",
      "Iteration 59, loss = 0.24272820\n",
      "Validation score: 0.951875\n",
      "Iteration 60, loss = 0.24127494\n",
      "Validation score: 0.952750\n",
      "Iteration 61, loss = 0.24205343\n",
      "Validation score: 0.954000\n",
      "Iteration 62, loss = 0.24216886\n",
      "Validation score: 0.954250\n",
      "Iteration 63, loss = 0.24255535\n",
      "Validation score: 0.953625\n",
      "Iteration 64, loss = 0.24135841\n",
      "Validation score: 0.953125\n",
      "Iteration 65, loss = 0.24119414\n",
      "Validation score: 0.952875\n",
      "Iteration 66, loss = 0.24178189\n",
      "Validation score: 0.952000\n",
      "Iteration 67, loss = 0.24080024\n",
      "Validation score: 0.953500\n",
      "Iteration 68, loss = 0.24132065\n",
      "Validation score: 0.952000\n",
      "Iteration 69, loss = 0.24144149\n",
      "Validation score: 0.953125\n",
      "Iteration 70, loss = 0.24116395\n",
      "Validation score: 0.953750\n",
      "Iteration 71, loss = 0.24094095\n",
      "Validation score: 0.954750\n",
      "Iteration 72, loss = 0.24045677\n",
      "Validation score: 0.952250\n",
      "Iteration 73, loss = 0.24009863\n",
      "Validation score: 0.955500\n",
      "Iteration 74, loss = 0.24029919\n",
      "Validation score: 0.952125\n",
      "Iteration 75, loss = 0.24024773\n",
      "Validation score: 0.952000\n",
      "Iteration 76, loss = 0.24032983\n",
      "Validation score: 0.952000\n",
      "Iteration 77, loss = 0.24023787\n",
      "Validation score: 0.951625\n",
      "Iteration 78, loss = 0.24031153\n",
      "Validation score: 0.951875\n",
      "Iteration 79, loss = 0.24117843\n",
      "Validation score: 0.953125\n",
      "Iteration 80, loss = 0.24007343\n",
      "Validation score: 0.954625\n",
      "Iteration 81, loss = 0.23911456\n",
      "Validation score: 0.953500\n",
      "Iteration 82, loss = 0.23987988\n",
      "Validation score: 0.952750\n",
      "Iteration 83, loss = 0.23931142\n",
      "Validation score: 0.955750\n",
      "Iteration 84, loss = 0.23869374\n",
      "Validation score: 0.948625\n",
      "Iteration 85, loss = 0.23927977\n",
      "Validation score: 0.953250\n",
      "Iteration 86, loss = 0.23911555\n",
      "Validation score: 0.954750\n",
      "Iteration 87, loss = 0.23878027\n",
      "Validation score: 0.954625\n",
      "Iteration 88, loss = 0.23883273\n",
      "Validation score: 0.955000\n",
      "Iteration 89, loss = 0.23924341\n",
      "Validation score: 0.955375\n",
      "Iteration 90, loss = 0.23857791\n",
      "Validation score: 0.955000\n",
      "Iteration 91, loss = 0.23856558\n",
      "Validation score: 0.953250\n",
      "Iteration 92, loss = 0.23859725\n",
      "Validation score: 0.952000\n",
      "Iteration 93, loss = 0.23799843\n",
      "Validation score: 0.952250\n",
      "Iteration 94, loss = 0.23905745\n",
      "Validation score: 0.955000\n",
      "Iteration 95, loss = 0.23859714\n",
      "Validation score: 0.953250\n",
      "Iteration 96, loss = 0.23842199\n",
      "Validation score: 0.955750\n",
      "Iteration 97, loss = 0.23764279\n",
      "Validation score: 0.954375\n",
      "Iteration 98, loss = 0.23761452\n",
      "Validation score: 0.952250\n",
      "Iteration 99, loss = 0.23866398\n",
      "Validation score: 0.955375\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48285952\n",
      "Validation score: 0.822000\n",
      "Iteration 2, loss = 0.53407735\n",
      "Validation score: 0.901500\n",
      "Iteration 3, loss = 0.39644869\n",
      "Validation score: 0.918500\n",
      "Iteration 4, loss = 0.34871452\n",
      "Validation score: 0.925875\n",
      "Iteration 5, loss = 0.32319062\n",
      "Validation score: 0.935000\n",
      "Iteration 6, loss = 0.30660383\n",
      "Validation score: 0.934250\n",
      "Iteration 7, loss = 0.29664227\n",
      "Validation score: 0.936250\n",
      "Iteration 8, loss = 0.28966910\n",
      "Validation score: 0.941000\n",
      "Iteration 9, loss = 0.28151718\n",
      "Validation score: 0.943000\n",
      "Iteration 10, loss = 0.27766708\n",
      "Validation score: 0.944000\n",
      "Iteration 11, loss = 0.27401136\n",
      "Validation score: 0.943875\n",
      "Iteration 12, loss = 0.27110148\n",
      "Validation score: 0.944125\n",
      "Iteration 13, loss = 0.26794978\n",
      "Validation score: 0.942500\n",
      "Iteration 14, loss = 0.26655080\n",
      "Validation score: 0.947125\n",
      "Iteration 15, loss = 0.26440231\n",
      "Validation score: 0.948875\n",
      "Iteration 16, loss = 0.26197716\n",
      "Validation score: 0.946375\n",
      "Iteration 17, loss = 0.26249843\n",
      "Validation score: 0.949875\n",
      "Iteration 18, loss = 0.25948676\n",
      "Validation score: 0.945875\n",
      "Iteration 19, loss = 0.25885186\n",
      "Validation score: 0.950500\n",
      "Iteration 20, loss = 0.25765863\n",
      "Validation score: 0.945875\n",
      "Iteration 21, loss = 0.25642816\n",
      "Validation score: 0.947625\n",
      "Iteration 22, loss = 0.25493925\n",
      "Validation score: 0.948750\n",
      "Iteration 23, loss = 0.25391323\n",
      "Validation score: 0.951125\n",
      "Iteration 24, loss = 0.25472680\n",
      "Validation score: 0.951375\n",
      "Iteration 25, loss = 0.25288096\n",
      "Validation score: 0.950125\n",
      "Iteration 26, loss = 0.25165599\n",
      "Validation score: 0.948875\n",
      "Iteration 27, loss = 0.25122761\n",
      "Validation score: 0.949375\n",
      "Iteration 28, loss = 0.25172953\n",
      "Validation score: 0.952500\n",
      "Iteration 29, loss = 0.24999228\n",
      "Validation score: 0.947750\n",
      "Iteration 30, loss = 0.25033709\n",
      "Validation score: 0.947125\n",
      "Iteration 31, loss = 0.24926302\n",
      "Validation score: 0.951625\n",
      "Iteration 32, loss = 0.24871284\n",
      "Validation score: 0.948875\n",
      "Iteration 33, loss = 0.24822845\n",
      "Validation score: 0.949875\n",
      "Iteration 34, loss = 0.24737813\n",
      "Validation score: 0.951375\n",
      "Iteration 35, loss = 0.24761199\n",
      "Validation score: 0.952375\n",
      "Iteration 36, loss = 0.24722186\n",
      "Validation score: 0.952000\n",
      "Iteration 37, loss = 0.24692839\n",
      "Validation score: 0.951125\n",
      "Iteration 38, loss = 0.24617992\n",
      "Validation score: 0.951250\n",
      "Iteration 39, loss = 0.24685274\n",
      "Validation score: 0.951625\n",
      "Iteration 40, loss = 0.24610349\n",
      "Validation score: 0.952250\n",
      "Iteration 41, loss = 0.24504521\n",
      "Validation score: 0.952750\n",
      "Iteration 42, loss = 0.24548894\n",
      "Validation score: 0.951875\n",
      "Iteration 43, loss = 0.24495001\n",
      "Validation score: 0.950875\n",
      "Iteration 44, loss = 0.24475336\n",
      "Validation score: 0.951000\n",
      "Iteration 45, loss = 0.24444147\n",
      "Validation score: 0.951750\n",
      "Iteration 46, loss = 0.24469609\n",
      "Validation score: 0.950625\n",
      "Iteration 47, loss = 0.24436863\n",
      "Validation score: 0.951000\n",
      "Iteration 48, loss = 0.24410569\n",
      "Validation score: 0.953000\n",
      "Iteration 49, loss = 0.24305513\n",
      "Validation score: 0.954375\n",
      "Iteration 50, loss = 0.24397471\n",
      "Validation score: 0.952750\n",
      "Iteration 51, loss = 0.24313802\n",
      "Validation score: 0.952000\n",
      "Iteration 52, loss = 0.24233016\n",
      "Validation score: 0.949875\n",
      "Iteration 53, loss = 0.24316976\n",
      "Validation score: 0.950750\n",
      "Iteration 54, loss = 0.24240826\n",
      "Validation score: 0.952000\n",
      "Iteration 55, loss = 0.24257773\n",
      "Validation score: 0.953500\n",
      "Iteration 56, loss = 0.24103498\n",
      "Validation score: 0.953875\n",
      "Iteration 57, loss = 0.24186903\n",
      "Validation score: 0.952125\n",
      "Iteration 58, loss = 0.24180626\n",
      "Validation score: 0.952875\n",
      "Iteration 59, loss = 0.24174621\n",
      "Validation score: 0.951375\n",
      "Iteration 60, loss = 0.24167197\n",
      "Validation score: 0.952750\n",
      "Iteration 61, loss = 0.24074661\n",
      "Validation score: 0.955750\n",
      "Iteration 62, loss = 0.24115577\n",
      "Validation score: 0.952500\n",
      "Iteration 63, loss = 0.24103617\n",
      "Validation score: 0.953500\n",
      "Iteration 64, loss = 0.24078702\n",
      "Validation score: 0.953875\n",
      "Iteration 65, loss = 0.24092527\n",
      "Validation score: 0.954625\n",
      "Iteration 66, loss = 0.24042678\n",
      "Validation score: 0.952625\n",
      "Iteration 67, loss = 0.24064714\n",
      "Validation score: 0.951500\n",
      "Iteration 68, loss = 0.23994389\n",
      "Validation score: 0.952875\n",
      "Iteration 69, loss = 0.24003632\n",
      "Validation score: 0.952875\n",
      "Iteration 70, loss = 0.23968806\n",
      "Validation score: 0.952000\n",
      "Iteration 71, loss = 0.24015918\n",
      "Validation score: 0.950750\n",
      "Iteration 72, loss = 0.24033083\n",
      "Validation score: 0.953250\n",
      "Iteration 73, loss = 0.24017858\n",
      "Validation score: 0.952500\n",
      "Iteration 74, loss = 0.23971940\n",
      "Validation score: 0.953625\n",
      "Iteration 75, loss = 0.23938635\n",
      "Validation score: 0.953250\n",
      "Iteration 76, loss = 0.24032722\n",
      "Validation score: 0.952875\n",
      "Iteration 77, loss = 0.23937874\n",
      "Validation score: 0.955375\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01375083\n",
      "Validation score: 0.897167\n",
      "Iteration 2, loss = 0.36142775\n",
      "Validation score: 0.933333\n",
      "Iteration 3, loss = 0.29429184\n",
      "Validation score: 0.946417\n",
      "Iteration 4, loss = 0.26370159\n",
      "Validation score: 0.949917\n",
      "Iteration 5, loss = 0.24692681\n",
      "Validation score: 0.959250\n",
      "Iteration 6, loss = 0.23537229\n",
      "Validation score: 0.961167\n",
      "Iteration 7, loss = 0.22889239\n",
      "Validation score: 0.962667\n",
      "Iteration 8, loss = 0.22337284\n",
      "Validation score: 0.966333\n",
      "Iteration 9, loss = 0.21888483\n",
      "Validation score: 0.964583\n",
      "Iteration 10, loss = 0.21572891\n",
      "Validation score: 0.966917\n",
      "Iteration 11, loss = 0.21402652\n",
      "Validation score: 0.968833\n",
      "Iteration 12, loss = 0.21049188\n",
      "Validation score: 0.967667\n",
      "Iteration 13, loss = 0.20847427\n",
      "Validation score: 0.968417\n",
      "Iteration 14, loss = 0.20660666\n",
      "Validation score: 0.970250\n",
      "Iteration 15, loss = 0.20526483\n",
      "Validation score: 0.968833\n",
      "Iteration 16, loss = 0.20431537\n",
      "Validation score: 0.971583\n",
      "Iteration 17, loss = 0.20304561\n",
      "Validation score: 0.972000\n",
      "Iteration 18, loss = 0.20272324\n",
      "Validation score: 0.973917\n",
      "Iteration 19, loss = 0.20119191\n",
      "Validation score: 0.971917\n",
      "Iteration 20, loss = 0.20033569\n",
      "Validation score: 0.969667\n",
      "Iteration 21, loss = 0.20001005\n",
      "Validation score: 0.971917\n",
      "Iteration 22, loss = 0.19896427\n",
      "Validation score: 0.973667\n",
      "Iteration 23, loss = 0.19875949\n",
      "Validation score: 0.971750\n",
      "Iteration 24, loss = 0.19845758\n",
      "Validation score: 0.973833\n",
      "Iteration 25, loss = 0.19733782\n",
      "Validation score: 0.974167\n",
      "Iteration 26, loss = 0.19696518\n",
      "Validation score: 0.973583\n",
      "Iteration 27, loss = 0.19652679\n",
      "Validation score: 0.974250\n",
      "Iteration 28, loss = 0.19630943\n",
      "Validation score: 0.973583\n",
      "Iteration 29, loss = 0.19620056\n",
      "Validation score: 0.973833\n",
      "Iteration 30, loss = 0.19540124\n",
      "Validation score: 0.973167\n",
      "Iteration 31, loss = 0.19505634\n",
      "Validation score: 0.973250\n",
      "Iteration 32, loss = 0.19474238\n",
      "Validation score: 0.974583\n",
      "Iteration 33, loss = 0.19475730\n",
      "Validation score: 0.972750\n",
      "Iteration 34, loss = 0.19440582\n",
      "Validation score: 0.971667\n",
      "Iteration 35, loss = 0.19430755\n",
      "Validation score: 0.971667\n",
      "Iteration 36, loss = 0.19365133\n",
      "Validation score: 0.974750\n",
      "Iteration 37, loss = 0.19398308\n",
      "Validation score: 0.974833\n",
      "Iteration 38, loss = 0.19365825\n",
      "Validation score: 0.974833\n",
      "Iteration 39, loss = 0.19324835\n",
      "Validation score: 0.973667\n",
      "Iteration 40, loss = 0.19282169\n",
      "Validation score: 0.974583\n",
      "Iteration 41, loss = 0.19276064\n",
      "Validation score: 0.974167\n",
      "Iteration 42, loss = 0.19291023\n",
      "Validation score: 0.974417\n",
      "Iteration 43, loss = 0.19246952\n",
      "Validation score: 0.974417\n",
      "Iteration 44, loss = 0.19128439\n",
      "Validation score: 0.973500\n",
      "Iteration 45, loss = 0.19213032\n",
      "Validation score: 0.974917\n",
      "Iteration 46, loss = 0.19194890\n",
      "Validation score: 0.975500\n",
      "Iteration 47, loss = 0.19180832\n",
      "Validation score: 0.974750\n",
      "Iteration 48, loss = 0.19127669\n",
      "Validation score: 0.975750\n",
      "Iteration 49, loss = 0.19120619\n",
      "Validation score: 0.974833\n",
      "Iteration 50, loss = 0.19101174\n",
      "Validation score: 0.975333\n",
      "Iteration 51, loss = 0.19091663\n",
      "Validation score: 0.974333\n",
      "Iteration 52, loss = 0.19097943\n",
      "Validation score: 0.975500\n",
      "Iteration 53, loss = 0.19065022\n",
      "Validation score: 0.975417\n",
      "Iteration 54, loss = 0.19058669\n",
      "Validation score: 0.975250\n",
      "Iteration 55, loss = 0.19080721\n",
      "Validation score: 0.976583\n",
      "Iteration 56, loss = 0.19033259\n",
      "Validation score: 0.975333\n",
      "Iteration 57, loss = 0.19089036\n",
      "Validation score: 0.974083\n",
      "Iteration 58, loss = 0.19075490\n",
      "Validation score: 0.974917\n",
      "Iteration 59, loss = 0.19023567\n",
      "Validation score: 0.976083\n",
      "Iteration 60, loss = 0.18988423\n",
      "Validation score: 0.974167\n",
      "Iteration 61, loss = 0.18940846\n",
      "Validation score: 0.975583\n",
      "Iteration 62, loss = 0.18997478\n",
      "Validation score: 0.974667\n",
      "Iteration 63, loss = 0.18999636\n",
      "Validation score: 0.975333\n",
      "Iteration 64, loss = 0.18963553\n",
      "Validation score: 0.975750\n",
      "Iteration 65, loss = 0.18961739\n",
      "Validation score: 0.975250\n",
      "Iteration 66, loss = 0.19002225\n",
      "Validation score: 0.974917\n",
      "Iteration 67, loss = 0.18911516\n",
      "Validation score: 0.973333\n",
      "Iteration 68, loss = 0.18987802\n",
      "Validation score: 0.976083\n",
      "Iteration 69, loss = 0.18876537\n",
      "Validation score: 0.975500\n",
      "Iteration 70, loss = 0.18957124\n",
      "Validation score: 0.974500\n",
      "Iteration 71, loss = 0.18928759\n",
      "Validation score: 0.976250\n",
      "Validation score did not improve more than tol=0.000000 for 15 consecutive epochs. Stopping.\n",
      "\n",
      "\n",
      "----------------------------\n",
      "Mejores parametros: \n",
      "{'activation': 'logistic', 'alpha': 0.05, 'hidden_layer_sizes': (80, 20)}\n",
      "R^2 modelo: 97.59 %\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modelo con pipeline\n",
    "\n",
    "(X_train , y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train = X_train.reshape(60000,784).astype('float32')/255.0\n",
    "X_test = X_test.reshape(10000,784).astype('float32')/255.0\n",
    "\n",
    "print('\\n')\n",
    "print('Dim X train: ',X_train.shape)\n",
    "print('Dim Y train: ',X_test.shape)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Definir modelo de regression por ANN (modelo inicial)\n",
    "model = MLPClassifier((100,60,30),\n",
    "                      activation='logistic',\n",
    "                      solver='sgd',\n",
    "                      random_state=42,\n",
    "                      batch_size = 'auto',\n",
    "                      max_iter=300,\n",
    "                      verbose=True,\n",
    "                      early_stopping=True,\n",
    "                      learning_rate_init = 0.2,\n",
    "                      power_t = 0.9,\n",
    "                      alpha = 0.01, \n",
    "                      shuffle=True,\n",
    "                      epsilon=1e-10,\n",
    "                      tol=1e-8,\n",
    "                      n_iter_no_change=15,\n",
    "                      validation_fraction=0.2)\n",
    "\n",
    "\n",
    "# entrenar modelos y busqueda mediante GridSearchCV\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Grilla de busqueda de mejores parametros para la ANN\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,20,10),(80,20),(20,10)],\n",
    "    'activation': ['tanh','logistic'],\n",
    "    'alpha': [0.0001, 0.05]}\n",
    "\n",
    "# Gridsearch CV\n",
    "search = GridSearchCV(model, param_grid,cv=3)\n",
    "\n",
    "# entrenar modelos y busqueda mediante GridSearchCV\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# ver ajuste del modelo entrenado\n",
    "valor = search.score(X_test,y_test)\n",
    "\n",
    "# Mejores parametros\n",
    "print('\\n\\n----------------------------')\n",
    "print('Mejores parametros: ')\n",
    "print(search.best_params_)\n",
    "print('R^2 modelo:' ,np.round(valor*100.0,3),'%')\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T16:43:32.269135Z",
     "start_time": "2021-02-15T16:43:32.186356Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R^2 de modelo: 97.59 %\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "[8 8 1 0 9 7 1 6 0 5]\n",
      "\n",
      "Valores Orginales: \n",
      "[8 8 1 0 9 7 1 6 0 5]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediccion\n",
    "\n",
    "valor = search.score(X_test,y_test)\n",
    "print('\\nR^2 de modelo:' ,np.round(valor*100.0,2),'%')\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(search.predict(X_test[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Clasificador con Keras + Scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Opcion 01 con Keras (Sin GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T00:30:48.427001Z",
     "start_time": "2021-02-16T00:27:56.368820Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "X_train shape:  (60000, 784)\n",
      "X_test shape:  (10000, 784)\n",
      "-------------------\n",
      "y_train shape:  (60000,)\n",
      "y_test shape:  (10000,)\n",
      "-------------------\n",
      "Clases clasificacion:  10\n",
      "\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "47296/48000 [============================>.] - ETA: 0s - loss: 0.8011 - accuracy: 0.8758\n",
      "Epoch 00001: val_loss improved from inf to 0.44101, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 81us/sample - loss: 0.7964 - accuracy: 0.8765 - val_loss: 0.4410 - val_accuracy: 0.9386\n",
      "Epoch 2/50\n",
      "47584/48000 [============================>.] - ETA: 0s - loss: 0.3928 - accuracy: 0.9433\n",
      "Epoch 00002: val_loss improved from 0.44101 to 0.34230, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.3922 - accuracy: 0.9434 - val_loss: 0.3423 - val_accuracy: 0.9523\n",
      "Epoch 3/50\n",
      "47680/48000 [============================>.] - ETA: 0s - loss: 0.3227 - accuracy: 0.9553\n",
      "Epoch 00003: val_loss improved from 0.34230 to 0.29860, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 75us/sample - loss: 0.3224 - accuracy: 0.9554 - val_loss: 0.2986 - val_accuracy: 0.9604\n",
      "Epoch 4/50\n",
      "47488/48000 [============================>.] - ETA: 0s - loss: 0.2896 - accuracy: 0.9611\n",
      "Epoch 00004: val_loss improved from 0.29860 to 0.27270, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.2895 - accuracy: 0.9611 - val_loss: 0.2727 - val_accuracy: 0.9635\n",
      "Epoch 5/50\n",
      "47488/48000 [============================>.] - ETA: 0s - loss: 0.2722 - accuracy: 0.9631\n",
      "Epoch 00005: val_loss improved from 0.27270 to 0.26332, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.2723 - accuracy: 0.9631 - val_loss: 0.2633 - val_accuracy: 0.9647\n",
      "Epoch 6/50\n",
      "47488/48000 [============================>.] - ETA: 0s - loss: 0.2600 - accuracy: 0.9651 ETA: 0s - loss: 0.2583 - accuracy: 0. - ETA: 0s\n",
      "Epoch 00006: val_loss did not improve from 0.26332\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.2598 - accuracy: 0.9651 - val_loss: 0.2635 - val_accuracy: 0.9623\n",
      "Epoch 7/50\n",
      "47616/48000 [============================>.] - ETA: 0s - loss: 0.2504 - accuracy: 0.9667 ETA - ETA\n",
      "Epoch 00007: val_loss improved from 0.26332 to 0.25440, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 74us/sample - loss: 0.2502 - accuracy: 0.9667 - val_loss: 0.2544 - val_accuracy: 0.9659\n",
      "Epoch 8/50\n",
      "47424/48000 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.9682 ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.2\n",
      "Epoch 00008: val_loss improved from 0.25440 to 0.24676, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 75us/sample - loss: 0.2418 - accuracy: 0.9681 - val_loss: 0.2468 - val_accuracy: 0.9676\n",
      "Epoch 9/50\n",
      "47584/48000 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.9683 ETA: 1s - loss: 0.2330 - accura - ETA: 0s\n",
      "Epoch 00009: val_loss did not improve from 0.24676\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 0.2368 - accuracy: 0.9682 - val_loss: 0.2505 - val_accuracy: 0.9639\n",
      "Epoch 10/50\n",
      "47680/48000 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.9691 ETA: 0s - loss: 0.2294 - ac - ETA: 0s - loss: 0.230\n",
      "Epoch 00010: val_loss did not improve from 0.24676\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.2320 - accuracy: 0.9691 - val_loss: 0.2551 - val_accuracy: 0.9614\n",
      "Epoch 11/50\n",
      "47264/48000 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9700 ETA: 2s - - ETA: 1s - loss: 0.2255 - ac - ETA: 0s - loss: 0.2266 - ac - ETA: 0s - loss: 0\n",
      "Epoch 00011: val_loss improved from 0.24676 to 0.23124, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 74us/sample - loss: 0.2278 - accuracy: 0.9698 - val_loss: 0.2312 - val_accuracy: 0.9688\n",
      "Epoch 12/50\n",
      "47360/48000 [============================>.] - ETA: 0s - loss: 0.2245 - accuracy: 0.9700\n",
      "Epoch 00012: val_loss improved from 0.23124 to 0.22874, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 75us/sample - loss: 0.2245 - accuracy: 0.9699 - val_loss: 0.2287 - val_accuracy: 0.9688\n",
      "Epoch 13/50\n",
      "47872/48000 [============================>.] - ETA: 0s - loss: 0.2239 - accuracy: 0.9706 - ETA: 1s - loss: 0.2167 - accuracy: 0. - ETA: 1s - loss: 0 - ETA\n",
      "Epoch 00013: val_loss did not improve from 0.22874\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.2239 - accuracy: 0.9706 - val_loss: 0.2439 - val_accuracy: 0.9655\n",
      "Epoch 14/50\n",
      "47840/48000 [============================>.] - ETA: 0s - loss: 0.2217 - accuracy: 0.9703 ETA: 2s - loss: 0.2082 - accuracy:  - ETA: 2s - loss: 0.2091 - accuracy: 0.97 - ETA: 2s - loss: 0.2090 - accura - ETA: 1s - loss: 0.2108 - ac - ETA: 1s - loss: 0.2 - E\n",
      "Epoch 00014: val_loss did not improve from 0.22874\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.2220 - accuracy: 0.9702 - val_loss: 0.2422 - val_accuracy: 0.9636\n",
      "Epoch 15/50\n",
      "47840/48000 [============================>.] - ETA: 0s - loss: 0.2185 - accuracy: 0.9704 ETA: 2s - loss: 0.2047 - accura - ETA: 2s - loss: 0.2 - ETA: 0s - loss: 0.2166 \n",
      "Epoch 00015: val_loss improved from 0.22874 to 0.22690, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 73us/sample - loss: 0.2183 - accuracy: 0.9705 - val_loss: 0.2269 - val_accuracy: 0.9682\n",
      "Epoch 16/50\n",
      "47648/48000 [============================>.] - ETA: 0s - loss: 0.2149 - accuracy: 0.9714 E - ETA: 1s - loss: 0.2136 - ac - ETA: \n",
      "Epoch 00016: val_loss improved from 0.22690 to 0.22218, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 73us/sample - loss: 0.2148 - accuracy: 0.9714 - val_loss: 0.2222 - val_accuracy: 0.9680\n",
      "Epoch 17/50\n",
      "47744/48000 [============================>.] - ETA: 0s - loss: 0.2114 - accuracy: 0.9723 ETA:  - ETA: 1s - loss: 0.2102 - accuracy - ETA: 0s - loss: 0.2113 - accuracy:  - ETA: 0s - los\n",
      "Epoch 00017: val_loss improved from 0.22218 to 0.21817, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 74us/sample - loss: 0.2116 - accuracy: 0.9723 - val_loss: 0.2182 - val_accuracy: 0.9704\n",
      "Epoch 18/50\n",
      "47648/48000 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9710 ETA: 0s - loss: 0.2115 - accu - ETA: 0s - loss: 0.2115 - accuracy\n",
      "Epoch 00018: val_loss did not improve from 0.21817\n",
      "48000/48000 [==============================] - 4s 73us/sample - loss: 0.2126 - accuracy: 0.9710 - val_loss: 0.2284 - val_accuracy: 0.9668\n",
      "Epoch 19/50\n",
      "47264/48000 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9715 ETA: 2s - loss: 0.1963 - accuracy: 0. - ETA: \n",
      "Epoch 00019: val_loss improved from 0.21817 to 0.21221, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 74us/sample - loss: 0.2107 - accuracy: 0.9715 - val_loss: 0.2122 - val_accuracy: 0.9714\n",
      "Epoch 20/50\n",
      "47232/48000 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.9712\n",
      "Epoch 00020: val_loss did not improve from 0.21221\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 0.2096 - accuracy: 0.9711 - val_loss: 0.2277 - val_accuracy: 0.9683\n",
      "Epoch 21/50\n",
      "47328/48000 [============================>.] - ETA: 0s - loss: 0.2051 - accuracy: 0.9731 ETA: 1s - loss: 0.2021 - accuracy:  - ETA: 1s - loss: 0.2020 - accuracy:  - ETA: 1s - loss: - ETA: 0s - loss: 0.2029 - \n",
      "Epoch 00021: val_loss did not improve from 0.21221\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.2059 - accuracy: 0.9729 - val_loss: 0.2234 - val_accuracy: 0.9673\n",
      "Epoch 22/50\n",
      "47968/48000 [============================>.] - ETA: 0s - loss: 0.2048 - accuracy: 0.9735 ETA: 2s - loss: 0.2012 - ac - ETA: 2s - loss: 0.1973 - ac - ETA: 1s - loss: 0.1987 - accu - ETA: 1s - loss: 0.2013  - ETA: 1s - loss: 0.2014  - ETA: 0s - loss:\n",
      "Epoch 00022: val_loss did not improve from 0.21221\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.2048 - accuracy: 0.9735 - val_loss: 0.2162 - val_accuracy: 0.9686\n",
      "Epoch 23/50\n",
      "47552/48000 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9719 ETA: 0s - loss: - ETA: 0s - loss: 0.2059 - accuracy\n",
      "Epoch 00023: val_loss did not improve from 0.21221\n",
      "48000/48000 [==============================] - 3s 67us/sample - loss: 0.2061 - accuracy: 0.9719 - val_loss: 0.2301 - val_accuracy: 0.9644\n",
      "Epoch 24/50\n",
      "47872/48000 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9734 ETA: 1s - l - ETA: 0s - loss: 0 - ETA: 0s - loss: 0.2021 - accu\n",
      "Epoch 00024: val_loss improved from 0.21221 to 0.21009, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.2034 - accuracy: 0.9733 - val_loss: 0.2101 - val_accuracy: 0.9709\n",
      "Epoch 25/50\n",
      "47168/48000 [============================>.] - ETA: 0s - loss: 0.1999 - accuracy: 0.9733 ETA: 2s - l - ETA: 1s - loss: 0.1961 - \n",
      "Epoch 00025: val_loss improved from 0.21009 to 0.20745, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.1999 - accuracy: 0.9733 - val_loss: 0.2075 - val_accuracy: 0.9730\n",
      "Epoch 26/50\n",
      "47520/48000 [============================>.] - ETA: 0s - loss: 0.1993 - accuracy: 0.9732 ETA: 0s\n",
      "Epoch 00026: val_loss did not improve from 0.20745\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 0.1994 - accuracy: 0.9731 - val_loss: 0.2147 - val_accuracy: 0.9699\n",
      "Epoch 27/50\n",
      "47840/48000 [============================>.] - ETA: 0s - loss: 0.2011 - accuracy: 0.9721 ETA: 0s - loss: 0.1977 - ac - ETA: 0s - loss: 0.2\n",
      "Epoch 00027: val_loss did not improve from 0.20745\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.2013 - accuracy: 0.9720 - val_loss: 0.2101 - val_accuracy: 0.9718\n",
      "Epoch 28/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1968 - accuracy: 0.9740 ETA: 0s - loss: 0.1948  - ETA: 0s - loss: 0.1963 - accu\n",
      "Epoch 00028: val_loss did not improve from 0.20745\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.1969 - accuracy: 0.9740 - val_loss: 0.2110 - val_accuracy: 0.9682\n",
      "Epoch 29/50\n",
      "47456/48000 [============================>.] - ETA: 0s - loss: 0.1989 - accuracy: 0.9730 ETA: 1s - loss: 0.1931 - accuracy - ETA: 1s - loss: 0.1931 - accuracy:  - ETA: 1s - loss: 0.1939 - accuracy: 0. - ETA\n",
      "Epoch 00029: val_loss did not improve from 0.20745\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 0.1989 - accuracy: 0.9730 - val_loss: 0.2170 - val_accuracy: 0.9676\n",
      "Epoch 30/50\n",
      "47456/48000 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9737 ETA: 1s - loss: 0.1922 -  - ETA: 1s - l - ETA: 0s - loss: 0.1952 - accu - ETA: 0s - loss: 0.1961 - accura - ETA: 0s - loss: 0.1961 - accuracy: 0.97\n",
      "Epoch 00030: val_loss improved from 0.20745 to 0.20667, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.1963 - accuracy: 0.9738 - val_loss: 0.2067 - val_accuracy: 0.9706\n",
      "Epoch 31/50\n",
      "47392/48000 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9739 ETA: 1s - loss: 0.1894 - accuracy: 0. - ETA: 0s - loss: 0.1963 - \n",
      "Epoch 00031: val_loss did not improve from 0.20667\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.1953 - accuracy: 0.9740 - val_loss: 0.2192 - val_accuracy: 0.9662\n",
      "Epoch 32/50\n",
      "47328/48000 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9730 ETA: 0s - loss: 0.1942 - accuracy - ETA: 0s - los\n",
      "Epoch 00032: val_loss did not improve from 0.20667\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 0.1965 - accuracy: 0.9730 - val_loss: 0.2082 - val_accuracy: 0.9694\n",
      "Epoch 33/50\n",
      "47328/48000 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9732 ETA: 1s - loss: 0.1887 - ac - ETA: 1s - loss: 0.1909 - accu - ETA: 1s - los - ETA: 0s - loss: 0.1931 - accuracy - ETA: 0s - loss: 0.1934 - accuracy: 0.\n",
      "Epoch 00033: val_loss did not improve from 0.20667\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 0.1943 - accuracy: 0.9731 - val_loss: 0.2078 - val_accuracy: 0.9692\n",
      "Epoch 34/50\n",
      "47328/48000 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9753 ETA: 0s - loss: 0.1915 - accuracy\n",
      "Epoch 00034: val_loss did not improve from 0.20667\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.1915 - accuracy: 0.9755 - val_loss: 0.2131 - val_accuracy: 0.9693\n",
      "Epoch 35/50\n",
      "47648/48000 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9732 ETA: 1s - l - ETA: 0s - loss: 0.1882 -  - ETA: 0s - loss: 0.1\n",
      "Epoch 00035: val_loss did not improve from 0.20667\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 0.1942 - accuracy: 0.9732 - val_loss: 0.2230 - val_accuracy: 0.9652\n",
      "Epoch 36/50\n",
      "47424/48000 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9738\n",
      "Epoch 00036: val_loss improved from 0.20667 to 0.19983, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.1919 - accuracy: 0.9739 - val_loss: 0.1998 - val_accuracy: 0.9718\n",
      "Epoch 37/50\n",
      "47648/48000 [============================>.] - ETA: 0s - loss: 0.1924 - accuracy: 0.9741 ETA: 2s - loss: 0.1906 - accuracy: 0.97 - ETA: 2s - loss: 0.1915 - accura - ETA: 1s - loss: 0\n",
      "Epoch 00037: val_loss did not improve from 0.19983\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.1923 - accuracy: 0.9741 - val_loss: 0.2295 - val_accuracy: 0.9638\n",
      "Epoch 38/50\n",
      "47776/48000 [============================>.] - ETA: 0s - loss: 0.1915 - accuracy: 0.9742 E - ETA: 0s - loss: 0.1911 - \n",
      "Epoch 00038: val_loss did not improve from 0.19983\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.1914 - accuracy: 0.9743 - val_loss: 0.2070 - val_accuracy: 0.9689\n",
      "Epoch 39/50\n",
      "47424/48000 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9742 ETA: 0s -\n",
      "Epoch 00039: val_loss did not improve from 0.19983\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 0.1909 - accuracy: 0.9743 - val_loss: 0.2108 - val_accuracy: 0.9679\n",
      "Epoch 40/50\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 0.1909 - accuracy: 0.9739 ETA: 2s - loss: 0.1711  - ETA: 2s - loss: 0.1807 - accuracy: 0.97 - ETA: 1s -\n",
      "Epoch 00040: val_loss did not improve from 0.19983\n",
      "48000/48000 [==============================] - 3s 73us/sample - loss: 0.1910 - accuracy: 0.9739 - val_loss: 0.2233 - val_accuracy: 0.9624\n",
      "Epoch 41/50\n",
      "47840/48000 [============================>.] - ETA: 0s - loss: 0.1885 - accuracy: 0.9747\n",
      "Epoch 00041: val_loss did not improve from 0.19983\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 0.1885 - accuracy: 0.9746 - val_loss: 0.2046 - val_accuracy: 0.9703\n",
      "Epoch 42/50\n",
      "47680/48000 [============================>.] - ETA: 0s - loss: 0.1901 - accuracy: 0.9746 ETA: 2s - loss: 0.1775 - ac - ETA: 1s - loss: 0.1794 \n",
      "Epoch 00042: val_loss did not improve from 0.19983\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.1903 - accuracy: 0.9745 - val_loss: 0.2121 - val_accuracy: 0.9681\n",
      "Epoch 43/50\n",
      "47904/48000 [============================>.] - ETA: 0s - loss: 0.1888 - accuracy: 0.9742 ETA: 2s - l - ETA: 1s - los - ETA: 0s - loss: 0.1859 - accu - ETA: 0s - loss: 0.187\n",
      "Epoch 00043: val_loss did not improve from 0.19983\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 0.1888 - accuracy: 0.9743 - val_loss: 0.2027 - val_accuracy: 0.9693\n",
      "Epoch 44/50\n",
      "47296/48000 [============================>.] - ETA: 0s - loss: 0.1866 - accuracy: 0.9751\n",
      "Epoch 00044: val_loss did not improve from 0.19983\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.1870 - accuracy: 0.9750 - val_loss: 0.2109 - val_accuracy: 0.9705\n",
      "Epoch 45/50\n",
      "47328/48000 [============================>.] - ETA: 0s - loss: 0.1832 - accuracy: 0.9759\n",
      "Epoch 00045: val_loss improved from 0.19983 to 0.19767, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 0.1833 - accuracy: 0.9758 - val_loss: 0.1977 - val_accuracy: 0.9724\n",
      "Epoch 46/50\n",
      "47232/48000 [============================>.] - ETA: 0s - loss: 0.1802 - accuracy: 0.9764 ETA: 1s - loss: 0.1681 -  - ETA: 1s - loss: 0.1744  - ETA: 0s - loss: 0.1803 - accuracy: 0.9764\n",
      "Epoch 00046: val_loss improved from 0.19767 to 0.19047, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.1801 - accuracy: 0.9765 - val_loss: 0.1905 - val_accuracy: 0.9733\n",
      "Epoch 47/50\n",
      "47520/48000 [============================>.] - ETA: 0s - loss: 0.1813 - accuracy: 0.9759 ETA: 2s - loss: 0.1689 - accuracy: 0. - ETA: 2s - loss: - ETA: 1s - loss: 0.1\n",
      "Epoch 00047: val_loss did not improve from 0.19047\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.1816 - accuracy: 0.9758 - val_loss: 0.2043 - val_accuracy: 0.9697\n",
      "Epoch 48/50\n",
      "47232/48000 [============================>.] - ETA: 0s - loss: 0.1774 - accuracy: 0.9772 ETA: 0s - loss: 0.1761 - accuracy:  - ETA: 0s - loss: 0.1758 - accura - ETA: 0s - loss: 0.1763 - \n",
      "Epoch 00048: val_loss did not improve from 0.19047\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 0.1776 - accuracy: 0.9771 - val_loss: 0.1909 - val_accuracy: 0.9718\n",
      "Epoch 49/50\n",
      "47680/48000 [============================>.] - ETA: 0s - loss: 0.1802 - accuracy: 0.9756 ETA: 1s - loss: 0.1 - ETA: 0s - l - ETA: 0s - loss: 0.1804 - accuracy: 0.97\n",
      "Epoch 00049: val_loss did not improve from 0.19047\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 0.1803 - accuracy: 0.9756 - val_loss: 0.2357 - val_accuracy: 0.9606\n",
      "Epoch 50/50\n",
      "47328/48000 [============================>.] - ETA: 0s - loss: 0.1773 - accuracy: 0.9766\n",
      "Epoch 00050: val_loss did not improve from 0.19047\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 0.1773 - accuracy: 0.9766 - val_loss: 0.1952 - val_accuracy: 0.9732\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.1841 - accuracy: 0.9733\n",
      "\n",
      "\n",
      "Puntaje total:  0.9733\n"
     ]
    }
   ],
   "source": [
    "# modelo keras\n",
    "\n",
    "# cargar datos\n",
    "(X_train , y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacion entre 0-1\n",
    "X_train = X_train.reshape(60000,784).astype('float32')/255.0\n",
    "X_test = X_test.reshape(10000,784).astype('float32')/255.0\n",
    "\n",
    "clases = len(set(y_test))\n",
    "\n",
    "print('\\n\\nX_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('-------------------')\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "print('-------------------')\n",
    "print('Clases clasificacion: ',clases)\n",
    "print('\\n')\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "dimension = X_train.shape[1:]\n",
    "\n",
    "def crear_modelo(clase=clases,init='glorot_uniform',alpha=0.001): \n",
    "    # modelo secuencial\n",
    "    modeli = keras.Sequential()\n",
    "    modeli.add(Dense(80,activation='tanh',kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha),input_shape=dimension))\n",
    "    modeli.add(Dense(40,activation='tanh',kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "    modeli.add(Dense(20,activation='sigmoid',kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "    modeli.add(Dense(clase,activation='softmax'))\n",
    "    \n",
    "    #compilar y retornar objeto modelo\n",
    "    modeli.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam' ,metrics=['accuracy'])\n",
    "    return modeli\n",
    "\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# Callbacks para el entrenamiento\n",
    "direccion = 'mejor_modelo_entrenado.h5' # nombre de archivo a guardar modelo entrenado\n",
    "chk = keras.callbacks.ModelCheckpoint(direccion,save_best_only=True,verbose=2)\n",
    "stp = keras.callbacks.EarlyStopping(patience=20,mode='auto',min_delta=0,restore_best_weights=True,verbose=2)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(factor=0.9,patience=8,verbose=2)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "epochs = 50\n",
    "#batch_size = 128\n",
    "shuffle =True\n",
    "callbacks = [chk,stp,lrs]\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# wrapping... de keras a scikit model... esto convierte a model a tipo Scikite (posee .fit() , .predict() ,  .evaluate())\n",
    "model= tf.keras.wrappers.scikit_learn.KerasClassifier(crear_modelo,epochs=epochs,callbacks=callbacks,shuffle=True,validation_split=0.2)\n",
    "\n",
    "# entrenar modelo\n",
    "history = model.fit(X_train, y_train)\n",
    "\n",
    "#############################################################\n",
    "####### Ver curvas de Aprendizaje del modelo #############\n",
    "\n",
    "historia = history.history\n",
    "\n",
    "print('\\n\\nCurvas de Aprendizaje\\n')\n",
    "for nombre,valores in historia.items():\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.title('Curva de ' + nombre)\n",
    "    plt.xlabel('Interacion \"i\"')\n",
    "    plt.ylabel(nombre)\n",
    "    plt.plot(valores)\n",
    "    plt.show()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# test de rendimiento - mse\n",
    "puntaje = model.score(X_test,y_test)\n",
    "print('\\n\\nPuntaje total: ',puntaje)\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T00:33:21.557891Z",
     "start_time": "2021-02-16T00:33:20.917736Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.1841 - accuracy: 0.9733\n",
      "\n",
      "Puntaje de modelo: 0.97\n",
      "------------------------------\n",
      "\n",
      "Valores de Prediccion: \n",
      "10/10 [==============================] - 0s 499us/sample\n",
      "[4 1 3 0 1 0 0 4 4 7]\n",
      "\n",
      "Valores Orginales: \n",
      "[4 8 3 6 1 0 0 9 4 9]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predecir - Aplicar Modelo\n",
    "\n",
    "valor = model.score(X_test,y_test)\n",
    "print('\\nPuntaje de modelo:' ,np.round(valor,2))\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10  # cantidad de muestras a elegir aleatoriamente\n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "print('\\nValores de Prediccion: ')\n",
    "print(np.round(search.predict(X_test[indices]),3))\n",
    "print('\\nValores Orginales: ')\n",
    "print(np.round(y_test[indices],3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Opcion 02 - Keras + GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T00:26:38.054625Z",
     "start_time": "2021-02-16T00:22:59.951034Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "X_train shape:  (60000, 784)\n",
      "X_test shape:  (10000, 784)\n",
      "-------------------\n",
      "y_train shape:  (60000,)\n",
      "y_test shape:  (10000,)\n",
      "-------------------\n",
      "Clases clasificacion:  10\n",
      "\n",
      "\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 78us/sample - loss: 2.1382 - accuracy: 0.5261 - val_loss: 1.7924 - val_accuracy: 0.6699\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 1.7986 - accuracy: 0.6649\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 78us/sample - loss: 2.1385 - accuracy: 0.4658 - val_loss: 1.7979 - val_accuracy: 0.6519y: 0.\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.8269 - accuracy: 0.6204\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.1754 - accuracy: 0.4658 - val_loss: 1.8326 - val_accuracy: 0.6687\n",
      "12000/12000 [==============================] - 0s 40us/sample - loss: 1.8577 - accuracy: 0.6368\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 76us/sample - loss: 2.1919 - accuracy: 0.4773 - val_loss: 1.8942 - val_accuracy: 0.6499\n",
      "12000/12000 [==============================] - 0s 38us/sample - loss: 1.9184 - accuracy: 0.6262\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 76us/sample - loss: 2.2290 - accuracy: 0.3876 - val_loss: 1.9276 - val_accuracy: 0.55292.2703 - accu\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.9146 - accuracy: 0.5595\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 74us/sample - loss: 2.1572 - accuracy: 0.4351 - val_loss: 1.8396 - val_accuracy: 0.6652\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.8380 - accuracy: 0.6593\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 77us/sample - loss: 2.2146 - accuracy: 0.4053 - val_loss: 1.9154 - val_accuracy: 0.6209s - ETA: 0s - l\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 1.9353 - accuracy: 0.5927\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 76us/sample - loss: 2.1876 - accuracy: 0.4453 - val_loss: 1.8672 - val_accuracy: 0.6293\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.8873 - accuracy: 0.6069\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 78us/sample - loss: 2.2052 - accuracy: 0.3728 - val_loss: 1.9294 - val_accuracy: 0.5077ss: 2.3503 - accu - ETA: 0s -\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 1.9348 - accuracy: 0.5052\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 77us/sample - loss: 2.1595 - accuracy: 0.4586 - val_loss: 1.8676 - val_accuracy: 0.6119\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.8504 - accuracy: 0.6323\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.1686 - accuracy: 0.3784 - val_loss: 1.8750 - val_accuracy: 0.5292: 2.4\n",
      "12000/12000 [==============================] - 0s 39us/sample - loss: 1.8791 - accuracy: 0.5203\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 76us/sample - loss: 2.2088 - accuracy: 0.3769 - val_loss: 1.9055 - val_accuracy: 0.6232ETA: 0s - loss: 2.2668 - accuracy: 0. - ETA: 0s - loss: 2.2537 - ac\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.9217 - accuracy: 0.5987\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.1064 - accuracy: 0.4003 - val_loss: 1.7663 - val_accuracy: 0.5725184\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 1.7905 - accuracy: 0.5418\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 77us/sample - loss: 2.1179 - accuracy: 0.4506 - val_loss: 1.8115 - val_accuracy: 0.5915 - accuracy - ETA: 1s - loss: 2.2362 - accuracy - ETA: 0s - los\n",
      "12000/12000 [==============================] - 0s 40us/sample - loss: 1.8343 - accuracy: 0.5689\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 76us/sample - loss: 2.1304 - accuracy: 0.4369 - val_loss: 1.8557 - val_accuracy: 0.6170s - loss: 2\n",
      "12000/12000 [==============================] - 0s 38us/sample - loss: 1.8469 - accuracy: 0.6347\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 78us/sample - loss: 2.0593 - accuracy: 0.4314 - val_loss: 1.7550 - val_accuracy: 0.5655 0s - loss: 2.1344 -  - ETA: 0s - loss: 2.0820 - accuracy\n",
      "12000/12000 [==============================] - 0s 38us/sample - loss: 1.7604 - accuracy: 0.5615\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 80us/sample - loss: 2.0863 - accuracy: 0.4857 - val_loss: 1.7635 - val_accuracy: 0.6899\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 1.7815 - accuracy: 0.6683\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 81us/sample - loss: 2.1758 - accuracy: 0.4179 - val_loss: 1.8883 - val_accuracy: 0.6324y: 0.41\n",
      "12000/12000 [==============================] - 0s 38us/sample - loss: 1.9073 - accuracy: 0.6028\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 80us/sample - loss: 2.1802 - accuracy: 0.3547 - val_loss: 1.9161 - val_accuracy: 0.5130\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.9277 - accuracy: 0.5004\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 78us/sample - loss: 2.1817 - accuracy: 0.3364 - val_loss: 1.9275 - val_accuracy: 0.4957\n",
      "12000/12000 [==============================] - 0s 35us/sample - loss: 1.9222 - accuracy: 0.5068\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 74us/sample - loss: 2.5199 - accuracy: 0.1073 - val_loss: 2.5023 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.5010 - accuracy: 0.1126\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.5235 - accuracy: 0.1110 - val_loss: 2.5021 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 35us/sample - loss: 2.5011 - accuracy: 0.1151\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.5185 - accuracy: 0.1079 - val_loss: 2.5002 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 34us/sample - loss: 2.4994 - accuracy: 0.1151\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 74us/sample - loss: 2.5237 - accuracy: 0.1067 - val_loss: 2.5007 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 38us/sample - loss: 2.5003 - accuracy: 0.1131\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 71us/sample - loss: 2.5136 - accuracy: 0.1108 - val_loss: 2.5017 - val_accuracy: 0.1152\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.5029 - accuracy: 0.1060\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 79us/sample - loss: 2.5039 - accuracy: 0.1092 - val_loss: 2.4887 - val_accuracy: 0.1067A: 0s - loss: 2.5102 - ac - ETA: 0s - loss: 2.507\n",
      "12000/12000 [==============================] - 0s 41us/sample - loss: 2.4877 - accuracy: 0.1132\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 74us/sample - loss: 2.5109 - accuracy: 0.1076 - val_loss: 2.4934 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.4930 - accuracy: 0.1151\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.5085 - accuracy: 0.1079 - val_loss: 2.4913 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 38us/sample - loss: 2.4908 - accuracy: 0.1151\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.5204 - accuracy: 0.1137 - val_loss: 2.4923 - val_accuracy: 0.1065- loss: 2.5210 - accuracy: 0.11\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 2.4921 - accuracy: 0.1131\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.5001 - accuracy: 0.1166 - val_loss: 2.4878 - val_accuracy: 0.1152\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.4889 - accuracy: 0.1060\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.4307 - accuracy: 0.1143 - val_loss: 2.4223 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.4213 - accuracy: 0.1126\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 79us/sample - loss: 2.4423 - accuracy: 0.1068 - val_loss: 2.4263 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 39us/sample - loss: 2.4255 - accuracy: 0.1151\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 77us/sample - loss: 2.4377 - accuracy: 0.1148 - val_loss: 2.4250 - val_accuracy: 0.1065loss: - ETA\n",
      "12000/12000 [==============================] - 0s 41us/sample - loss: 2.4240 - accuracy: 0.1151s - loss: 2.4237 - accu\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 77us/sample - loss: 2.4473 - accuracy: 0.1222 - val_loss: 2.4247 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 39us/sample - loss: 2.4241 - accuracy: 0.1131\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 79us/sample - loss: 2.4380 - accuracy: 0.1136 - val_loss: 2.4236 - val_accuracy: 0.1152\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 2.4246 - accuracy: 0.1060\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 72us/sample - loss: 2.4251 - accuracy: 0.1099 - val_loss: 2.4176 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.4160 - accuracy: 0.1126\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.4417 - accuracy: 0.1116 - val_loss: 2.4197 - val_accuracy: 0.10650 - ETA: 1s - loss: 2.4611  - ETA: 0s - loss:\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.4184 - accuracy: 0.1151\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 74us/sample - loss: 2.4437 - accuracy: 0.1069 - val_loss: 2.4185 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.4174 - accuracy: 0.1151\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 76us/sample - loss: 2.4296 - accuracy: 0.1136 - val_loss: 2.4177 - val_accuracy: 0.1065\n",
      "12000/12000 [==============================] - 0s 40us/sample - loss: 2.4168 - accuracy: 0.1131\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 76us/sample - loss: 2.4335 - accuracy: 0.1142 - val_loss: 2.4206 - val_accuracy: 0.1152\n",
      "12000/12000 [==============================] - 0s 42us/sample - loss: 2.4214 - accuracy: 0.1060\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 77us/sample - loss: 2.3314 - accuracy: 0.2877 - val_loss: 2.0305 - val_accuracy: 0.5033\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 2.0338 - accuracy: 0.4927\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.2250 - accuracy: 0.3988 - val_loss: 1.8385 - val_accuracy: 0.6746ss: 2.4996  - ETA: 1s - loss: 2.4249 - accura - ETA\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.8600 - accuracy: 0.6423\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 72us/sample - loss: 2.2441 - accuracy: 0.3782 - val_loss: 1.8548 - val_accuracy: 0.5779\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 1.8738 - accuracy: 0.5625\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.3190 - accuracy: 0.3083 - val_loss: 2.0152 - val_accuracy: 0.5003\n",
      "12000/12000 [==============================] - 0s 38us/sample - loss: 2.0294 - accuracy: 0.4910\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.2672 - accuracy: 0.3644 - val_loss: 1.9264 - val_accuracy: 0.5248\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.9184 - accuracy: 0.5248\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.3097 - accuracy: 0.3485 - val_loss: 2.0059 - val_accuracy: 0.56072.4473 - accu - ETA: \n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.0054 - accuracy: 0.5565\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 72us/sample - loss: 2.2932 - accuracy: 0.3699 - val_loss: 1.9726 - val_accuracy: 0.5424\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.9812 - accuracy: 0.5405\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 71us/sample - loss: 2.2836 - accuracy: 0.3860 - val_loss: 1.9638 - val_accuracy: 0.5696\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 1.9781 - accuracy: 0.5540\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 4s 92us/sample - loss: 2.3579 - accuracy: 0.2918 - val_loss: 2.0516 - val_accuracy: 0.4946\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 2.0629 - accuracy: 0.4848\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 79us/sample - loss: 2.3483 - accuracy: 0.3609 - val_loss: 2.0435 - val_accuracy: 0.5408- loss: 2.3937 - \n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 2.0363 - accuracy: 0.5415\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 80us/sample - loss: 2.2666 - accuracy: 0.3262 - val_loss: 1.9336 - val_accuracy: 0.4984\n",
      "12000/12000 [==============================] - 0s 39us/sample - loss: 1.9370 - accuracy: 0.4957\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 76us/sample - loss: 2.2354 - accuracy: 0.4116 - val_loss: 1.9047 - val_accuracy: 0.5386\n",
      "12000/12000 [==============================] - 0s 38us/sample - loss: 1.9192 - accuracy: 0.5290\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 76us/sample - loss: 2.2188 - accuracy: 0.3054 - val_loss: 1.9065 - val_accuracy: 0.5218\n",
      "12000/12000 [==============================] - 0s 38us/sample - loss: 1.9218 - accuracy: 0.5173\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.2951 - accuracy: 0.2967 - val_loss: 2.0356 - val_accuracy: 0.5264\n",
      "12000/12000 [==============================] - ETA: 0s - loss: 2.0438 - accuracy: 0.52 - 0s 37us/sample - loss: 2.0421 - accuracy: 0.5233\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 79us/sample - loss: 2.2880 - accuracy: 0.3283 - val_loss: 2.0272 - val_accuracy: 0.5131\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 2.0112 - accuracy: 0.5370\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.2163 - accuracy: 0.3092 - val_loss: 1.9137 - val_accuracy: 0.4603\n",
      "12000/12000 [==============================] - 0s 40us/sample - loss: 1.9263 - accuracy: 0.4432\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 79us/sample - loss: 2.1948 - accuracy: 0.4176 - val_loss: 1.8379 - val_accuracy: 0.5962\n",
      "12000/12000 [==============================] - 0s 39us/sample - loss: 1.8591 - accuracy: 0.5741\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 81us/sample - loss: 2.2199 - accuracy: 0.3468 - val_loss: 1.8951 - val_accuracy: 0.5715\n",
      "12000/12000 [==============================] - 0s 40us/sample - loss: 1.9043 - accuracy: 0.5650\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 75us/sample - loss: 2.2592 - accuracy: 0.3249 - val_loss: 2.0387 - val_accuracy: 0.4430loss: 2.382 - ETA: 1s\n",
      "12000/12000 [==============================] - 0s 37us/sample - loss: 2.0412 - accuracy: 0.4398\n",
      "Train on 38400 samples, validate on 9600 samples\n",
      "38400/38400 [==============================] - 3s 78us/sample - loss: 2.2446 - accuracy: 0.3045 - val_loss: 1.9482 - val_accuracy: 0.5178\n",
      "12000/12000 [==============================] - 0s 36us/sample - loss: 1.9368 - accuracy: 0.5326\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "48000/48000 [==============================] - 4s 77us/sample - loss: 2.1174 - accuracy: 0.4473 - val_loss: 1.7515 - val_accuracy: 0.6438ss: 2.5072 - ac - ETA: 2s - loss: 2.4329 - accuracy: 0. - ETA: 2s - loss: 2.4135 - accuracy:  - ETA: 2s - loss: 2.3868 - accu - ETA: 1s - loss: 2.3348 -  - ETA: 1s - loss: 2.2718 - accuracy - ETA: 1s - loss: 2.2425 - accuracy: 0.38 - ETA: \n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 1.7555 - accuracy: 0.6329\n",
      "\n",
      "\n",
      "----------------------------\n",
      "Mejores parametros: \n",
      "{'activa': 'tanh', 'h1': 80, 'h2': 40, 'h3': 10}\n",
      "Mejor Puntaje: 0.633\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modelo GridSearchCV + Pipeline + Keras \n",
    "\n",
    "# cargar datos\n",
    "(X_train , y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacion entre 0-1\n",
    "X_train = X_train.reshape(60000,784).astype('float32')/255.0\n",
    "X_test = X_test.reshape(10000,784).astype('float32')/255.0\n",
    "\n",
    "clases = len(set(y_test))\n",
    "\n",
    "print('\\n\\nX_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('-------------------')\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "print('-------------------')\n",
    "print('Clases clasificacion: ',clases)\n",
    "print('\\n')\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "dimension = X_train.shape[1:]\n",
    "\n",
    "def crear_modelo(clase=clases,optimizer='sgd',init='glorot_uniform',h1=80,h2=40,h3=20,alpha=0.001,activa='tanh'): \n",
    "    # modelo secuencial\n",
    "    modeli = keras.Sequential()\n",
    "    modeli.add(Dense(h1,activation=activa,kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha),input_shape=dimension))\n",
    "    modeli.add(Dense(h2,activation=activa,kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "    modeli.add(Dense(h3,activation='sigmoid',kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "    modeli.add(Dense(clase,activation='softmax'))\n",
    "    \n",
    "    #compilar y retornar objeto modelo\n",
    "    modeli.compile(loss='sparse_categorical_crossentropy', optimizer = optimizer ,metrics=['accuracy'])\n",
    "    return modeli\n",
    "\n",
    "# Callbacks para el entrenamiento\n",
    "direccion = 'mejor_modelo_entrenado.h5' # nombre de archivo a guardar modelo entrenado\n",
    "chk = keras.callbacks.ModelCheckpoint(direccion,save_best_only=True,verbose=2)\n",
    "stp = keras.callbacks.EarlyStopping(patience=20,mode='auto',min_delta=0,restore_best_weights=True,verbose=2)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(factor=0.9,patience=8,verbose=2)\n",
    "\n",
    "\n",
    "#####################################\n",
    "### Parametros de entrenamiento #####\n",
    "epochs = 25\n",
    "shuffle =True\n",
    "callbacks = [chk,stp,lrs]\n",
    "#batch_size = 128\n",
    "\n",
    "# wrapping... de keras a scikit model... esto convierte a model a tipo Scikite (posee .fit() , .predict() ,  .evaluate())\n",
    "model= tf.keras.wrappers.scikit_learn.KerasClassifier(crear_modelo,shuffle=True,validation_split=0.2,verbose=1)\n",
    "\n",
    "\n",
    "# Grilla de busqueda de mejores parametros para la ANN\n",
    "#optimizers = ['rmsprop', 'adam']\n",
    "activadores = ['tanh','sigmoid','relu']\n",
    "#inits = ['glorot_uniform', 'normal']\n",
    "h1s = [80,40]\n",
    "h2s = [40,30]\n",
    "h3s = [10]\n",
    "\n",
    "# grilla de parametros\n",
    "param_grid = {'activa':activadores,\n",
    "             'h1': h1s,\n",
    "             'h2': h2s,\n",
    "             'h3': h3s}\n",
    "\n",
    "# Gridsearch CV\n",
    "search = GridSearchCV(model, param_grid)\n",
    "\n",
    "# entrenar modelos y busqueda mediante GridSearchCV\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# ver ajuste del modelo entrenado\n",
    "valor = search.score(X_test,y_test)\n",
    "\n",
    "# Mejores parametros\n",
    "print('\\n\\n----------------------------')\n",
    "print('Mejores parametros: ')\n",
    "print(search.best_params_)\n",
    "print('Mejor Puntaje:' ,np.round(valor,3))\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T00:27:37.777911Z",
     "start_time": "2021-02-16T00:27:37.220668Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 46us/sample - loss: 1.7555 - accuracy: 0.6329\n",
      "\n",
      "Puntaje de modelo: 0.63\n",
      "------------------------------\n",
      "10/10 [==============================] - 0s 2ms/sample\n",
      "\n",
      "Probabilidades de prediccion: \n",
      "[[0.357 0.047 0.208 0.045 0.036 0.102 0.067 0.03  0.051 0.055]\n",
      " [0.1   0.023 0.059 0.048 0.233 0.096 0.037 0.089 0.138 0.177]\n",
      " [0.038 0.133 0.074 0.06  0.127 0.029 0.067 0.247 0.078 0.146]\n",
      " [0.066 0.271 0.148 0.135 0.02  0.053 0.161 0.058 0.045 0.042]\n",
      " [0.054 0.298 0.127 0.148 0.017 0.059 0.176 0.045 0.043 0.034]\n",
      " [0.029 0.441 0.097 0.083 0.025 0.031 0.103 0.111 0.039 0.041]\n",
      " [0.246 0.017 0.089 0.063 0.098 0.206 0.051 0.023 0.118 0.089]\n",
      " [0.022 0.294 0.081 0.074 0.055 0.022 0.067 0.243 0.054 0.087]\n",
      " [0.308 0.042 0.151 0.059 0.047 0.163 0.077 0.024 0.072 0.057]\n",
      " [0.03  0.057 0.044 0.08  0.199 0.052 0.049 0.172 0.129 0.187]]\n",
      "------------------------------\n",
      "\n",
      "Clases Prediccion: \n",
      "10/10 [==============================] - 0s 399us/sample\n",
      "[0 4 7 1 1 1 0 1 0 4]\n",
      "\n",
      "Clases Orginales: \n",
      "[0 9 4 5 2 1 0 7 0 9]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predecir - Aplicar Modelo\n",
    "\n",
    "valor = search.score(X_test,y_test)\n",
    "print('\\nPuntaje de modelo:' ,np.round(valor,2))\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10 \n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "predichos = search.predict_proba(X_test[indices])\n",
    "predichos = np.round(predichos,3)\n",
    "\n",
    "print('\\nProbabilidades de prediccion: ')\n",
    "print(predichos)\n",
    "print('------------------------------')\n",
    "\n",
    "print('\\nClases Prediccion: ')\n",
    "print(search.predict(X_test[indices]))\n",
    "print('\\nClases Orginales: ')\n",
    "print(y_test[indices])\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Clasificador con Keras + Scikit (Con GridSearchCV & Opcion Multicapas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br><br><b>Importante !!!</b>: Es vital tener en cuenta que si lo que se quiere es convertir un modelo <b>Keras</b> a uno tipo <b>Scikit</b> para poder aplicar sus bondades como es el GridSearchCV, no se puede usar todos los modelos de Keras, el recomendado o unica opcion es usar <b>keras.Sequential()</b> esto debido a que para el Clasificador le brinda acceso al calculo mediante <b>modelo.predic_proba</b> y <b>modelo.predic</b>, estos son fundamentales en el calculo una vez entrenado (fit) el modelo a los datos.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Opcion Sin GridSearch Multicapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T01:30:05.857799Z",
     "start_time": "2021-02-17T01:27:50.046454Z"
    },
    "code_folding": [
     0,
     3
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "X_train shape:  (60000, 784)\n",
      "X_test shape:  (10000, 784)\n",
      "-------------------\n",
      "y_train shape:  (60000,)\n",
      "y_test shape:  (10000,)\n",
      "-------------------\n",
      "Clases clasificacion:  10\n",
      "\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/40\n",
      "47360/48000 [============================>.] - ETA: 0s - loss: 2.2765 - accuracy: 0.4096\n",
      "Epoch 00001: val_loss improved from inf to 2.03515, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 4s 79us/sample - loss: 2.2742 - accuracy: 0.4119 - val_loss: 2.0352 - val_accuracy: 0.6364\n",
      "Epoch 2/40\n",
      "47776/48000 [============================>.] - ETA: 0s - loss: 2.0436 - accuracy: 0.6234\n",
      "Epoch 00002: val_loss improved from 2.03515 to 1.92749, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 2.0433 - accuracy: 0.6237 - val_loss: 1.9275 - val_accuracy: 0.7287\n",
      "Epoch 3/40\n",
      "47968/48000 [============================>.] - ETA: 0s - loss: 1.9610 - accuracy: 0.7022\n",
      "Epoch 00003: val_loss improved from 1.92749 to 1.84527, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.9610 - accuracy: 0.7022 - val_loss: 1.8453 - val_accuracy: 0.8176\n",
      "Epoch 4/40\n",
      "47456/48000 [============================>.] - ETA: 0s - loss: 1.9061 - accuracy: 0.7486\n",
      "Epoch 00004: val_loss improved from 1.84527 to 1.79917, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 73us/sample - loss: 1.9058 - accuracy: 0.7489 - val_loss: 1.7992 - val_accuracy: 0.8500\n",
      "Epoch 5/40\n",
      "47200/48000 [============================>.] - ETA: 0s - loss: 1.8663 - accuracy: 0.7827\n",
      "Epoch 00005: val_loss improved from 1.79917 to 1.76972, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 1.8659 - accuracy: 0.7832 - val_loss: 1.7697 - val_accuracy: 0.8683\n",
      "Epoch 6/40\n",
      "47296/48000 [============================>.] - ETA: 0s - loss: 1.8391 - accuracy: 0.8005\n",
      "Epoch 00006: val_loss improved from 1.76972 to 1.74989, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.8388 - accuracy: 0.8008 - val_loss: 1.7499 - val_accuracy: 0.8794\n",
      "Epoch 7/40\n",
      "47744/48000 [============================>.] - ETA: 0s - loss: 1.8204 - accuracy: 0.8114\n",
      "Epoch 00007: val_loss improved from 1.74989 to 1.73711, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.8203 - accuracy: 0.8114 - val_loss: 1.7371 - val_accuracy: 0.8843\n",
      "Epoch 8/40\n",
      "47136/48000 [============================>.] - ETA: 0s - loss: 1.8035 - accuracy: 0.8226\n",
      "Epoch 00008: val_loss improved from 1.73711 to 1.72593, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 1.8033 - accuracy: 0.8227 - val_loss: 1.7259 - val_accuracy: 0.8903\n",
      "Epoch 9/40\n",
      "47424/48000 [============================>.] - ETA: 0s - loss: 1.7917 - accuracy: 0.8275\n",
      "Epoch 00009: val_loss improved from 1.72593 to 1.71563, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 73us/sample - loss: 1.7918 - accuracy: 0.8274 - val_loss: 1.7156 - val_accuracy: 0.8945\n",
      "Epoch 10/40\n",
      "47424/48000 [============================>.] - ETA: 0s - loss: 1.7808 - accuracy: 0.8326\n",
      "Epoch 00010: val_loss improved from 1.71563 to 1.70724, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.7808 - accuracy: 0.8325 - val_loss: 1.7072 - val_accuracy: 0.8972\n",
      "Epoch 11/40\n",
      "47584/48000 [============================>.] - ETA: 0s - loss: 1.7715 - accuracy: 0.8374\n",
      "Epoch 00011: val_loss improved from 1.70724 to 1.69926, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.7714 - accuracy: 0.8375 - val_loss: 1.6993 - val_accuracy: 0.9003\n",
      "Epoch 12/40\n",
      "47264/48000 [============================>.] - ETA: 0s - loss: 1.7630 - accuracy: 0.8402\n",
      "Epoch 00012: val_loss improved from 1.69926 to 1.69274, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 72us/sample - loss: 1.7630 - accuracy: 0.8402 - val_loss: 1.6927 - val_accuracy: 0.9014\n",
      "Epoch 13/40\n",
      "47968/48000 [============================>.] - ETA: 0s - loss: 1.7529 - accuracy: 0.8459\n",
      "Epoch 00013: val_loss improved from 1.69274 to 1.68708, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 1.7529 - accuracy: 0.8459 - val_loss: 1.6871 - val_accuracy: 0.9032\n",
      "Epoch 14/40\n",
      "47616/48000 [============================>.] - ETA: 0s - loss: 1.7486 - accuracy: 0.8461\n",
      "Epoch 00014: val_loss improved from 1.68708 to 1.68186, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 73us/sample - loss: 1.7484 - accuracy: 0.8464 - val_loss: 1.6819 - val_accuracy: 0.9042\n",
      "Epoch 15/40\n",
      "47328/48000 [============================>.] - ETA: 0s - loss: 1.7421 - accuracy: 0.8485\n",
      "Epoch 00015: val_loss improved from 1.68186 to 1.67555, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 1.7422 - accuracy: 0.8483 - val_loss: 1.6756 - val_accuracy: 0.9079\n",
      "Epoch 16/40\n",
      "47712/48000 [============================>.] - ETA: 0s - loss: 1.7365 - accuracy: 0.8500\n",
      "Epoch 00016: val_loss improved from 1.67555 to 1.67224, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.7365 - accuracy: 0.8500 - val_loss: 1.6722 - val_accuracy: 0.9057\n",
      "Epoch 17/40\n",
      "47360/48000 [============================>.] - ETA: 0s - loss: 1.7295 - accuracy: 0.8542\n",
      "Epoch 00017: val_loss improved from 1.67224 to 1.66730, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.7295 - accuracy: 0.8540 - val_loss: 1.6673 - val_accuracy: 0.9072\n",
      "Epoch 18/40\n",
      "47968/48000 [============================>.] - ETA: 0s - loss: 1.7249 - accuracy: 0.8550\n",
      "Epoch 00018: val_loss improved from 1.66730 to 1.66305, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 1.7249 - accuracy: 0.8551 - val_loss: 1.6630 - val_accuracy: 0.9100\n",
      "Epoch 19/40\n",
      "47424/48000 [============================>.] - ETA: 0s - loss: 1.7191 - accuracy: 0.8580 ETA: 0s -\n",
      "Epoch 00019: val_loss improved from 1.66305 to 1.65956, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.7189 - accuracy: 0.8581 - val_loss: 1.6596 - val_accuracy: 0.9094\n",
      "Epoch 20/40\n",
      "47776/48000 [============================>.] - ETA: 0s - loss: 1.7161 - accuracy: 0.8574\n",
      "Epoch 00020: val_loss improved from 1.65956 to 1.65572, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.7161 - accuracy: 0.8574 - val_loss: 1.6557 - val_accuracy: 0.9100\n",
      "Epoch 21/40\n",
      "47392/48000 [============================>.] - ETA: 0s - loss: 1.7106 - accuracy: 0.8602\n",
      "Epoch 00021: val_loss improved from 1.65572 to 1.65182, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 1.7107 - accuracy: 0.8602 - val_loss: 1.6518 - val_accuracy: 0.9125\n",
      "Epoch 22/40\n",
      "47520/48000 [============================>.] - ETA: 0s - loss: 1.7079 - accuracy: 0.8597\n",
      "Epoch 00022: val_loss improved from 1.65182 to 1.64865, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.7078 - accuracy: 0.8599 - val_loss: 1.6487 - val_accuracy: 0.9127\n",
      "Epoch 23/40\n",
      "47424/48000 [============================>.] - ETA: 0s - loss: 1.7020 - accuracy: 0.8634\n",
      "Epoch 00023: val_loss improved from 1.64865 to 1.64538, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.7021 - accuracy: 0.8632 - val_loss: 1.6454 - val_accuracy: 0.9130\n",
      "Epoch 24/40\n",
      "47584/48000 [============================>.] - ETA: 0s - loss: 1.7003 - accuracy: 0.8635\n",
      "Epoch 00024: val_loss improved from 1.64538 to 1.64397, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 1.7003 - accuracy: 0.8635 - val_loss: 1.6440 - val_accuracy: 0.9121\n",
      "Epoch 25/40\n",
      "47616/48000 [============================>.] - ETA: 0s - loss: 1.6963 - accuracy: 0.8650\n",
      "Epoch 00025: val_loss improved from 1.64397 to 1.63996, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.6963 - accuracy: 0.8650 - val_loss: 1.6400 - val_accuracy: 0.9133\n",
      "Epoch 26/40\n",
      "47328/48000 [============================>.] - ETA: 0s - loss: 1.6942 - accuracy: 0.8644\n",
      "Epoch 00026: val_loss improved from 1.63996 to 1.63735, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 1.6943 - accuracy: 0.8643 - val_loss: 1.6373 - val_accuracy: 0.9137\n",
      "Epoch 27/40\n",
      "47488/48000 [============================>.] - ETA: 0s - loss: 1.6903 - accuracy: 0.8675\n",
      "Epoch 00027: val_loss improved from 1.63735 to 1.63477, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 1.6902 - accuracy: 0.8676 - val_loss: 1.6348 - val_accuracy: 0.9150\n",
      "Epoch 28/40\n",
      "47744/48000 [============================>.] - ETA: 0s - loss: 1.6890 - accuracy: 0.8662\n",
      "Epoch 00028: val_loss improved from 1.63477 to 1.63294, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 1.6889 - accuracy: 0.8663 - val_loss: 1.6329 - val_accuracy: 0.9146\n",
      "Epoch 29/40\n",
      "47936/48000 [============================>.] - ETA: 0s - loss: 1.6864 - accuracy: 0.8669\n",
      "Epoch 00029: val_loss improved from 1.63294 to 1.62999, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 1.6864 - accuracy: 0.8670 - val_loss: 1.6300 - val_accuracy: 0.9158\n",
      "Epoch 30/40\n",
      "47552/48000 [============================>.] - ETA: 0s - loss: 1.6819 - accuracy: 0.8704\n",
      "Epoch 00030: val_loss improved from 1.62999 to 1.62831, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 67us/sample - loss: 1.6819 - accuracy: 0.8704 - val_loss: 1.6283 - val_accuracy: 0.9161\n",
      "Epoch 31/40\n",
      "47168/48000 [============================>.] - ETA: 0s - loss: 1.6798 - accuracy: 0.8708\n",
      "Epoch 00031: val_loss improved from 1.62831 to 1.62618, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 69us/sample - loss: 1.6797 - accuracy: 0.8709 - val_loss: 1.6262 - val_accuracy: 0.9180\n",
      "Epoch 32/40\n",
      "47296/48000 [============================>.] - ETA: 0s - loss: 1.6778 - accuracy: 0.8717\n",
      "Epoch 00032: val_loss improved from 1.62618 to 1.62438, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 67us/sample - loss: 1.6780 - accuracy: 0.8714 - val_loss: 1.6244 - val_accuracy: 0.9166\n",
      "Epoch 33/40\n",
      "47232/48000 [============================>.] - ETA: 0s - loss: 1.6764 - accuracy: 0.8706\n",
      "Epoch 00033: val_loss improved from 1.62438 to 1.62344, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.6764 - accuracy: 0.8706 - val_loss: 1.6234 - val_accuracy: 0.9162\n",
      "Epoch 34/40\n",
      "47456/48000 [============================>.] - ETA: 0s - loss: 1.6728 - accuracy: 0.8739\n",
      "Epoch 00034: val_loss improved from 1.62344 to 1.62217, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.6728 - accuracy: 0.8739 - val_loss: 1.6222 - val_accuracy: 0.9162\n",
      "Epoch 35/40\n",
      "47808/48000 [============================>.] - ETA: 0s - loss: 1.6723 - accuracy: 0.8733\n",
      "Epoch 00035: val_loss improved from 1.62217 to 1.61925, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 1.6723 - accuracy: 0.8733 - val_loss: 1.6192 - val_accuracy: 0.9185\n",
      "Epoch 36/40\n",
      "47104/48000 [============================>.] - ETA: 0s - loss: 1.6692 - accuracy: 0.8757\n",
      "Epoch 00036: val_loss improved from 1.61925 to 1.61789, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.6691 - accuracy: 0.8757 - val_loss: 1.6179 - val_accuracy: 0.9190\n",
      "Epoch 37/40\n",
      "47264/48000 [============================>.] - ETA: 0s - loss: 1.6677 - accuracy: 0.8750\n",
      "Epoch 00037: val_loss improved from 1.61789 to 1.61613, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 1.6676 - accuracy: 0.8752 - val_loss: 1.6161 - val_accuracy: 0.9195\n",
      "Epoch 38/40\n",
      "47680/48000 [============================>.] - ETA: 0s - loss: 1.6673 - accuracy: 0.8752\n",
      "Epoch 00038: val_loss improved from 1.61613 to 1.61567, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.6671 - accuracy: 0.8754 - val_loss: 1.6157 - val_accuracy: 0.9187\n",
      "Epoch 39/40\n",
      "47456/48000 [============================>.] - ETA: 0s - loss: 1.6645 - accuracy: 0.8770\n",
      "Epoch 00039: val_loss improved from 1.61567 to 1.61475, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.6646 - accuracy: 0.8769 - val_loss: 1.6147 - val_accuracy: 0.9196\n",
      "Epoch 40/40\n",
      "47424/48000 [============================>.] - ETA: 0s - loss: 1.6639 - accuracy: 0.8768 ETA: 0s\n",
      "Epoch 00040: val_loss improved from 1.61475 to 1.61291, saving model to mejor_modelo_entrenado.h5\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.6641 - accuracy: 0.8766 - val_loss: 1.6129 - val_accuracy: 0.9208\n",
      "10000/10000 [==============================] - 1s 63us/sample - loss: 1.6146 - accuracy: 0.9197\n",
      "\n",
      "\n",
      "Puntaje total:  0.9197\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modelo keras Multicapas - sin gridsearch\n",
    "\n",
    "# Definir funcion para facilitar ingreso de funciones de activacion\n",
    "def activ(funci,canti):\n",
    "    total=[]\n",
    "    if(len(canti)==1):\n",
    "        for funcion in funci:\n",
    "            for iy in range(canti):\n",
    "                total.append(funcion)\n",
    "    elif(len(funci)==len(canti)):\n",
    "        for ix in range(len(funci)):\n",
    "            for iy in range(canti[ix]):\n",
    "                total.append(funci[ix])\n",
    "    else:\n",
    "        print('Mal ingresado parametros')\n",
    "    return total\n",
    "## Ejemplo: activ(['tanh','sigm'],[2,4]) devuele: ['tanh','tanh','sigm','sigm','sigm','sigm'] .. en la cantidad [2,4] indicada\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "# cargar datos\n",
    "(X_train , y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacion entre 0-1\n",
    "X_train = X_train.reshape(60000,784).astype('float32')/255.0\n",
    "X_test = X_test.reshape(10000,784).astype('float32')/255.0\n",
    "\n",
    "# cantidad de clases detectadas segun los valores enteros que toma la salida\n",
    "clases = len(set(y_test))\n",
    "\n",
    "print('\\n\\nX_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('-------------------')\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "print('-------------------')\n",
    "print('Clases clasificacion: ',clases)\n",
    "print('\\n')\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# dimension entrada...se indica en la primera capa de la NN\n",
    "dimension = X_train.shape[1:]\n",
    "\n",
    "def crear_modelo_cldeep(optimizer='sgd',h=[30,40,10,10],activa=activ(['tanh','softmax'],[3,1]),from_logits=True,init ='glorot_uniform',alpha=0.001,dropout=0.2): \n",
    "    \n",
    "    # Funcion de activacion \"None\" equivale a decirle que la activacion es la funcion lineal a(x)=x ... es la funcion lineal pura\n",
    "    \n",
    "    # modelo secuencial\n",
    "    modeli = keras.Sequential()\n",
    "    \n",
    "    # definir dimension de entradas\n",
    "    modeli.add(keras.Input(shape=(dimension)))\n",
    "    \n",
    "    # De la primera capa a la penultima\n",
    "    for ix in range(len(h)-1):\n",
    "        modeli.add(Dense(h[ix],kernel_initializer=init,kernel_regularizer=keras.regularizers.l2(alpha)))\n",
    "\n",
    "        # Agregar capa de Drop-Out a capas intermedias con un ratio igual a \"dropout\"=probabilidad desactivacion de neurona aleatoreamente\n",
    "        modeli.add(keras.layers.Dropout(rate=dropout))\n",
    "    \n",
    "    # Capa de salida o final\n",
    "    modeli.add(Dense(h[-1],activation=activa[-1]))\n",
    "\n",
    "    #compilar y retornar objeto modelo\n",
    "    \n",
    "    # Se recomienda colocar from_logits = True, dado que en primer lugar esto normalizará la salida a un valor entre 0-1\n",
    "    # Esto es especialmente util para funciones que no estan entre [0-1] en su salida y se requiere normalizar a valor probabilidad\n",
    "    # Ademas y punto vital, es que brinda mayor estabilidad numerica a los calculos.\n",
    "    \n",
    "    # Seleccionar la perdida (loss) adecuada al tipo de salida que se tenga (salida con valores entre [0-1], o valores enteros {1,2,3,4,..})\n",
    "    modeli.compile(optimizer = optimizer ,metrics=['accuracy'],loss=keras.losses.SparseCategoricalCrossentropy(from_logits=from_logits))\n",
    "    return modeli\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "# Callbacks para el entrenamiento\n",
    "direccion = 'mejor_modelo_entrenado.h5' # nombre de archivo a guardar modelo entrenado\n",
    "chk = keras.callbacks.ModelCheckpoint(direccion,save_best_only=True,verbose=2)\n",
    "stp = keras.callbacks.EarlyStopping(patience=20,mode='auto',min_delta=0,restore_best_weights=True,verbose=2)\n",
    "lrs = keras.callbacks.ReduceLROnPlateau(factor=0.9,patience=10,verbose=2)\n",
    "\n",
    "# Parametros de entrenamiento\n",
    "epochs = 40\n",
    "#dropout= 0.2 # desactivacion/activacion aleatoria de neuronas durante entrenamiento\n",
    "#batch_size = 128\n",
    "#shuffle =True\n",
    "callbacks = [chk,stp,lrs]\n",
    "\n",
    "#############################################################################################################################\n",
    "###### CAPAS DE NEURONAS DEL MODELO #######\n",
    "\n",
    "# Se indican cuantas neuronas por capa se requiere por capa ... cuidado, la ultima capa es la cantidad de salidas. Determinar la cantidad de salidas bien\n",
    "capas=[60,40,10,clases]  \n",
    "\n",
    "# estas son las funciones de activacion por cada capa ... cuidado que la ultima capa de activacion es para la capa final del modelo \n",
    "activacion = activ(['tanh','softmax'],[3,1])\n",
    "# activacion=['tanh','tanh','tanh','softmax'] ## opcion equivalente a usar \"activ\"\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "# wrapping... de keras a scikit model... esto convierte a model a tipo Scikite (posee .fit() , .predict() ,  .evaluate())\n",
    "model= tf.keras.wrappers.scikit_learn.KerasClassifier(crear_modelo_cldeep,h=capas,activa=activacion,epochs=epochs,callbacks=callbacks,shuffle=True,validation_split=0.2)\n",
    "\n",
    "# entrenar modelo\n",
    "history = model.fit(X_train, y_train)\n",
    "\n",
    "#############################################################\n",
    "####### Ver curvas de Aprendizaje del modelo #############\n",
    "\n",
    "historia = history.history\n",
    "\n",
    "print('\\n\\nCurvas de Aprendizaje\\n')\n",
    "for nombre,valores in historia.items():\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.title('Curva de ' + nombre)\n",
    "    plt.xlabel('Interacion \"i\"')\n",
    "    plt.ylabel(nombre)\n",
    "    plt.plot(valores)\n",
    "    plt.show()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "\n",
    "# test de rendimiento - mse\n",
    "puntaje = model.score(X_test,y_test)\n",
    "print('\\n\\nPuntaje total: ',puntaje)\n",
    "print('\\n')\n",
    "\n",
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T01:31:24.061492Z",
     "start_time": "2021-02-17T01:31:23.373671Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 58us/sample - loss: 1.6146 - accuracy: 0.9197\n",
      "\n",
      "R^2 de modelo: 91.97 %\n",
      "------------------------------\n",
      "\n",
      "Probabilidades de prediccion: \n",
      "[[0.    0.    0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.001 0.    0.    0.    0.999 0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.06  0.    0.148 0.    0.    0.    0.792 0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.99  0.    0.009 0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.    0.    0.   ]]\n",
      "------------------------------\n",
      "\n",
      "Clases Prediccion: \n",
      "[8 6 6 1 6 0 1 7 8 1]\n",
      "\n",
      "Clases Orginales: \n",
      "[8 6 6 1 5 0 1 7 8 1]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predecir - Aplicar Modelo\n",
    "\n",
    "valor = model.score(X_test,y_test)\n",
    "print('\\nR^2 de modelo:' ,np.round(valor*100.0,2),'%')\n",
    "print('------------------------------')\n",
    "\n",
    "# Probar prediccion\n",
    "tamano = 10 \n",
    "indices = np.random.randint(0,X_test.shape[0],tamano)\n",
    "\n",
    "predichos = model.predict_proba(X_test[indices])\n",
    "predichos = np.round(predichos,3)\n",
    "\n",
    "print('\\nProbabilidades de prediccion: ')\n",
    "print(predichos)\n",
    "print('------------------------------')\n",
    "\n",
    "print('\\nClases Prediccion: ')\n",
    "print(model.predict(X_test[indices]))\n",
    "print('\\nClases Orginales: ')\n",
    "print(y_test[indices])\n",
    "print('\\n\\n')\n",
    "\n",
    "# fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "295.683px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "597.769px",
    "left": "759.244px",
    "right": "20px",
    "top": "120px",
    "width": "311.569px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
